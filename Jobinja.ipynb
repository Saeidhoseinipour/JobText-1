{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a39de9b-2408-4f32-b5da-85d8a0aa0e29",
   "metadata": {},
   "source": [
    "# Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1f9f4f-f680-4893-b270-79c7ca37c991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#direction\n",
    "import os\n",
    "\n",
    "import os\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current Directory:\", current_directory)\n",
    "\n",
    "\n",
    "#new_directory = r\"D:\\My papers\\Application\"  # Use the 'r' prefix to handle backslashes in Windows paths\n",
    "\n",
    "new_directory = r\"D:\\My papers\\Application\\JobInja\"  # Use the 'r' prefix to handle backslashes in Windows paths\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir(new_directory)\n",
    "\n",
    "# Verify that the directory has been changed\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current Directory:\", current_directory)\n",
    "#D:\\My papers\\Application\\Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4188bde-9231-489a-aca8-c8a2b6b40568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3446591-747c-40b0-8f29-baad349cbac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eaaed5ff-a39e-478a-8edb-a2ee32d66eee",
   "metadata": {},
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d892405-756e-46cb-8164-1b8a5e7c8939",
   "metadata": {},
   "source": [
    "- Read codes\n",
    "- See dataset and story telling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3697bf-3eeb-46a4-a94a-e8ad9c44bcef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# JobInja API Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb8e49a-b897-4832-a447-5f4fa32d56f9",
   "metadata": {},
   "source": [
    "# Trash code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5556ba0b-b6b2-404d-8ae8-de4f429be5a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs file urls.txt not found. Using example URL...\n",
      "Processing batch 1/1 (1 URLs)\n",
      "Saved JSON data to JobInja\\v9\\batch_1.json\n",
      "Saved CSV data to JobInja\\v9\\batch_1.csv\n",
      "Saved JSON data to JobInja\\v9\\all_jobs.json\n",
      "Saved CSV data to JobInja\\v9\\all_jobs.csv\n",
      "\n",
      "Scraping completed:\n",
      "Total jobs scraped: 1\n",
      "Results saved in: JobInja/v9\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "from typing import Dict, List, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "\n",
    "class JobinjaScraper:\n",
    "    def __init__(self, save_dir: str = \"output\", delay: float = 1.0, max_workers: int = 5):\n",
    "        \"\"\"\n",
    "        Initialize the scraper\n",
    "        \n",
    "        Args:\n",
    "            save_dir: Directory to save output files\n",
    "            delay: Time to wait between requests (seconds)\n",
    "            max_workers: Maximum number of concurrent threads\n",
    "        \"\"\"\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.delay = delay\n",
    "        self.max_workers = max_workers\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        # Create save directory if it doesn't exist\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def get_page_content(self, url: str) -> Optional[str]:\n",
    "        \"\"\"Fetch the page content from the given URL\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching page {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_job_info(self, html_content: str, url: str) -> Dict:\n",
    "        \"\"\"Extract job information from HTML content\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        job_info = {'url': url}  # Include URL in the results\n",
    "\n",
    "        # Extract job title\n",
    "        title_div = soup.select_one('div.c-jobView__titleText h1')\n",
    "        if title_div:\n",
    "            job_info['عنوان شغلی'] = title_div.text.strip()\n",
    "\n",
    "        # Extract information from the first info box\n",
    "        info_box = soup.select_one('ul.c-jobView__firstInfoBox')\n",
    "        if info_box:\n",
    "            info_items = info_box.select('li.c-infoBox__item')\n",
    "            for item in info_items:\n",
    "                title = item.select_one('h4.c-infoBox__itemTitle')\n",
    "                value = item.select_one('div.tags span.black')\n",
    "                if title and value:\n",
    "                    key = title.text.strip()\n",
    "                    job_info[key] = value.text.strip()\n",
    "\n",
    "        # Extract bottom info box information\n",
    "        bottom_info_box = soup.select('ul.c-infoBox.u-mB0 li.c-infoBox__item')\n",
    "        for item in bottom_info_box:\n",
    "            title = item.select_one('h4.c-infoBox__itemTitle')\n",
    "            values = item.select('div.tags span.black')\n",
    "            if title and values:\n",
    "                key = title.text.strip()\n",
    "                if len(values) > 1:\n",
    "                    job_info[key] = ', '.join(val.text.strip() for val in values)\n",
    "                else:\n",
    "                    job_info[key] = values[0].text.strip()\n",
    "\n",
    "        return job_info\n",
    "\n",
    "    def scrape_job(self, url: str) -> Optional[Dict]:\n",
    "        \"\"\"Scrape a single job posting\"\"\"\n",
    "        html_content = self.get_page_content(url)\n",
    "        if html_content:\n",
    "            job_info = self.extract_job_info(html_content, url)\n",
    "            time.sleep(self.delay)  # Respect the site by waiting between requests\n",
    "            return job_info\n",
    "        return None\n",
    "\n",
    "    def scrape_jobs(self, urls: List[str], batch_size: int = 100) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Scrape multiple job postings concurrently with batch processing\n",
    "        \n",
    "        Args:\n",
    "            urls: List of job posting URLs\n",
    "            batch_size: Number of URLs to process in each batch\n",
    "        Returns:\n",
    "            List of dictionaries containing job information\n",
    "        \"\"\"\n",
    "        all_results = []\n",
    "        total_batches = (len(urls) + batch_size - 1) // batch_size\n",
    "\n",
    "        for batch_num in range(total_batches):\n",
    "            start_idx = batch_num * batch_size\n",
    "            end_idx = min((batch_num + 1) * batch_size, len(urls))\n",
    "            batch_urls = urls[start_idx:end_idx]\n",
    "\n",
    "            print(f\"Processing batch {batch_num + 1}/{total_batches} ({len(batch_urls)} URLs)\")\n",
    "\n",
    "            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "                futures = [executor.submit(self.scrape_job, url) for url in batch_urls]\n",
    "                batch_results = []\n",
    "                for future in futures:\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        batch_results.append(result)\n",
    "\n",
    "            # Save batch results\n",
    "            if batch_results:\n",
    "                batch_filename = f\"batch_{batch_num + 1}\"\n",
    "                self.save_batch(batch_results, batch_filename)\n",
    "                all_results.extend(batch_results)\n",
    "\n",
    "        return all_results\n",
    "\n",
    "    def save_batch(self, data: List[Dict], filename_prefix: str):\n",
    "        \"\"\"Save batch results to both JSON and CSV\"\"\"\n",
    "        # Save to JSON\n",
    "        json_path = self.save_dir / f\"{filename_prefix}.json\"\n",
    "        self.save_to_json(data, json_path)\n",
    "\n",
    "        # Save to CSV\n",
    "        csv_path = self.save_dir / f\"{filename_prefix}.csv\"\n",
    "        self.save_to_csv(data, csv_path)\n",
    "\n",
    "    def save_to_json(self, data: List[Dict], filepath: Path):\n",
    "        \"\"\"Save results to JSON file\"\"\"\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Saved JSON data to {filepath}\")\n",
    "\n",
    "    def save_to_csv(self, data: List[Dict], filepath: Path):\n",
    "        \"\"\"Save results to CSV file\"\"\"\n",
    "        if not data:\n",
    "            print(\"No data to save\")\n",
    "            return\n",
    "\n",
    "        # Get all unique keys from all dictionaries\n",
    "        fieldnames = set()\n",
    "        for item in data:\n",
    "            fieldnames.update(item.keys())\n",
    "        fieldnames = sorted(list(fieldnames))\n",
    "\n",
    "        with open(filepath, 'w', encoding='utf-8', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(data)\n",
    "        print(f\"Saved CSV data to {filepath}\")\n",
    "\n",
    "def main():\n",
    "    # Example usage with custom save directory\n",
    "    save_dir = \"JobInja/v9\"  # You can change this to your preferred directory\n",
    "    \n",
    "    # Load URLs from file\n",
    "    urls_file = \"urls.txt\"  # Create this file with your URLs\n",
    "    try:\n",
    "        with open(urls_file, 'r', encoding='utf-8') as f:\n",
    "            urls = [line.strip() for line in f if line.strip()]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"URLs file {urls_file} not found. Using example URL...\")\n",
    "        urls = [\n",
    "            \"https://jobinja.ir/companies/khanoumi/jobs/Av6Y/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-%D8%AA%D9%88%D9%84%DB%8C%D8%AF-%D9%85%D8%AD%D8%AA%D9%88%D8%A7-%D9%87%DB%8C%D8%A8%D8%B1%DB%8C%D8%AF-%D8%AF%D8%B1-%D8%AE%D8%A7%D9%86%D9%88%D9%85%DB%8C\"\n",
    "        ]\n",
    "    \n",
    "    # Initialize scraper with custom save directory\n",
    "    scraper = JobinjaScraper(\n",
    "        save_dir=save_dir,\n",
    "        delay=1.0,  # 1 second delay between requests\n",
    "        max_workers=5  # 5 concurrent threads\n",
    "    )\n",
    "    \n",
    "    # Scrape all jobs\n",
    "    results = scraper.scrape_jobs(urls, batch_size=100)\n",
    "    \n",
    "    # Save final combined results\n",
    "    scraper.save_to_json(results, Path(save_dir) / \"all_jobs.json\")\n",
    "    scraper.save_to_csv(results, Path(save_dir) / \"all_jobs.csv\")\n",
    "    \n",
    "    print(f\"\\nScraping completed:\")\n",
    "    print(f\"Total jobs scraped: {len(results)}\")\n",
    "    print(f\"Results saved in: {save_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e641d758-c7f1-4df0-87a5-5e10e65a29d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90d9ebe3-2f1c-4a85-8b31-1c7cd36aa6ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page 1...\n",
      "Found 20 URLs on page 1\n",
      "Successfully saved 20 URLs to JobInja\\urls\\page_1_urls.json\n",
      "Processing page 2...\n",
      "Found 20 URLs on page 2\n",
      "Successfully saved 20 URLs to JobInja\\urls\\page_2_urls.json\n",
      "Processing page 3...\n",
      "Found 20 URLs on page 3\n",
      "Successfully saved 20 URLs to JobInja\\urls\\page_3_urls.json\n",
      "Successfully saved 60 URLs to JobInja\\urls\\all_urls_20241107_194707.json\n",
      "Successfully saved 60 URLs to JobInja\\urls\\all_urls_20241107_194707.txt\n",
      "\n",
      "Extracted URLs:\n",
      "--------------------------------------------------------------------------------\n",
      "1. https://jobinja.ir/companies/pishro-sanat-arshit-1/jobs/Apb6/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D9%85%D8%B3%D8%A6%D9%88%D9%84-%D8%AF%D9%81%D8%AA%D8%B1-%D9%85%D8%B3%D9%84%D8%B7-%D8%A8%D9%87-%D8%A7%D9%86%DA%AF%D9%84%DB%8C%D8%B3%DB%8C-%D8%AE%D8%A7%D9%86%D9%85-%D8%AF%D8%B1-%D8%B4%D8%B1%DA%A9%D8%AA-%D9%BE%DB%8C%D8%B4%D8%B1%D9%88%D8%B5%D9%86%D8%B9%D8%AA-%D8%A2%D8%B1%D8%B4%DB%8C%D8%AA\n",
      "2. https://jobinja.ir/companies/verge/jobs/ApET/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-senior-front-end-developer-%D8%AF%D8%B1-%DA%A9%D9%84%D8%A7%D8%AF%DB%8C%D9%86%D8%A7%D8%AA%D9%88%D8%B1\n",
      "3. https://jobinja.ir/companies/aiwaweb/jobs/A6Xo/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-%D8%B3%D8%A6%D9%88-%DA%A9%D8%B1%D8%AC-%D8%AF%D8%B1-%D8%A2%DB%8C%D9%88%D8%A7-%D9%88%D8%A8\n",
      "4. https://jobinja.ir/companies/wizidar-1/jobs/AmwA/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D9%81%DB%8C%D9%84%D9%85%D8%A8%D8%B1%D8%AF%D8%A7%D8%B1-%D9%88-%D8%AA%D8%AF%D9%88%DB%8C%D9%86%DA%AF%D8%B1-%D8%A2%D9%82%D8%A7-%D8%AF%D8%B1-%D9%88%DB%8C%D8%B2%DB%8C%D8%AF%D8%A7%D8%B1\n",
      "5. https://jobinja.ir/companies/directam/jobs/A40w/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-devops-%D8%A7%D8%B5%D9%81%D9%87%D8%A7%D9%86-%D8%AF%D8%B1-%D8%AF%D8%A7%DB%8C%D8%B1%DA%A9%D8%AA%D9%85\n",
      "6. https://jobinja.ir/companies/nizva-1/jobs/A7IG/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D9%85%D8%AF%DB%8C%D8%B1-%D9%81%D8%B1%D9%88%D8%B4-%D8%AE%D8%A7%D9%86%D9%85-%D8%AF%D8%B1-%D9%86%DB%8C%D8%B2%D9%88%D8%A7\n",
      "7. https://jobinja.ir/companies/nizva-1/jobs/Am4F/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D9%81%D8%B1%D9%88%D8%B4%D9%86%D8%AF%D9%87-%D8%AE%D8%A7%D9%86%D9%85-%D8%AF%D8%B1-%D9%86%DB%8C%D8%B2%D9%88%D8%A7\n",
      "8. https://jobinja.ir/companies/autokhatib/jobs/AmWZ/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D9%88%DA%A9%DB%8C%D9%84-%D9%BE%D8%A7%DB%8C%D9%87-%DB%8C%DA%A9-%D8%AF%D8%A7%D8%AF%DA%AF%D8%B3%D8%AA%D8%B1%DB%8C-%D8%AF%D8%B1-%D8%A7%D8%AA%D9%88-%D8%AE%D8%B7%DB%8C%D8%A8\n",
      "9. https://jobinja.ir/companies/veerayco/jobs/A7y2/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D9%85%D9%87%D9%86%D8%AF%D8%B3-%D8%A8%D8%B1%D9%82-%D9%88-%D8%A7%D9%84%DA%A9%D8%AA%D8%B1%D9%88%D9%86%DB%8C%DA%A9-%D8%A2%D9%82%D8%A7-%D9%85%D8%B4%D9%87%D8%AF-%D8%AF%D8%B1-%D9%88%DB%8C%D8%B1%D8%A7%DB%8C%DA%A9%D9%88\n",
      "10. https://jobinja.ir/companies/auto-bi-nazir/jobs/A7hw/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D8%B3%D8%B1%D9%BE%D8%B1%D8%B3%D8%AA-%D8%B9%D9%85%D9%84%DB%8C%D8%A7%D8%AA-%D9%81%D8%B1%D9%88%D8%B4-%D8%AF%D8%B1-%D9%85%D9%87%D8%A7%D9%85-%D8%AE%D9%88%D8%AF%D8%B1%D9%88-%D8%A8%DB%8C-%D9%86%D8%B8%DB%8C%D8%B1\n",
      "\n",
      "... and 50 more URLs\n",
      "--------------------------------------------------------------------------------\n",
      "Total URLs: 60\n",
      "\n",
      "Files saved in: JobInja/urls\n",
      "Total URLs extracted: 60\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "class JobinjaURLExtractor:\n",
    "    def __init__(self, base_url: str = \"https://jobinja.ir/jobs\", save_dir: str = \"jobinja_data\"):\n",
    "        \"\"\"\n",
    "        Initialize the URL extractor\n",
    "        \n",
    "        Args:\n",
    "            base_url: Base URL to scrape from\n",
    "            save_dir: Directory to save output files\n",
    "        \"\"\"\n",
    "        self.base_url = base_url\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        # Create save directory if it doesn't exist\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def extract_job_urls_from_html(self, html_content: str) -> List[str]:\n",
    "        \"\"\"Extract job URLs from HTML content\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        job_links = soup.find_all('a', class_='o-listView__itemIndicator')\n",
    "        \n",
    "        urls = []\n",
    "        for link in job_links:\n",
    "            href = link.get('href')\n",
    "            if href and href.startswith('https://jobinja.ir/companies/'):\n",
    "                clean_url = href.split('?')[0] if '?' in href else href\n",
    "                urls.append(clean_url)\n",
    "        \n",
    "        return urls\n",
    "\n",
    "    def get_page_content(self, page: int = 1) -> str:\n",
    "        \"\"\"Fetch page content from jobinja\"\"\"\n",
    "        url = f\"{self.base_url}?page={page}\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching page {page}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_urls_from_pages(self, start_page: int = 1, end_page: int = 1) -> List[str]:\n",
    "        \"\"\"Extract job URLs from multiple pages\"\"\"\n",
    "        all_urls = []\n",
    "        \n",
    "        for page in range(start_page, end_page + 1):\n",
    "            print(f\"Processing page {page}...\")\n",
    "            html_content = self.get_page_content(page)\n",
    "            \n",
    "            if html_content:\n",
    "                urls = self.extract_job_urls_from_html(html_content)\n",
    "                all_urls.extend(urls)\n",
    "                print(f\"Found {len(urls)} URLs on page {page}\")\n",
    "            else:\n",
    "                print(f\"Failed to fetch page {page}\")\n",
    "            \n",
    "            # Save intermediate results for each page\n",
    "            self.save_urls_to_file(urls, f\"page_{page}_urls.json\")\n",
    "        \n",
    "        return all_urls\n",
    "\n",
    "    def save_urls_to_file(self, urls: List[str], filename: str):\n",
    "        \"\"\"\n",
    "        Save extracted URLs to a file in the specified directory\n",
    "        \n",
    "        Args:\n",
    "            urls: List of URLs to save\n",
    "            filename: Name of the file to save\n",
    "        \"\"\"\n",
    "        file_path = self.save_dir / filename\n",
    "        try:\n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(urls, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"Successfully saved {len(urls)} URLs to {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving URLs to file: {e}\")\n",
    "\n",
    "    def save_urls_to_text(self, urls: List[str], filename: str):\n",
    "        \"\"\"\n",
    "        Save URLs to a plain text file (one URL per line)\n",
    "        \n",
    "        Args:\n",
    "            urls: List of URLs to save\n",
    "            filename: Name of the file to save\n",
    "        \"\"\"\n",
    "        file_path = self.save_dir / filename\n",
    "        try:\n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                for url in urls:\n",
    "                    f.write(f\"{url}\\n\")\n",
    "            print(f\"Successfully saved {len(urls)} URLs to {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving URLs to text file: {e}\")\n",
    "\n",
    "    def print_urls(self, urls: List[str], max_display: int = None):\n",
    "        \"\"\"\n",
    "        Print URLs to console\n",
    "        \n",
    "        Args:\n",
    "            urls: List of URLs to print\n",
    "            max_display: Maximum number of URLs to display (None for all)\n",
    "        \"\"\"\n",
    "        if not urls:\n",
    "            print(\"No URLs found.\")\n",
    "            return\n",
    "\n",
    "        print(\"\\nExtracted URLs:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        display_urls = urls[:max_display] if max_display else urls\n",
    "        for i, url in enumerate(display_urls, 1):\n",
    "            print(f\"{i}. {url}\")\n",
    "        \n",
    "        if max_display and len(urls) > max_display:\n",
    "            print(f\"\\n... and {len(urls) - max_display} more URLs\")\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "        print(f\"Total URLs: {len(urls)}\")\n",
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "    save_dir = \"JobInja/urls\"  # Specify save directory\n",
    "    \n",
    "    # Initialize extractor with save directory\n",
    "    extractor = JobinjaURLExtractor(save_dir=save_dir)\n",
    "    \n",
    "    # Extract URLs from first 3 pages\n",
    "    urls = extractor.extract_urls_from_pages(start_page=1, end_page=3)\n",
    "    \n",
    "    # Save URLs in different formats\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save as JSON\n",
    "    extractor.save_urls_to_file(urls, f\"all_urls_{timestamp}.json\")\n",
    "    \n",
    "    # Save as text file\n",
    "    extractor.save_urls_to_text(urls, f\"all_urls_{timestamp}.txt\")\n",
    "    \n",
    "    # Print URLs to console (display first 10)\n",
    "    extractor.print_urls(urls, max_display=10)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nFiles saved in: {save_dir}\")\n",
    "    print(f\"Total URLs extracted: {len(urls)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97febaea-89b7-43e6-8fff-27a511a4ef6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ca59f10-49e2-4529-a47b-bb1ef8fd3a16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting URL extraction (saving to JobInja/urls2)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 30/30 [01:00<00:00,  2.01s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved JSON results to: JobInja\\urls2\\all_urls_20241107_195433.json\n",
      "Saved text results to: JobInja\\urls2\\all_urls_20241107_195433.txt\n",
      "\n",
      "Scraping Summary\n",
      "--------------------------------------------------\n",
      "Total pages scraped: 30\n",
      "Total URLs found: 600\n",
      "Average URLs per page: 20.0\n",
      "\n",
      "Sample URLs (first 10):\n",
      "1. https://jobinja.ir/companies/pishro-sanat-arshit-1/jobs/Apb6/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D9%85%D8%B3%D8%A6%D9%88%D9%84-%D8%AF%D9%81%D8%AA%D8%B1-%D9%85%D8%B3%D9%84%D8%B7-%D8%A8%D9%87-%D8%A7%D9%86%DA%AF%D9%84%DB%8C%D8%B3%DB%8C-%D8%AE%D8%A7%D9%86%D9%85-%D8%AF%D8%B1-%D8%B4%D8%B1%DA%A9%D8%AA-%D9%BE%DB%8C%D8%B4%D8%B1%D9%88%D8%B5%D9%86%D8%B9%D8%AA-%D8%A2%D8%B1%D8%B4%DB%8C%D8%AA\n",
      "2. https://jobinja.ir/companies/verge/jobs/ApET/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-senior-front-end-developer-%D8%AF%D8%B1-%DA%A9%D9%84%D8%A7%D8%AF%DB%8C%D9%86%D8%A7%D8%AA%D9%88%D8%B1\n",
      "3. https://jobinja.ir/companies/aiwaweb/jobs/A6Xo/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-%D8%B3%D8%A6%D9%88-%DA%A9%D8%B1%D8%AC-%D8%AF%D8%B1-%D8%A2%DB%8C%D9%88%D8%A7-%D9%88%D8%A8\n",
      "4. https://jobinja.ir/companies/wizidar-1/jobs/AmwA/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D9%81%DB%8C%D9%84%D9%85%D8%A8%D8%B1%D8%AF%D8%A7%D8%B1-%D9%88-%D8%AA%D8%AF%D9%88%DB%8C%D9%86%DA%AF%D8%B1-%D8%A2%D9%82%D8%A7-%D8%AF%D8%B1-%D9%88%DB%8C%D8%B2%DB%8C%D8%AF%D8%A7%D8%B1\n",
      "5. https://jobinja.ir/companies/directam/jobs/A40w/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-devops-%D8%A7%D8%B5%D9%81%D9%87%D8%A7%D9%86-%D8%AF%D8%B1-%D8%AF%D8%A7%DB%8C%D8%B1%DA%A9%D8%AA%D9%85\n",
      "6. https://jobinja.ir/companies/nizva-1/jobs/A7IG/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D9%85%D8%AF%DB%8C%D8%B1-%D9%81%D8%B1%D9%88%D8%B4-%D8%AE%D8%A7%D9%86%D9%85-%D8%AF%D8%B1-%D9%86%DB%8C%D8%B2%D9%88%D8%A7\n",
      "7. https://jobinja.ir/companies/nizva-1/jobs/Am4F/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D9%81%D8%B1%D9%88%D8%B4%D9%86%D8%AF%D9%87-%D8%AE%D8%A7%D9%86%D9%85-%D8%AF%D8%B1-%D9%86%DB%8C%D8%B2%D9%88%D8%A7\n",
      "8. https://jobinja.ir/companies/autokhatib/jobs/AmWZ/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D9%88%DA%A9%DB%8C%D9%84-%D9%BE%D8%A7%DB%8C%D9%87-%DB%8C%DA%A9-%D8%AF%D8%A7%D8%AF%DA%AF%D8%B3%D8%AA%D8%B1%DB%8C-%D8%AF%D8%B1-%D8%A7%D8%AA%D9%88-%D8%AE%D8%B7%DB%8C%D8%A8\n",
      "9. https://jobinja.ir/companies/veerayco/jobs/A7y2/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D9%85%D9%87%D9%86%D8%AF%D8%B3-%D8%A8%D8%B1%D9%82-%D9%88-%D8%A7%D9%84%DA%A9%D8%AA%D8%B1%D9%88%D9%86%DB%8C%DA%A9-%D8%A2%D9%82%D8%A7-%D9%85%D8%B4%D9%87%D8%AF-%D8%AF%D8%B1-%D9%88%DB%8C%D8%B1%D8%A7%DB%8C%DA%A9%D9%88\n",
      "10. https://jobinja.ir/companies/auto-bi-nazir/jobs/A7hw/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D8%B3%D8%B1%D9%BE%D8%B1%D8%B3%D8%AA-%D8%B9%D9%85%D9%84%DB%8C%D8%A7%D8%AA-%D9%81%D8%B1%D9%88%D8%B4-%D8%AF%D8%B1-%D9%85%D9%87%D8%A7%D9%85-%D8%AE%D9%88%D8%AF%D8%B1%D9%88-%D8%A8%DB%8C-%D9%86%D8%B8%DB%8C%D8%B1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Dict\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "class JobinjaPaginatedURLExtractor:\n",
    "    def __init__(self, save_dir: str = \"jobinja_data\", delay: float = 1.0):\n",
    "        \"\"\"\n",
    "        Initialize the URL extractor\n",
    "        \n",
    "        Args:\n",
    "            save_dir: Directory to save output files\n",
    "            delay: Delay between requests in seconds\n",
    "        \"\"\"\n",
    "        self.base_url = \"https://jobinja.ir/jobs\"\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.delay = delay\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        # Create save directory if it doesn't exist\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def get_page_url(self, page: int) -> str:\n",
    "        \"\"\"Generate URL for specific page\"\"\"\n",
    "        return f\"{self.base_url}?page={page}\"\n",
    "\n",
    "    def extract_urls_from_html(self, html_content: str) -> List[str]:\n",
    "        \"\"\"Extract job URLs from HTML content\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        job_links = soup.find_all('a', class_='o-listView__itemIndicator')\n",
    "        \n",
    "        urls = []\n",
    "        for link in job_links:\n",
    "            href = link.get('href')\n",
    "            if href and href.startswith('https://jobinja.ir/companies/'):\n",
    "                # Clean up URL by removing tracking parameters\n",
    "                clean_url = href.split('?')[0] if '?' in href else href\n",
    "                urls.append(clean_url)\n",
    "        \n",
    "        return urls\n",
    "\n",
    "    def get_page_content(self, page: int) -> str:\n",
    "        \"\"\"Fetch page content with retry mechanism\"\"\"\n",
    "        url = self.get_page_url(page)\n",
    "        max_retries = 3\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.get(url, headers=self.headers)\n",
    "                response.raise_for_status()\n",
    "                return response.text\n",
    "            except requests.RequestException as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    print(f\"Error fetching page {page} after {max_retries} attempts: {e}\")\n",
    "                    return None\n",
    "                time.sleep(self.delay * 2)  # Wait longer between retries\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def scrape_all_pages(self, start_page: int = 1, end_page: int = 30) -> Dict[int, List[str]]:\n",
    "        \"\"\"\n",
    "        Scrape URLs from all pages in range\n",
    "        \n",
    "        Args:\n",
    "            start_page: First page to scrape\n",
    "            end_page: Last page to scrape\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping page numbers to lists of URLs\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Create progress bar\n",
    "        with tqdm(total=end_page - start_page + 1, desc=\"Scraping pages\") as pbar:\n",
    "            for page in range(start_page, end_page + 1):\n",
    "                # Fetch and process page\n",
    "                html_content = self.get_page_content(page)\n",
    "                \n",
    "                if html_content:\n",
    "                    urls = self.extract_urls_from_html(html_content)\n",
    "                    results[page] = urls\n",
    "                    \n",
    "                    # Save intermediate results\n",
    "                    self.save_page_results(page, urls)\n",
    "                else:\n",
    "                    results[page] = []\n",
    "                    print(f\"\\nFailed to fetch page {page}\")\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({'URLs': len(results[page])})\n",
    "                \n",
    "                # Respect the site by waiting between requests\n",
    "                time.sleep(self.delay)\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def save_page_results(self, page: int, urls: List[str]):\n",
    "        \"\"\"Save results for individual page\"\"\"\n",
    "        filename = f\"page_{page:03d}_urls.json\"\n",
    "        file_path = self.save_dir / filename\n",
    "        \n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'page': page,\n",
    "                'urls': urls,\n",
    "                'count': len(urls),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    def save_combined_results(self, results: Dict[int, List[str]], format: str = 'both'):\n",
    "        \"\"\"\n",
    "        Save combined results in specified format\n",
    "        \n",
    "        Args:\n",
    "            results: Dictionary of results by page\n",
    "            format: Output format ('json', 'txt', or 'both')\n",
    "        \"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Flatten URLs into a single list\n",
    "        all_urls = []\n",
    "        for page_urls in results.values():\n",
    "            all_urls.extend(page_urls)\n",
    "        \n",
    "        # Save as JSON if requested\n",
    "        if format in ['json', 'both']:\n",
    "            json_path = self.save_dir / f\"all_urls_{timestamp}.json\"\n",
    "            with open(json_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump({\n",
    "                    'total_urls': len(all_urls),\n",
    "                    'total_pages': len(results),\n",
    "                    'urls_by_page': results,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"\\nSaved JSON results to: {json_path}\")\n",
    "        \n",
    "        # Save as text if requested\n",
    "        if format in ['txt', 'both']:\n",
    "            txt_path = self.save_dir / f\"all_urls_{timestamp}.txt\"\n",
    "            with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "                for url in all_urls:\n",
    "                    f.write(f\"{url}\\n\")\n",
    "            print(f\"Saved text results to: {txt_path}\")\n",
    "        \n",
    "        return all_urls\n",
    "\n",
    "    def print_summary(self, results: Dict[int, List[str]], max_display: int = 10):\n",
    "        \"\"\"Print summary of results\"\"\"\n",
    "        total_urls = sum(len(urls) for urls in results.values())\n",
    "        \n",
    "        print(\"\\nScraping Summary\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Total pages scraped: {len(results)}\")\n",
    "        print(f\"Total URLs found: {total_urls}\")\n",
    "        print(f\"Average URLs per page: {total_urls / len(results):.1f}\")\n",
    "        \n",
    "        # Print sample URLs\n",
    "        print(f\"\\nSample URLs (first {max_display}):\")\n",
    "        urls_shown = 0\n",
    "        for page, page_urls in sorted(results.items()):\n",
    "            for url in page_urls:\n",
    "                if urls_shown >= max_display:\n",
    "                    break\n",
    "                print(f\"{urls_shown + 1}. {url}\")\n",
    "                urls_shown += 1\n",
    "            if urls_shown >= max_display:\n",
    "                break\n",
    "\n",
    "def main():\n",
    "    # Initialize extractor\n",
    "    save_dir = \"JobInja/urls2\"  # Specify save directory\n",
    "    extractor = JobinjaPaginatedURLExtractor(save_dir=save_dir, delay=1.0)\n",
    "    \n",
    "    # Scrape pages 1 to 30\n",
    "    print(f\"Starting URL extraction (saving to {save_dir})...\")\n",
    "    results = extractor.scrape_all_pages(start_page=1, end_page=30)\n",
    "    \n",
    "    # Save combined results\n",
    "    all_urls = extractor.save_combined_results(results, format='both')\n",
    "    \n",
    "    # Print summary\n",
    "    extractor.print_summary(results)\n",
    "    \n",
    "    return all_urls\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    urls = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6018b611-244c-4c87-a203-287bb956ae52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55a63d1b-b089-436c-bae0-b3ea4ba3fdfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   3%|▎         | 1/30 [00:03<01:32,  3.20s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 1 results to JobInja\\v12\\pages\\page_001.json\n",
      "Scraping page 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   7%|▋         | 2/30 [00:05<01:13,  2.64s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 2 results to JobInja\\v12\\pages\\page_002.json\n",
      "Scraping page 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  10%|█         | 3/30 [00:07<01:09,  2.57s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 3 results to JobInja\\v12\\pages\\page_003.json\n",
      "Scraping page 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  13%|█▎        | 4/30 [00:10<01:04,  2.49s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 4 results to JobInja\\v12\\pages\\page_004.json\n",
      "Scraping page 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  17%|█▋        | 5/30 [00:12<01:01,  2.46s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 5 results to JobInja\\v12\\pages\\page_005.json\n",
      "Scraping page 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  20%|██        | 6/30 [00:15<00:58,  2.45s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 6 results to JobInja\\v12\\pages\\page_006.json\n",
      "Scraping page 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  23%|██▎       | 7/30 [00:17<00:53,  2.32s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 7 results to JobInja\\v12\\pages\\page_007.json\n",
      "Scraping page 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  27%|██▋       | 8/30 [00:19<00:51,  2.33s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 8 results to JobInja\\v12\\pages\\page_008.json\n",
      "Scraping page 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  30%|███       | 9/30 [00:22<00:50,  2.39s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 9 results to JobInja\\v12\\pages\\page_009.json\n",
      "Scraping page 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  33%|███▎      | 10/30 [00:24<00:49,  2.47s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 10 results to JobInja\\v12\\pages\\page_010.json\n",
      "Scraping page 11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  37%|███▋      | 11/30 [00:27<00:45,  2.41s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 11 results to JobInja\\v12\\pages\\page_011.json\n",
      "Scraping page 12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  40%|████      | 12/30 [00:29<00:43,  2.43s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 12 results to JobInja\\v12\\pages\\page_012.json\n",
      "Scraping page 13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  43%|████▎     | 13/30 [00:31<00:40,  2.39s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 13 results to JobInja\\v12\\pages\\page_013.json\n",
      "Scraping page 14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  47%|████▋     | 14/30 [00:34<00:38,  2.41s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 14 results to JobInja\\v12\\pages\\page_014.json\n",
      "Scraping page 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  50%|█████     | 15/30 [00:36<00:37,  2.52s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 15 results to JobInja\\v12\\pages\\page_015.json\n",
      "Scraping page 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  53%|█████▎    | 16/30 [00:44<00:55,  3.99s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 16 results to JobInja\\v12\\pages\\page_016.json\n",
      "Scraping page 17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  57%|█████▋    | 17/30 [00:49<00:57,  4.45s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 17 results to JobInja\\v12\\pages\\page_017.json\n",
      "Scraping page 18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  60%|██████    | 18/30 [00:52<00:48,  4.02s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 18 results to JobInja\\v12\\pages\\page_018.json\n",
      "Scraping page 19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  63%|██████▎   | 19/30 [00:55<00:38,  3.55s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 19 results to JobInja\\v12\\pages\\page_019.json\n",
      "Scraping page 20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  67%|██████▋   | 20/30 [00:58<00:33,  3.38s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 20 results to JobInja\\v12\\pages\\page_020.json\n",
      "Scraping page 21...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  70%|███████   | 21/30 [01:04<00:38,  4.25s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 21 results to JobInja\\v12\\pages\\page_021.json\n",
      "Scraping page 22...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  73%|███████▎  | 22/30 [01:33<01:32, 11.60s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 22 results to JobInja\\v12\\pages\\page_022.json\n",
      "Scraping page 23...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  77%|███████▋  | 23/30 [03:13<04:26, 38.07s/it, URLs=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching page 23: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Scraping page 24...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  80%|████████  | 24/30 [03:25<03:01, 30.32s/it, URLs=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching page 24: HTTPSConnectionPool(host='jobinja.ir', port=443): Max retries exceeded with url: /jobs?page=24 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000026D1BD0EA50>: Failed to resolve 'jobinja.ir' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Scraping page 25...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  83%|████████▎ | 25/30 [03:32<01:56, 23.22s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 25 results to JobInja\\v12\\pages\\page_025.json\n",
      "Scraping page 26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  87%|████████▋ | 26/30 [03:33<01:07, 16.81s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 26 results to JobInja\\v12\\pages\\page_026.json\n",
      "Scraping page 27...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  90%|█████████ | 27/30 [03:35<00:36, 12.31s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 27 results to JobInja\\v12\\pages\\page_027.json\n",
      "Scraping page 28...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  93%|█████████▎| 28/30 [03:37<00:18,  9.18s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 28 results to JobInja\\v12\\pages\\page_028.json\n",
      "Scraping page 29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  97%|█████████▋| 29/30 [03:39<00:07,  7.03s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 29 results to JobInja\\v12\\pages\\page_029.json\n",
      "Scraping page 30...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 30/30 [03:41<00:00,  5.49s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 30 results to JobInja\\v12\\pages\\page_030.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 30/30 [03:42<00:00,  7.42s/it, URLs=20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved combined results to JobInja\\v12\\all_pages_20241107_230935.json\n",
      "\n",
      "Scraping Summary\n",
      "--------------------------------------------------\n",
      "Total pages scraped: 30\n",
      "Total URLs found: 560\n",
      "Average URLs per page: 18.7\n",
      "\n",
      "Results saved in: JobInja\\v12\n",
      "Individual pages saved in: JobInja\\v12\\pages\n",
      "\n",
      "Example URLs from first page:\n",
      "1. https://jobinja.ir/companies/pishro-sanat-arshit-1/jobs/Apb6/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D9%85%D8%B3%D8%A6%D9%88%D9%84-%D8%AF%D9%81%D8%AA%D8%B1-%D9%85%D8%B3%D9%84%D8%B7-%D8%A8%D9%87-%D8%A7%D9%86%DA%AF%D9%84%DB%8C%D8%B3%DB%8C-%D8%AE%D8%A7%D9%86%D9%85-%D8%AF%D8%B1-%D8%B4%D8%B1%DA%A9%D8%AA-%D9%BE%DB%8C%D8%B4%D8%B1%D9%88%D8%B5%D9%86%D8%B9%D8%AA-%D8%A2%D8%B1%D8%B4%DB%8C%D8%AA\n",
      "2. https://jobinja.ir/companies/verge/jobs/ApET/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-senior-front-end-developer-%D8%AF%D8%B1-%DA%A9%D9%84%D8%A7%D8%AF%DB%8C%D9%86%D8%A7%D8%AA%D9%88%D8%B1\n",
      "3. https://jobinja.ir/companies/aiwaweb/jobs/A6Xo/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-%D8%B3%D8%A6%D9%88-%DA%A9%D8%B1%D8%AC-%D8%AF%D8%B1-%D8%A2%DB%8C%D9%88%D8%A7-%D9%88%D8%A8\n",
      "4. https://jobinja.ir/companies/wizidar-1/jobs/AmwA/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D9%81%DB%8C%D9%84%D9%85%D8%A8%D8%B1%D8%AF%D8%A7%D8%B1-%D9%88-%D8%AA%D8%AF%D9%88%DB%8C%D9%86%DA%AF%D8%B1-%D8%A2%D9%82%D8%A7-%D8%AF%D8%B1-%D9%88%DB%8C%D8%B2%DB%8C%D8%AF%D8%A7%D8%B1\n",
      "5. https://jobinja.ir/companies/directam/jobs/A40w/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-devops-%D8%A7%D8%B5%D9%81%D9%87%D8%A7%D9%86-%D8%AF%D8%B1-%D8%AF%D8%A7%DB%8C%D8%B1%DA%A9%D8%AA%D9%85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Dict, List, Optional\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "class JobinjaPageScraper:\n",
    "    def __init__(self, save_dir: str = \"jobinja_data\", delay: float = 1.0, max_workers: int = 5):\n",
    "        \"\"\"\n",
    "        Initialize the scraper\n",
    "        \n",
    "        Args:\n",
    "            save_dir: Directory to save output files\n",
    "            delay: Time to wait between requests (seconds)\n",
    "            max_workers: Maximum number of concurrent threads\n",
    "        \"\"\"\n",
    "        self.base_url = \"https://jobinja.ir/jobs\"\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.delay = delay\n",
    "        self.max_workers = max_workers\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        # Create save directory and subdirectories\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.pages_dir = self.save_dir / \"pages\"\n",
    "        self.pages_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    def get_page_content(self, page: int) -> Optional[str]:\n",
    "        \"\"\"Fetch content for a specific page\"\"\"\n",
    "        url = f\"{self.base_url}?page={page}\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching page {page}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_page_urls(self, html_content: str) -> List[str]:\n",
    "        \"\"\"Extract job URLs from a page\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        job_links = soup.find_all('a', class_='o-listView__itemIndicator')\n",
    "        \n",
    "        urls = []\n",
    "        for link in job_links:\n",
    "            href = link.get('href')\n",
    "            if href and href.startswith('https://jobinja.ir/companies/'):\n",
    "                clean_url = href.split('?')[0]\n",
    "                urls.append(clean_url)\n",
    "        \n",
    "        return urls\n",
    "\n",
    "    def scrape_page(self, page: int) -> Dict:\n",
    "        \"\"\"Scrape a single page\"\"\"\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        \n",
    "        html_content = self.get_page_content(page)\n",
    "        if not html_content:\n",
    "            return {\n",
    "                'page': page,\n",
    "                'urls': [],\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'status': 'failed'\n",
    "            }\n",
    "        \n",
    "        urls = self.extract_page_urls(html_content)\n",
    "        \n",
    "        result = {\n",
    "            'page': page,\n",
    "            'urls': urls,\n",
    "            'count': len(urls),\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'status': 'success'\n",
    "        }\n",
    "        \n",
    "        # Save page results immediately\n",
    "        self.save_page_results(page, result)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def scrape_pages(self, start_page: int = 1, end_page: int = 30) -> Dict[int, List[str]]:\n",
    "        \"\"\"\n",
    "        Scrape multiple pages\n",
    "        \n",
    "        Args:\n",
    "            start_page: First page to scrape\n",
    "            end_page: Last page to scrape\n",
    "        Returns:\n",
    "            Dictionary mapping page numbers to lists of URLs\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Create progress bar\n",
    "        with tqdm(total=end_page - start_page + 1, desc=\"Scraping pages\") as pbar:\n",
    "            for page in range(start_page, end_page + 1):\n",
    "                result = self.scrape_page(page)\n",
    "                results[page] = result['urls']\n",
    "                \n",
    "                # Update progress\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({'URLs': len(result['urls'])})\n",
    "                \n",
    "                # Wait between requests\n",
    "                time.sleep(self.delay)\n",
    "        \n",
    "        # Save combined results\n",
    "        self.save_combined_results(results)\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def save_page_results(self, page: int, data: Dict):\n",
    "        \"\"\"Save results for a single page\"\"\"\n",
    "        filename = f\"page_{page:03d}.json\"\n",
    "        filepath = self.pages_dir / filename\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Saved page {page} results to {filepath}\")\n",
    "\n",
    "    def save_combined_results(self, results: Dict[int, List[str]]):\n",
    "        \"\"\"Save combined results from all pages\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filepath = self.save_dir / f\"all_pages_{timestamp}.json\"\n",
    "        \n",
    "        # Calculate total URLs\n",
    "        total_urls = sum(len(urls) for urls in results.values())\n",
    "        \n",
    "        data = {\n",
    "            'total_urls': total_urls,\n",
    "            'total_pages': len(results),\n",
    "            'urls_by_page': results,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"\\nSaved combined results to {filepath}\")\n",
    "\n",
    "    def print_summary(self, results: Dict[int, List[str]]):\n",
    "        \"\"\"Print summary of results\"\"\"\n",
    "        total_urls = sum(len(urls) for urls in results.values())\n",
    "        \n",
    "        print(\"\\nScraping Summary\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Total pages scraped: {len(results)}\")\n",
    "        print(f\"Total URLs found: {total_urls}\")\n",
    "        print(f\"Average URLs per page: {total_urls / len(results):.1f}\")\n",
    "        print(f\"\\nResults saved in: {self.save_dir}\")\n",
    "        print(f\"Individual pages saved in: {self.pages_dir}\")\n",
    "\n",
    "def main():\n",
    "    # Initialize scraper with custom save directory\n",
    "    save_dir = \"JobInja/v12\"  # You can change this to your preferred directory\n",
    "    scraper = JobinjaPageScraper(\n",
    "        save_dir=save_dir,\n",
    "        delay=1.0,  # 1 second delay between requests\n",
    "        max_workers=5  # 5 concurrent threads\n",
    "    )\n",
    "    \n",
    "    # Scrape pages 1-30\n",
    "    results = scraper.scrape_pages(start_page=1, end_page=30)\n",
    "    \n",
    "    # Print summary\n",
    "    scraper.print_summary(results)\n",
    "    \n",
    "    print(\"\\nExample URLs from first page:\")\n",
    "    if 1 in results and results[1]:\n",
    "        for i, url in enumerate(results[1][:5], 1):\n",
    "            print(f\"{i}. {url}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6292e21-705e-4fca-a483-d2f131b4129a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using font: Vazir\n",
      "Starting job processing...\n",
      "Processing file: C:\\Users\\saeid\\JobInja\\v11\\all_jobs.json\n",
      "Using font: C:\\Users\\saeid\\AppData\\Local\\Microsoft\\Windows\\Fonts\\Vazir-Regular.ttf\n",
      "\n",
      "Processing 60 jobs...\n",
      "Processing job 60/60\n",
      "Successfully processed 60 jobs\n",
      "Word clouds saved in: C:\\Users\\saeid\\jobinja_processed_data\\wordclouds\n",
      "Processed file saved to: C:\\Users\\saeid\\jobinja_processed_data\\jobs_with_wordclouds.json\n",
      "\n",
      "Processing Summary:\n",
      "--------------------------------------------------\n",
      "Total jobs processed: 60\n",
      "Jobs with word clouds: 60\n",
      "\n",
      "Sample processed job:\n",
      "ID: 1\n",
      "Company: pishro-sanat-arshit-1\n",
      "Title: استخدام مسئول دفتر (مسلط به انگلیسی-خانم)\n",
      "Has word cloud: Yes\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from hazm import Normalizer, word_tokenize\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "class JobinjaDataProcessor:\n",
    "    def __init__(self, save_dir: str = \"jobinja_data\"):\n",
    "        \"\"\"\n",
    "        Initialize the processor\n",
    "        \"\"\"\n",
    "        self.save_dir = Path(save_dir.strip().replace('\\x0b', '').replace('\\x07', ''))\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Create directory for word clouds\n",
    "        self.wordcloud_dir = self.save_dir / \"wordclouds\"\n",
    "        self.wordcloud_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Initialize Persian text processor\n",
    "        self.normalizer = Normalizer()\n",
    "        \n",
    "        # Get available fonts\n",
    "        self.font_path = self.get_persian_font()\n",
    "        if not self.font_path:\n",
    "            print(\"Warning: No suitable Persian font found. Word clouds may not render correctly.\")\n",
    "        \n",
    "        # Persian stopwords\n",
    "        self.stopwords = set([\n",
    "            'و', 'در', 'به', 'از', 'که', 'می', 'این', 'است', 'را', 'با', 'های', 'برای',\n",
    "            'آن', 'خود', 'تا', 'کرد', 'بر', 'هر', 'نیز', 'ما', 'اما', 'یا', 'شد', 'او',\n",
    "            'ها', 'هم', 'شده', 'کند', 'من', 'باید', 'دارد', 'دیگر', 'همه', 'شود', 'یک',\n",
    "            'می‌باشد', 'می‌شود', 'شرکت', 'سال', 'کار', 'فعالیت', 'ایران', 'دارای', 'بوده',\n",
    "            'توسط', 'بین', 'پس', 'شما', 'گردید', 'باشد', 'توانید', 'بخش', 'زمینه', 'افراد',\n",
    "            'چند', 'بود', 'جهت', 'دهد', 'شوند', 'بسیار', 'براساس', 'پیش', 'بیش', 'پس'\n",
    "        ])\n",
    "\n",
    "    def get_persian_font(self) -> Optional[str]:\n",
    "        \"\"\"Find a suitable Persian font\"\"\"\n",
    "        # List of possible Persian fonts\n",
    "        persian_fonts = [\n",
    "            'Vazir', 'Iran', 'IRANSans', 'Tahoma', 'Arial', 'B Nazanin', \n",
    "            'Times New Roman', 'DejaVu Sans', 'Noto Naskh Arabic'\n",
    "        ]\n",
    "        \n",
    "        # First check if Vazir.ttf exists in current directory\n",
    "        if os.path.exists('Vazir.ttf'):\n",
    "            return 'Vazir.ttf'\n",
    "            \n",
    "        # Then check system fonts\n",
    "        system_fonts = set([f.name for f in fm.fontManager.ttflist])\n",
    "        \n",
    "        for font in persian_fonts:\n",
    "            if font in system_fonts:\n",
    "                try:\n",
    "                    font_path = fm.findfont(font)\n",
    "                    print(f\"Using font: {font}\")\n",
    "                    return font_path\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "        return None\n",
    "\n",
    "    def create_circular_mask(self, width, height):\n",
    "        \"\"\"Create circular mask for word cloud\"\"\"\n",
    "        center = (int(width/2), int(height/2))\n",
    "        radius = min(width, height) // 2\n",
    "        Y, X = np.ogrid[:height, :width]\n",
    "        dist_from_center = np.sqrt((X - center[0])**2 + (Y - center[1])**2)\n",
    "        mask = dist_from_center <= radius\n",
    "        return mask\n",
    "\n",
    "    def preprocess_persian_text(self, text: str) -> str:\n",
    "        \"\"\"Preprocess Persian text for word cloud\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "            \n",
    "        # Normalize text\n",
    "        text = self.normalizer.normalize(text)\n",
    "        \n",
    "        # Remove special characters and extra whitespace\n",
    "        text = re.sub(r'[^\\w\\s\\u0600-\\u06FF]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Tokenize\n",
    "        words = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords and short words\n",
    "        words = [w for w in words if w not in self.stopwords and len(w) > 1]\n",
    "        \n",
    "        return ' '.join(words)\n",
    "\n",
    "    def generate_wordcloud(self, text: str, job_id: int) -> Optional[str]:\n",
    "        \"\"\"Generate word cloud for Persian text\"\"\"\n",
    "        try:\n",
    "            if not text or not self.font_path:\n",
    "                return None\n",
    "                \n",
    "            # Preprocess text\n",
    "            processed_text = self.preprocess_persian_text(text)\n",
    "            if not processed_text:\n",
    "                return None\n",
    "                \n",
    "            # Reshape Persian text\n",
    "            processed_text = arabic_reshaper.reshape(processed_text)\n",
    "            processed_text = get_display(processed_text)\n",
    "            \n",
    "            # Create word cloud\n",
    "            mask = self.create_circular_mask(400, 400)\n",
    "            wordcloud = WordCloud(\n",
    "                width=400,\n",
    "                height=400,\n",
    "                background_color='white',\n",
    "                mask=mask,\n",
    "                font_path=self.font_path,\n",
    "                max_words=100,\n",
    "                min_font_size=10,\n",
    "                prefer_horizontal=0.7\n",
    "            ).generate(processed_text)\n",
    "            \n",
    "            # Create figure\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Save to BytesIO\n",
    "            img_data = BytesIO()\n",
    "            plt.savefig(img_data, format='png', bbox_inches='tight', pad_inches=0)\n",
    "            plt.close()\n",
    "            \n",
    "            # Convert to base64\n",
    "            img_data.seek(0)\n",
    "            img_base64 = base64.b64encode(img_data.read()).decode()\n",
    "            \n",
    "            # Save individual word cloud image\n",
    "            wordcloud_path = self.wordcloud_dir / f\"wordcloud_{job_id}.png\"\n",
    "            wordcloud.to_file(str(wordcloud_path))\n",
    "            \n",
    "            return img_base64\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating word cloud for job {job_id}: {e}\")\n",
    "            if 'font' in str(e).lower():\n",
    "                print(\"Font-related error. Available fonts:\", fm.findSystemFonts())\n",
    "            return None\n",
    "\n",
    "    def extract_company_name(self, url: str) -> str:\n",
    "        \"\"\"Extract company name from job URL\"\"\"\n",
    "        try:\n",
    "            pattern = r'companies/([^/]+)/jobs'\n",
    "            match = re.search(pattern, url)\n",
    "            return match.group(1) if match else \"unknown\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting company name from URL {url}: {e}\")\n",
    "            return \"unknown\"\n",
    "\n",
    "    def process_job(self, job: Dict, index: int) -> Dict:\n",
    "        \"\"\"Process a single job entry\"\"\"\n",
    "        # Extract company name\n",
    "        company_name = self.extract_company_name(job.get('url', ''))\n",
    "        \n",
    "        # Generate word cloud\n",
    "        company_desc = job.get('معرفی شرکت', '')\n",
    "        wordcloud_base64 = self.generate_wordcloud(company_desc, index)\n",
    "        \n",
    "        # Create processed job dict\n",
    "        processed_job = {\n",
    "            'id': index,\n",
    "            'company_name': company_name,\n",
    "            'wordcloud': wordcloud_base64 if wordcloud_base64 else None,\n",
    "            **job\n",
    "        }\n",
    "        \n",
    "        return processed_job\n",
    "\n",
    "    def process_file(self, input_file: str, output_file: str = None):\n",
    "        \"\"\"Process a JSON file\"\"\"\n",
    "        try:\n",
    "            # Validate input file\n",
    "            input_path = Path(input_file)\n",
    "            if not input_path.exists():\n",
    "                print(f\"Error: Input file not found: {input_path}\")\n",
    "                return\n",
    "\n",
    "            # Set output file\n",
    "            if output_file is None:\n",
    "                timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "                output_path = self.save_dir / f\"jobs_processed_{timestamp}.json\"\n",
    "            else:\n",
    "                output_path = Path(output_file)\n",
    "\n",
    "            print(f\"Processing file: {input_path}\")\n",
    "            print(f\"Using font: {self.font_path}\")\n",
    "\n",
    "            # Read input file\n",
    "            with open(input_path, 'r', encoding='utf-8') as f:\n",
    "                jobs = json.load(f)\n",
    "\n",
    "            # Process jobs\n",
    "            processed_jobs = []\n",
    "            total_jobs = len(jobs)\n",
    "            \n",
    "            print(f\"\\nProcessing {total_jobs} jobs...\")\n",
    "            for i, job in enumerate(jobs, 1):\n",
    "                print(f\"Processing job {i}/{total_jobs}\", end='\\r')\n",
    "                processed_job = self.process_job(job, i)\n",
    "                processed_jobs.append(processed_job)\n",
    "\n",
    "            # Save processed jobs\n",
    "            output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(processed_jobs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            print(f\"\\nSuccessfully processed {len(processed_jobs)} jobs\")\n",
    "            print(f\"Word clouds saved in: {self.wordcloud_dir}\")\n",
    "            print(f\"Processed file saved to: {output_path}\")\n",
    "            \n",
    "            # Print sample and stats\n",
    "            self.print_summary(processed_jobs)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing file: {e}\")\n",
    "            print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "    def print_summary(self, jobs: List[Dict]):\n",
    "        \"\"\"Print summary of processed jobs\"\"\"\n",
    "        print(\"\\nProcessing Summary:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Count jobs with word clouds\n",
    "        jobs_with_clouds = sum(1 for job in jobs if job.get('wordcloud'))\n",
    "        print(f\"Total jobs processed: {len(jobs)}\")\n",
    "        print(f\"Jobs with word clouds: {jobs_with_clouds}\")\n",
    "        \n",
    "        # Print sample job\n",
    "        if jobs:\n",
    "            print(\"\\nSample processed job:\")\n",
    "            job = jobs[0]\n",
    "            print(f\"ID: {job['id']}\")\n",
    "            print(f\"Company: {job['company_name']}\")\n",
    "            print(f\"Title: {job.get('عنوان شغلی', 'No title')}\")\n",
    "            print(f\"Has word cloud: {'Yes' if job.get('wordcloud') else 'No'}\")\n",
    "\n",
    "def main():\n",
    "    # Get current directory\n",
    "    current_dir = Path.cwd()\n",
    "    \n",
    "    # Initialize processor\n",
    "    save_dir = current_dir / \"jobinja_processed_data\"\n",
    "    processor = JobinjaDataProcessor(save_dir=str(save_dir))\n",
    "    \n",
    "    # Set file paths\n",
    "    input_file = current_dir / \"JobInja\" / \"v11\" / \"all_jobs.json\"\n",
    "    output_file = save_dir / \"jobs_with_wordclouds.json\"\n",
    "    \n",
    "    print(\"Starting job processing...\")\n",
    "    processor.process_file(\n",
    "        input_file=str(input_file),\n",
    "        output_file=str(output_file)\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9f05be9-e395-40a1-bcc7-e563c1e73524",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools in c:\\users\\saeid\\anaconda3\\lib\\site-packages (68.0.0)\n",
      "Collecting setuptools\n",
      "  Obtaining dependency information for setuptools from https://files.pythonhosted.org/packages/90/12/282ee9bce8b58130cb762fbc9beabd531549952cac11fc56add11dcb7ea0/setuptools-75.3.0-py3-none-any.whl.metadata\n",
      "  Downloading setuptools-75.3.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: wheel in c:\\users\\saeid\\anaconda3\\lib\\site-packages (0.38.4)\n",
      "Collecting wheel\n",
      "  Obtaining dependency information for wheel from https://files.pythonhosted.org/packages/1b/d1/9babe2ccaecff775992753d8686970b1e2755d21c8a63be73aba7a4e7d77/wheel-0.44.0-py3-none-any.whl.metadata\n",
      "  Downloading wheel-0.44.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading setuptools-75.3.0-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/1.3 MB 656.4 kB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.1/1.3 MB 751.6 kB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.1/1.3 MB 590.8 kB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.2/1.3 MB 748.1 kB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.2/1.3 MB 846.9 kB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.3/1.3 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.5/1.3 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.5/1.3 MB 1.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.6/1.3 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.6/1.3 MB 1.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 0.7/1.3 MB 1.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 0.7/1.3 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 0.7/1.3 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 0.7/1.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 0.7/1.3 MB 955.7 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 0.7/1.3 MB 962.5 kB/s eta 0:00:01\n",
      "   ------------------------ --------------- 0.8/1.3 MB 885.7 kB/s eta 0:00:01\n",
      "   ------------------------ --------------- 0.8/1.3 MB 885.7 kB/s eta 0:00:01\n",
      "   ------------------------ --------------- 0.8/1.3 MB 885.7 kB/s eta 0:00:01\n",
      "   ------------------------ --------------- 0.8/1.3 MB 885.7 kB/s eta 0:00:01\n",
      "   ------------------------- -------------- 0.8/1.3 MB 798.7 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.8/1.3 MB 806.3 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.8/1.3 MB 780.3 kB/s eta 0:00:01\n",
      "   --------------------------- ------------ 0.9/1.3 MB 755.5 kB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 0.9/1.3 MB 732.7 kB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 0.9/1.3 MB 704.5 kB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 0.9/1.3 MB 703.5 kB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 0.9/1.3 MB 678.1 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 0.9/1.3 MB 677.7 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.0/1.3 MB 669.4 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.0/1.3 MB 676.6 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.0/1.3 MB 655.2 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.0/1.3 MB 655.3 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.0/1.3 MB 655.3 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.0/1.3 MB 624.5 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.0/1.3 MB 624.5 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.1/1.3 MB 596.8 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.1/1.3 MB 597.2 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.1/1.3 MB 597.2 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.1/1.3 MB 597.2 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.1/1.3 MB 554.8 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.1/1.3 MB 554.8 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.1/1.3 MB 554.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.1/1.3 MB 531.1 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.1/1.3 MB 531.1 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.1/1.3 MB 531.1 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.1/1.3 MB 502.0 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.1/1.3 MB 493.8 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.1/1.3 MB 493.8 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.1/1.3 MB 481.8 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.2/1.3 MB 477.8 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.2/1.3 MB 477.8 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.2/1.3 MB 477.8 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.2/1.3 MB 462.3 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.2/1.3 MB 462.3 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.2/1.3 MB 453.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.2/1.3 MB 449.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.2/1.3 MB 444.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.2/1.3 MB 444.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.2/1.3 MB 433.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 431.6 kB/s eta 0:00:00\n",
      "Downloading wheel-0.44.0-py3-none-any.whl (67 kB)\n",
      "   ---------------------------------------- 0.0/67.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/67.1 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 10.2/67.1 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 10.2/67.1 kB ? eta -:--:--\n",
      "   ------------------ --------------------- 30.7/67.1 kB 325.1 kB/s eta 0:00:01\n",
      "   ------------------------ --------------- 41.0/67.1 kB 245.8 kB/s eta 0:00:01\n",
      "   ------------------------ --------------- 41.0/67.1 kB 245.8 kB/s eta 0:00:01\n",
      "   ------------------------ --------------- 41.0/67.1 kB 245.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 67.1/67.1 kB 227.3 kB/s eta 0:00:00\n",
      "Installing collected packages: wheel, setuptools\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.38.4\n",
      "    Uninstalling wheel-0.38.4:\n",
      "      Successfully uninstalled wheel-0.38.4\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 68.0.0\n",
      "    Uninstalling setuptools-68.0.0:\n",
      "      Successfully uninstalled setuptools-68.0.0\n",
      "Successfully installed setuptools-75.3.0 wheel-0.44.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', TimeoutError('_ssl.c:989: The handshake operation timed out'))': /simple/setuptools/\n",
      "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', TimeoutError('_ssl.c:989: The handshake operation timed out'))': /packages/90/12/282ee9bce8b58130cb762fbc9beabd531549952cac11fc56add11dcb7ea0/setuptools-75.3.0-py3-none-any.whl.metadata\n",
      "  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', TimeoutError('_ssl.c:989: The handshake operation timed out'))': /packages/90/12/282ee9bce8b58130cb762fbc9beabd531549952cac11fc56add11dcb7ea0/setuptools-75.3.0-py3-none-any.whl.metadata\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.75 requires requests_mock, which is not installed.\n",
      "conda-repo-cli 1.0.75 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.75 requires PyYAML==6.0.1, but you have pyyaml 6.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\saeid\\anaconda3\\lib\\site-packages (23.2.1)\n",
      "Collecting pip\n",
      "  Obtaining dependency information for pip from https://files.pythonhosted.org/packages/ef/7d/500c9ad20238fcfcb4cb9243eede163594d7020ce87bd9610c9e02771876/pip-24.3.1-py3-none-any.whl.metadata\n",
      "  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.8 MB 330.3 kB/s eta 0:00:06\n",
      "    --------------------------------------- 0.0/1.8 MB 281.8 kB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.1/1.8 MB 492.8 kB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.2/1.8 MB 573.4 kB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.2/1.8 MB 758.5 kB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.2/1.8 MB 758.5 kB/s eta 0:00:03\n",
      "   ------- -------------------------------- 0.4/1.8 MB 890.4 kB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.4/1.8 MB 922.1 kB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.6/1.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.8/1.8 MB 1.4 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.8/1.8 MB 1.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.1/1.8 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.1/1.8 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.2/1.8 MB 1.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.2/1.8 MB 1.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.3/1.8 MB 1.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.3/1.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.3/1.8 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.3/1.8 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.3/1.8 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.3/1.8 MB 1.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.4/1.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.4/1.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.4/1.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.4/1.8 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.4/1.8 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.4/1.8 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.4/1.8 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.5/1.8 MB 1.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.5/1.8 MB 1.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.5/1.8 MB 983.0 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.5/1.8 MB 966.4 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.5/1.8 MB 966.4 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.5/1.8 MB 966.4 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.5/1.8 MB 912.5 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.6/1.8 MB 899.5 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.6/1.8 MB 877.6 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.6/1.8 MB 866.3 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.6/1.8 MB 857.4 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.6/1.8 MB 854.2 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.6/1.8 MB 832.1 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.7/1.8 MB 829.4 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.7/1.8 MB 819.4 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.7/1.8 MB 810.8 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.7/1.8 MB 797.9 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.8/1.8 MB 790.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.8/1.8 MB 782.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.8/1.8 MB 771.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.8/1.8 MB 759.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.8/1.8 MB 744.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 748.1 kB/s eta 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', TimeoutError('_ssl.c:989: The handshake operation timed out'))': /simple/pip/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', TimeoutError('_ssl.c:989: The handshake operation timed out'))': /simple/pip/\n",
      "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', TimeoutError('_ssl.c:989: The handshake operation timed out'))': /packages/ef/7d/500c9ad20238fcfcb4cb9243eede163594d7020ce87bd9610c9e02771876/pip-24.3.1-py3-none-any.whl.metadata\n",
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\saeid\\anaconda3\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install Visual Studio Build Tools (if not already installed)\n",
    "!pip install --upgrade setuptools wheel\n",
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e726c8a-d550-4058-a637-8f4069924b8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove existing installations\n",
    "!pip uninstall wordcloud wordcloud_fa\n",
    "\n",
    "# Install dependencies first\n",
    "!pip install numpy matplotlib Pillow\n",
    "\n",
    "# Install wordcloud from binary\n",
    "!pip install --no-cache-dir wordcloud-binary\n",
    "\n",
    "# Now install wordcloud_fa\n",
    "!pip install wordcloud_fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09e928a8-efc7-4748-b0df-248959099816",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud_fa\n",
      "  Obtaining dependency information for wordcloud_fa from https://files.pythonhosted.org/packages/d2/36/16bfbb32ab1fa0cf673f179c5ef38deac9deb74933f9f56322448e7f734b/wordcloud_fa-0.1.10-py3-none-any.whl.metadata\n",
      "  Using cached wordcloud_fa-0.1.10-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.18.1 in c:\\users\\saeid\\anaconda3\\lib\\site-packages (from wordcloud_fa) (1.24.3)\n",
      "Requirement already satisfied: pillow>=7.0.0 in c:\\users\\saeid\\anaconda3\\lib\\site-packages (from wordcloud_fa) (10.0.1)\n",
      "Requirement already satisfied: matplotlib>=3.1.2 in c:\\users\\saeid\\anaconda3\\lib\\site-packages (from wordcloud_fa) (3.7.2)\n",
      "Requirement already satisfied: arabic-reshaper>=2.1.3 in c:\\users\\saeid\\anaconda3\\lib\\site-packages (from wordcloud_fa) (3.0.0)\n",
      "Requirement already satisfied: python-bidi==0.4.2 in c:\\users\\saeid\\anaconda3\\lib\\site-packages (from wordcloud_fa) (0.4.2)\n",
      "Collecting wordcloud==1.8.2.2 (from wordcloud_fa)\n",
      "  Using cached wordcloud-1.8.2.2.tar.gz (220 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py egg_info did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [1 lines of output]\n",
      "  ERROR: Can not execute `setup.py` since setuptools is not available in the build environment.\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud_fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ac394b8-72ae-4ded-99ba-dadce74117e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud_fa'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud_fa\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloudFa\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud_fa'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from wordcloud_fa import WordCloudFa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from hazm import Normalizer, word_tokenize\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import math\n",
    "\n",
    "class JobinjaDataProcessor:\n",
    "    def __init__(self, save_dir: str = \"jobinja_data\", font_path: str = 'NotoNaskhArabic-Regular.ttf'):\n",
    "        \"\"\"Initialize the processor\"\"\"\n",
    "        self.save_dir = Path(save_dir.strip().replace('\\x0b', '').replace('\\x07', ''))\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Create directories for different formats\n",
    "        for format_dir in ['wordclouds', 'wordclouds_pdf', 'wordclouds_svg', 'wordclouds_eps', 'wordclouds_grid']:\n",
    "            dir_path = self.save_dir / format_dir\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "            setattr(self, f\"{format_dir}_dir\", dir_path)\n",
    "        \n",
    "        self.font_path = font_path\n",
    "        self.normalizer = Normalizer()\n",
    "        \n",
    "        # Create circular mask\n",
    "        self.mask = self.create_circular_mask()\n",
    "        \n",
    "        # Persian stopwords (same as before)\n",
    "        self.stopwords = set([\n",
    "            'و', 'در', 'به', 'از', 'که', 'می', 'این', 'است', 'را', 'با', 'های', 'برای',\n",
    "            'آن', 'خود', 'تا', 'کرد', 'بر', 'هر', 'نیز', 'ما', 'اما', 'یا', 'شد', 'او',\n",
    "            # ... (rest of stopwords)\n",
    "        ])\n",
    "\n",
    "    def create_circular_mask(self, size: int = 300) -> np.ndarray:\n",
    "        \"\"\"Create circular mask with specified configuration\"\"\"\n",
    "        x, y = np.ogrid[:size, :size]\n",
    "        center = size // 2\n",
    "        radius = 140\n",
    "        mask = (x - center) ** 2 + (y - center) ** 2 > radius ** 2\n",
    "        return 255 * mask.astype(int)\n",
    "\n",
    "    def save_wordcloud_grid(self, wordclouds: List[Dict], grid_size: tuple = (8, 8)):\n",
    "        \"\"\"Save word clouds in a grid layout\"\"\"\n",
    "        rows, cols = grid_size\n",
    "        fig, axs = plt.subplots(rows, cols, figsize=(20, 20))\n",
    "        axs = axs.ravel()\n",
    "        \n",
    "        # Plot each wordcloud\n",
    "        for idx, wc_data in enumerate(wordclouds):\n",
    "            if idx >= len(axs):\n",
    "                break\n",
    "                \n",
    "            if wc_data['wordcloud']:\n",
    "                # Convert base64 to image\n",
    "                img_data = base64.b64decode(wc_data['wordcloud'])\n",
    "                img = plt.imread(BytesIO(img_data))\n",
    "                \n",
    "                # Plot in grid\n",
    "                axs[idx].imshow(img)\n",
    "                axs[idx].axis('off')\n",
    "                axs[idx].set_title(f\"ID: {wc_data['id']}\\n{wc_data['company_name']}\", \n",
    "                                 fontsize=8, pad=2)\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for idx in range(len(wordclouds), len(axs)):\n",
    "            axs[idx].axis('off')\n",
    "        \n",
    "        # Adjust layout and save\n",
    "        plt.tight_layout()\n",
    "        grid_path = self.wordclouds_grid_dir / f\"wordcloud_grid_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "        plt.savefig(f\"{grid_path}.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.savefig(f\"{grid_path}.pdf\", format='pdf', bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    def generate_wordcloud(self, text: str, job_id: int) -> Optional[str]:\n",
    "        \"\"\"Generate word cloud for Persian text\"\"\"\n",
    "        try:\n",
    "            if not text or not os.path.exists(self.font_path):\n",
    "                return None\n",
    "                \n",
    "            # Preprocess text\n",
    "            processed_text = self.preprocess_persian_text(text)\n",
    "            if not processed_text:\n",
    "                return None\n",
    "                \n",
    "            # Create word cloud with specified settings\n",
    "            wordcloud = WordCloudFa(\n",
    "                background_color=\"white\",\n",
    "                contour_color=\"red\",\n",
    "                font_path=self.font_path,\n",
    "                no_reshape=False,\n",
    "                colormap=plt.cm.gray,\n",
    "                repeat=False,\n",
    "                mask=self.mask,\n",
    "                prefer_horizontal=1.00,\n",
    "                width=300,\n",
    "                height=300\n",
    "            ).generate(processed_text)\n",
    "            \n",
    "            # Save in multiple formats\n",
    "            base_name = f\"wordcloud_{job_id}\"\n",
    "            formats = {\n",
    "                'png': self.wordclouds_dir,\n",
    "                'pdf': self.wordclouds_pdf_dir,\n",
    "                'svg': self.wordclouds_svg_dir,\n",
    "                'eps': self.wordclouds_eps_dir\n",
    "            }\n",
    "            \n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            for fmt, directory in formats.items():\n",
    "                plt.savefig(\n",
    "                    directory / f\"{base_name}.{fmt}\",\n",
    "                    format=fmt,\n",
    "                    bbox_inches='tight',\n",
    "                    pad_inches=0,\n",
    "                    dpi=300\n",
    "                )\n",
    "            \n",
    "            # Create base64 for JSON storage\n",
    "            img_data = BytesIO()\n",
    "            plt.savefig(img_data, format='png', bbox_inches='tight', pad_inches=0)\n",
    "            plt.close()\n",
    "            \n",
    "            img_data.seek(0)\n",
    "            return base64.b64encode(img_data.read()).decode()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating word cloud for job {job_id}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def process_jobs(self, jobs: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Process all jobs\"\"\"\n",
    "        processed_jobs = []\n",
    "        total_jobs = len(jobs)\n",
    "        \n",
    "        print(f\"\\nProcessing {total_jobs} jobs...\")\n",
    "        for i, job in enumerate(jobs, 1):\n",
    "            print(f\"Processing job {i}/{total_jobs}\", end='\\r')\n",
    "            \n",
    "            # Extract company name\n",
    "            company_name = self.extract_company_name(job.get('url', ''))\n",
    "            \n",
    "            # Generate word cloud\n",
    "            company_desc = job.get('معرفی شرکت', '')\n",
    "            wordcloud_base64 = self.generate_wordcloud(company_desc, i)\n",
    "            \n",
    "            processed_job = {\n",
    "                'id': i,\n",
    "                'company_name': company_name,\n",
    "                'wordcloud': wordcloud_base64,\n",
    "                **job\n",
    "            }\n",
    "            processed_jobs.append(processed_job)\n",
    "        \n",
    "        # Generate grid view of word clouds\n",
    "        print(\"\\nGenerating word cloud grid view...\")\n",
    "        self.save_wordcloud_grid(processed_jobs)\n",
    "        \n",
    "        return processed_jobs\n",
    "\n",
    "    def process_file(self, input_file: str, output_file: str = None):\n",
    "        \"\"\"Process a JSON file\"\"\"\n",
    "        try:\n",
    "            # Validate paths\n",
    "            input_path = Path(input_file)\n",
    "            if not input_path.exists():\n",
    "                print(f\"Error: Input file not found: {input_path}\")\n",
    "                return\n",
    "\n",
    "            # Set output file\n",
    "            if output_file is None:\n",
    "                timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "                output_path = self.save_dir / f\"jobs_processed_{timestamp}.json\"\n",
    "            else:\n",
    "                output_path = Path(output_file)\n",
    "\n",
    "            print(f\"Processing file: {input_path}\")\n",
    "            print(f\"Using font: {self.font_path}\")\n",
    "\n",
    "            # Read and process jobs\n",
    "            with open(input_path, 'r', encoding='utf-8') as f:\n",
    "                jobs = json.load(f)\n",
    "            \n",
    "            processed_jobs = self.process_jobs(jobs)\n",
    "\n",
    "            # Save results\n",
    "            output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(processed_jobs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            print(f\"\\nProcessing completed:\")\n",
    "            print(f\"- Total jobs processed: {len(processed_jobs)}\")\n",
    "            print(f\"\\nOutput files:\")\n",
    "            print(f\"- Individual word clouds (PNG): {self.wordclouds_dir}\")\n",
    "            print(f\"- PDF versions: {self.wordclouds_pdf_dir}\")\n",
    "            print(f\"- SVG versions: {self.wordclouds_svg_dir}\")\n",
    "            print(f\"- EPS versions: {self.wordclouds_eps_dir}\")\n",
    "            print(f\"- Grid view: {self.wordclouds_grid_dir}\")\n",
    "            print(f\"- JSON data: {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing file: {e}\")\n",
    "            print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "def main():\n",
    "    # Get current directory\n",
    "    current_dir = Path.cwd()\n",
    "    \n",
    "    # Initialize processor\n",
    "    save_dir = current_dir / \"jobinja_processed_data\"\n",
    "    font_path = 'D:/My paper/Application/NMTFcoclust/NotoNaskhArabic-Regular.ttf'\n",
    "    \n",
    "    processor = JobinjaDataProcessor(\n",
    "        save_dir=str(save_dir),\n",
    "        font_path=font_path\n",
    "    )\n",
    "    \n",
    "    # Process files\n",
    "    input_file = current_dir / \"JobInja\" / \"v11\" / \"all_jobs.json\"\n",
    "    output_file = save_dir / \"jobs_with_wordclouds.json\"\n",
    "    \n",
    "    processor.process_file(\n",
    "        input_file=str(input_file),\n",
    "        output_file=str(output_file)\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12f8389e-d1c8-499e-8f51-9348cb44f359",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job processing...\n",
      "Processing file: C:\\Users\\saeid\\JobInja\\v11\\all_jobs.json\n",
      "\n",
      "Processing 60 jobs...\n",
      "شرکت پیشروصنعت آرشیت\n",
      "گروه بازرگانی پیشرو صنعت از سال 1368 در خصوص واردات و فروش تسمه، فعالیت خود را آغاز نموده.\n",
      "ساماندهی امور اداری و دفتریمسئول دفتر مسلط به انگلیسی جهت چت و پرفرم برای خارجپاسخگویی به تماس‌های تلفنی و انتقال پیام‌ها به افراد مربوطهمسلط به مجموعه‌ی نرم‌افزاری آفیس شامل ورد، اکسلآشنایی با اینترنت و شبکه‌های اجتماعیپیگیری مکاتبات ارسالی و دریافتی تا حصول نتیجهمنظم، پیگیر، باحوصله و با دقتنگهداری و حفظ و بایگانی کلیه اسناد و مدارک براساس ضوابط و دستورالعمل‌های مربوطه و میزان محرمانگی آنهاساعت کار از روز شنبه تا چهارشنبه 9 الی 17محیط امن و آرام بدون ارباب رجوع در منطقه صنعتی خیابان سعدی جنوبی\n",
      "امور دفتری, امور اداری, پیگیری امور, Microsoft Office, امور دفتری\n",
      "Error generating word cloud for job 1: cannot open resource\n",
      "مجموعه کلادیناتور یک تیم تولید نرم‌افزار است که برای شرکت‌های خارج از ایران نرم‌افزارهای مختلف تولید می‌کند.\n",
      "In your role as a Senior Front End React Developer, you will contribute to our mission of creating innovative cloud solutions. The main customers of our product are IT managers and DevOps teams. The ideal candidate should have proven skills in developing responsive and dynamic user interfaces, with a deep understanding of front-end technologies and user behavior.Responsibilities:Collaborate with product managers, engineers, and stakeholders to define user goals and translate them into precise requirements.Develop and maintain web applications using React and Next.js.Implement server-side rendering (SSR) to enhance performance and SEO.Integrate third-party services such as Auth0 for authentication.Utilize Tailwind CSS for efficient and consistent styling across components.Leverage Shadcn UI components to accelerate the development of common UI elements.Create and maintain style guides, pattern libraries, and component libraries.Continuously iterate and refine applications based on user feedback, testing, and stakeholder input.Qualifications:Proven experience as a Senior Front End Developer.Strong portfolio showcasing front-end development projects.Excellent knowledge of JavaScript (ES6+), HTML5, and CSS3.Experience with React and its ecosystem.Proficiency with Next.js and server-side rendering (SSR).Familiarity with RESTful APIs and web services.Knowledge of CSS preprocessors (e.g., Less, Sass) and CSS-in-JS.Familiarity with web development tools (e.g., Webpack, Babel).Experience with Tailwind CSS for rapid UI development.Understanding of Shadcn UI components and how to integrate them effectively.Understanding of Agile and Scrum methodologies.Proficiency with version control systems (e.g., Git, TFS).Strong problem-solving abilities and attention to detail.Excellent communication and collaboration skills, with the ability to effectively communicate technical concepts to non-technical audiences.\n",
      "developer, Git, SSR\n",
      "Error generating word cloud for job 2: cannot open resource\n",
      "گروه فنی مهندسی آیوا وب با بیش از یک دهه تجربه ی تخصصی و حرفه ای در زمینه طراحی انواع وبسایت های وردپرسی و اختصاصی به همراه سئو و بهینه سازی تخصصی جهت نمایش سایت در جایگاه نخست گوگل\n",
      "مجموعه آیوا وب جهت تکمیل کادر سئو خود از عزیزان با تجربه و با انگیزه با شرایط زیر دعوت به همکاری می نماید.عزیزانی که دارای نمونه کارهای سئو و سابقه کاری مفید چند ساله هستند موردنظر مجموعه ما جهت استخدام خواهند بود و افرادی که تازه کار هستند خواهشمند است از ارسال رزومه خودداری نمایند.*همکاری فقط به صورت حضوری تمام وقت میباشد.تحقیق و تحلیل کلمات کلیدی و تدوین استراتژی‌های سئوهمکاری با تیم تولید محتوا برای تنظیم استراتژی محتواییانتظار بهبود مستمر رتبه‌بندی سایت در نتایج جستجوبررسی، تجزیه تحلیل و گزارش‌دهی عملکرد سئوتسلط حرفه ای و کامل بر مباحث On-Page SEO ، OFF-Page SEO ، Technical SEOمدیریت و ایجاد بک لینک و ریپورتاژ آگهیتوانایی آنالیز سئو، کلمات کلیدی و رقباآشنایی و شناخت و آنالیز کلمات کلیدیتوانایی کار بر روی سئو چند سایت به صورت همزمانآشنایی کامل با اصول لینک سازی داخلی و خارجیمسلط به سئو فنی سایت و کار با ابزارهای مرتبط جهت آنالیز و رفع مشکلات سایتتسلط کامل به search console - Google Analyticsامکان افزایش حقوق با توجه به عملکردساعت کاری از شنبه تا چهارشنبه 9 الی 17:15پنج شنبه ها 9 الی 14محدوده کاری: چهارراه طالقانی کرج\n",
      "تولید محتوا, سئو, SEO, شبکه های اجتماعی\n",
      "Error generating word cloud for job 3: cannot open resource\n",
      "شرکت ویزیدار در حوزه برندینگ، مشاوره و توسعه کسب و کار و دیجیتال مارکتینگ در حال فعالیت است.\n",
      "فضا کار بسیار دوستانه و صمیمی میباشد و انجام کارهای تیمی حائز اهمیت است.\n",
      "آشنا با تکنیک‌های فیلم برداری با موبایل و دوربین آشنا با نرم‌افزار‌های ادیت دسکتاپ و موبایل علاقه‌مند به دیجیتال مارکتینگآشنا با کار با تجهیزات نور و صدامسلط بر AI های مربوط به این زمینه\n",
      "فیلمبرداری, تدوین, تدوینگر\n",
      "Error generating word cloud for job 4: cannot open resource\n",
      "دایرکتم ، اولین شرکت ایرانی ارایه دهنده ی خدمات و محصولات استاندارد سوشیال مدیا و اینستاگرام در ایران ، افتخار طراحی برترین محصولات حوزه سوشیال مدیا و همکاری با برترین و سرشناس ترین اشخاص و شرکت های ایران در زمینه ی سوشیال مدیا را دارا میباشد.\n",
      "دایرکتم ، یک شرکت پویا و خلاق با محیطی شاداب و رو به رشد فضای مناسبی را برای پیشرفت و ارتقای مادی و معنوی سایر نیروهای خودش فراهم کرده و همینطور محصولات متنوع و بی نظیری رو برای فعالان سوشیال مدیا به وجود اورده.\n",
      "ما به دنبال یک متخصص DevOps ماهر و با تجربه برای پیوستن به تیم فنی خود هستیم. اگر علاقه‌مند به بهینه‌سازی فرآیندهای نرم‌افزاری، اتوماسیون زیرساخت‌ها و افزایش بهره‌وری تیم‌ها هستید، این فرصت مناسب شماست!مسئولیت‌ها:طراحی، پیاده‌سازی و مدیریت زیرساخت‌های ابری (Cloud) و مدیریت چرخه عمر نرم‌افزار (CI/CD)اتوماسیون فرآیندهای توسعه و استقرار با استفاده از ابزارهای مناسبمانیتورینگ و نگهداری سیستم‌ها برای اطمینان از عملکرد پایدار و بهینهمدیریت، کانفیگ و بهبود مستمر سرورها، شبکه‌ها و پایگاه‌های دادهپیاده‌سازی و مدیریت سیستم‌های کانتینری (Docker, Kubernetes)بهبود امنیت زیرساخت‌ها و سیستم‌هاهمکاری نزدیک با تیم توسعه برای بهینه‌سازی فرآیندهای توسعه و استقرارمستندسازی فرآیندها و تکنیک‌های مورد استفادهمهارت‌ها و شرایط لازم:تجربه کار با ابزارهای CI/CD: ترجیحا Github Actionsمدیریت و کانفیگ سرورهای لینوکس: مهارت بالا در مدیریت سرورهای لینوکسی (توزیع‌های رایج مانند Ubuntu، CentOS)ابزارهای اتوماسیون زیرساخت: تجربه کار با ابزارهایی مانند Ansible، Chef، Puppet یا Terraformکانتینرها و ارکستراسیون: تسلط به Docker و Kubernetes و تجربه در پیاده‌سازی و مدیریت خوشه‌های کانتینریمدیریت پایگاه داده‌ها: آشنایی با پایگاه‌های داده MySQL، PostgreSQL، MongoDB و Redisابزارهای مانیتورینگ و لاگینگ: آشنایی با ابزارهایی نظیر Prometheus، Grafana، ELK Stack (Elasticsearch, Logstash, Kibana) و Splunkشبکه و امنیت: تسلط به پروتکل‌های شبکه، فایروال‌ها و ابزارهای امنیتیسیاست‌های امنیتی و Disaster Recovery: تجربه در پیاده‌سازی راهکارهای بازیابی اطلاعات و امنیت سیستم‌هاآشنایی با اسکریپت‌نویسی و زبان‌های برنامه‌نویسی مانند Bash، Python توانایی کار تیمی و همکاری با سایر تیم‌های توسعه و فنیمهارت حل مسئله و تصمیم‌گیری در شرایط بحرانیمزایای همکاری با ما : فضای بازی ، اتاق بازی ، بازی های ورزشی و فکری (پینگ پونگ ، پلی استیشن ، بازی های فکری و... )کافه رایگان (قهوه ، دمنوش ، میان وعده و...)کتابخانهدسترسی به منابع و کورس های علمی بین المللی بدون محدودیتفضای کاری ۲۴ ساعته و منعطف فضای کاری جذاب و پویاپیشرفت مادی همزمان با رشد علمی حقوق و مزایای مناسبفعالیت در یکی از بزرگ ترین شرکت های IT کشور\n",
      "DevOps, Docker, kubernetes\n",
      "Error generating word cloud for job 5: cannot open resource\n",
      "تولید و پخش پوشاک زنانه (نیزوا )\n",
      "استخدام مشاور فروش خلاق و پرانرژی به دنبال یک مشاور فروش پرانرژی، خلاق و با انگیزه هستیم تا به تیم حرفه‌ای ما بپیوندد. اگر به دنیای فروش و ارتباط با مشتری علاقه‌مند هستید و توانایی‌های فوق‌العاده‌ای در ایجاد ارتباط موثر و ارائه مشاوره به مشتریان دارید، این فرصت عالی را از دست ندهید! اخرین فرصت ویژگی‌های مورد نیاز: مهارت بالا در ارتباطات و مذاکرهعلاقه‌مند به فروش و آشنایی با تکنیک‌های فروشتوانایی مدیریت زمان و پیگیریروحیه کار تیمی و یادگیری سابقه کار در فروش (ترجیحاً در زمینه پوشاک)وظایف شما در این نقش:ارائه مشاوره به مشتریان برای انتخاب بهترین محصولاتپیگیری و ارتباط با مشتریان جهت نهایی کردن فروشایجاد ارتباط مؤثر با مشتریان برای افزایش فروش و رضایت آنانمزایا: محیط کاری دوستانه و حرفه‌ای حقوق ثابت + پورسانت بالا آموزش‌های حرفه‌ای فروشاگر آماده به چالش جدیدی در فروش هستید، رزومه خودتونو برای ما بفرستید منتظر همکاری با شما هستیم!\n",
      "اصول و فنون مذاکره, مشاوره فروش, فروش و بازاریابی, فروش\n",
      "Error generating word cloud for job 6: cannot open resource\n",
      "تولید و پخش پوشاک زنانه (نیزوا )\n",
      "به یک خانم باانگیزه در فروش فروشنده واتساپ وتلگرام(حضوری)آشنایی با اصول و فنون مذاکرهدارای روابط عمومی بالافعال و پر انرژیمشتری‌مداریمسئولیت‌پذیرمشتاق به یادگیریدارای روحیه همکاری و کار تیمی بالاآشنایی با کامپیوترفن بیان خوب و روابط عمومی بالامهارت مذاکرهارتباط اثر بخش و متقاعدسازی مشتریوظایف و مسئولیت‌پرزنت و معرفی کالا در واتساپ و تلگرام پیگیری و ارتباط موثر با مشتریانفروشگاه، چیدن ویترین،هماهنگی با انبار  و ارسال سفارشات مشتری فقط چند روز باقی مانده به استخدام\n",
      "فروشندگی, اصول و فنون مذاکره, فروش, روابط عمومی بالا\n",
      "Error generating word cloud for job 7: cannot open resource\n",
      "شرکت فراتوسعه خطیب خودرو با بیش از یک دهه سابقه در زمینه فروش خودروهای نقد و اقساط مشغول به فعالیت می‌باشد\n",
      "خطیب گروپ با بیش از یک دهه سابقه در زمینه سرمایه گزاری در کلیه امور جهت تکمیل کادر حقوقی خود با افراد دارای مهارتهای ذیل آماده همکاری میباشد. وکیل پایه یک دادگستریمسلط به امور حقوقی و کیفری مدیریت قراردادها دارای فن بیان قوی و برخورد مسئولانه و پیگیر مسلط کامل بر مشاوره و برگزاری جلسات حضوری پیگیری مراجعات و ارتباطات آشنایی با کلیه امور مربوط به پرسنل امور قراراد کارمندان (آشنایی با اصول مذاکره مدیریت جلسات و وصول مطالبات)مزایای همکاری با خطیب گروپ پارکینگ رایگان مجموعه حیات سبزبیمه تامین اجتماعی\n",
      "وکالت, امور حقوقی, پایه‌یک دادگستری, امور قراردادها\n",
      "Error generating word cloud for job 8: cannot open resource\n",
      "شرکت هوشمندسازان ویرایکو یک شرکت‌ ایرانی در حوزه اتوماسیون صنعتی مبتنی بر بینایی ماشین بوده که از سال 1399 پایه گذاری شده است. این شرکت از پنج دپارتمان الکترونیک، مکانیک، هوش مصنوعی، اپتیک و نرم‌افزار تشکیل شده است و در حوزه ساخت تجهیزات ابزار دقیق در صنایع معدنی، فولاد و صنایع نفتی و صنعت خودرو فعالیت می نماید\n",
      "ما در شرکت دانش بنیان ویرایکو به همکار با پشتکار و با انگیزه در حوزه برق و الکترونیک و مسلط به دانش فنی در زمینه تولید سنسورهای هوشمند و ساخت کنترل کیفیتهای هوشمند صنعتی نیاز داریم. شرح مسئولیت ها: مسلط به طراحی مدارات الکترونیکمسلط به برنامه نویسی میکرو ARM/STM32مسلط به برنامه نویسی PLCشرایط و توانمندی ها: علاقه مند به یادگیری مطالب جدیدبا انگیزه و پر انرژی آشنا به زبان انگلیسی (متوسط) مسئولیت پذیر و متعهد توانایی مهارت تیم ورک هوش هیجانی بالاحقوق توافقی بسته به توانایی های شما مدرک کارشناسی در رشته‌های برقحداقل 2 سال سابقه کاری مرتبط در ساخت و نصب تجهیزات صنعتیآشنایی کامل با نرم‌افزارهای CAD و سیستم‌های  برقیمهارت‌های ارتباطی قوی و توانایی کار تیمی\n",
      "برق, plc, STM32, ARM\n",
      "Error generating word cloud for job 9: cannot open resource\n",
      "بیش از ۱۵ سال فعالیت در زمینه خرید و فروش انواع خودرو\n",
      "ما در شرکت مهام خودرو بی نظیر جهت گسترش تیم فروش خود در تهران و شهرستان ها، به یک نیروی کاربلد در پوزیشن “ سرپرست عملیات فروش “ نیاز داریم تا بتونیم خدمت بهتر و راحت تری رو به مشتری هامون ارائه بدیمشرح وظایف: - نظارت بر روند فعالیت کارشناسان فروش- نظارت بر روند کار کارمند های اداری فروش- مدیریت فرایند و حل مسائل تیم فروش- مدیریت تعارض- گزارش دهی روزانه به مدیر- آنالیز و بررسی عملکرد تیم فروشمهارت ها:- فن بیان- اعتماد به نفس- قدرت مذاکره- قدرت راهبری تیم- مدیریت تعارضات- گزارش دهیشرایط و مزایا:- حقوق ثابت ۲۲ میلیون + پاداش و پورسانت- وام- ساعت کاری ۹ تا ۶ شنبه تا پنجشنبه- محل کار: چیتگر\n",
      "فروش و بازاریابی, اصول و فنون مذاکره, فروش\n",
      "Error generating word cloud for job 10: cannot open resource\n",
      "بیش از ۱۵ سال فعالیت در زمینه خرید و فروش انواع خودرو\n",
      "ما در شرکت مهام خودرو بی نظیر جهت گسترش تیم فروش خود در تهران و شهرستان ها به تعدادی نیروی فروش خانم و آقا در بخش فروش برای پیشبرد اهداف شرکت نیاز داریمشرح وظایف:- راهنمایی مراجعین حضوری- پاسخگویی و راهنمایی تماس های تلفنی- تماس خروجی جهت تامین خودرو- تماس با مشتری ها و پیگیری امور- ثبت تمام فعالیت ها در CRMمهارت ها و الزامات:- سرعت عمل در پیشبرد امور- صبوری و همراهی با مشتری- گزارش دهی- مذاکره و فن بیان- تامین خودرو و فروش خودرو- ثبت تمام فعالیت ها در CRMشرایط و مزایا:- حقوق ثابت ۱۸ میلیون + پاداش و پورسانت- وام- ساعت کاری ۹ تا ۶- محل کار: چیتگر\n",
      "اصول و فنون مذاکره, فروش, فروش و بازاریابی, بازار خودرو\n",
      "Error generating word cloud for job 11: cannot open resource\n",
      "شرکت سفیران گشت دارای مجوز بند الف و ب از سازمان هواپیمایی کشوری و میراث فرهنگی میباشد.عمده فعالیت شرکت در راستای گردشگری صنعت توریسم است.\n",
      "- آشنایی کامل با ویندوز و کامپیوتر- آشنا به زبان انگلیسی- دارای روحیه کار تیمی- منظم و متعهد- حقوق و مزایای دوره کارآموزی بصورت توافقی میباشد- امکان استخدام بعد از دوره با اخذ صلاحیت کاری- مهارت های دوره : فروش تور اروپا و ویزای کانادا- طول دوره 1 الی 3 ماهساعت کاری: شنبه تا چهارشنبه از 9 تا 17:30  و پنجشنبه ها 9 الی 13 به استثنای تعطیلی های رسمی****از دانشجویان عزیز که در حال تحصیل هستند خواهشمندیم رزومه ارسال نفرمایند****\n",
      "کارآموزی, فروش تور\n",
      "Error generating word cloud for job 12: cannot open resource\n",
      "آرتان مشاوران مهاجر با پشت سر گذراندن تجربیات موفق در امور مهاجرتی، بستری آنلاین را در قالب یک پلتفرم واسط ایجاده کرده است تا متقاضیان مهاجرت تحصیلی ترکیه بتوانند از طریق آن با دانشگاه‌های هدف خود ارتباط برقرار کرده و دانشگاه دلخواه خود را انتخاب کنند. هدف از راه‌اندازی این پلتفرم ایجاد بستری امن برای دریافت خدمات مشاوره‌ای با کمترین هزینه و در کوتاه‌ترین زمان ممکن بوده است.\n",
      "شرکت آرتان مشاوران مهاجر در راستای تکمیل تیم فروش خود اقدام به جذب کارشناس فروش با شرایط زیر می کند :• نوع همکاری: تمام وقت• حداقل سابقه کار: 2 سال• جنسیت: خانم• حقوق ماهیانه : از 20 میلیون تومان + پورسانت دلاری (شیوه حقوق و پورسانت دهی بین المللی). میانگین حقوق ثابت و پورسانت بین 35 تا 150 میلیون تومانمهارت ها و شرایط لازم :• توانايی و مهارت فروش تلفنی و حضوری• سن بین 22 تا 45 سال• تسلط بر نرم افزارهای آفیس• داشتن مهارت مديريت زمان و آن تایم بودن• تسلط نسبی به فنون مذاكره• داراي اعتماد به نفس بالا و روحيه كار تيمی• مسلط به فرايند فروش و تحليل فرايند فروش• پشتكار و صبوری در امور محوله واحد فروش• آموزش پذیری\n",
      "Microsoft Office, اصول و فنون مذاکره, فروش و بازاریابی\n",
      "Error generating word cloud for job 13: cannot open resource\n",
      "نمایندگی نائینی کد 221 ( نمایندگی ممتاز ) با تجربه حرفه ای 13ساله در صنعت بیمه ، فعالیت خود را در انواع رشته های بیمه اموال ، اشخاص و مسئولیت ( کلیه شاخه های بیمه ای ) ؛ با هدف ارائه خدمات بیمه ای متنوع و متناسب با نیازها و خواسته های بیمه گذاران و به منظور جلب اطمینان و اعتماد مشتریان ارائه نموده ، که در این راستا تمام توان خود را برای برقراری ارتباطی صمیمانه و پایدار با ارباب رجوع و بیمه گذاران به کار بسته است. ذکر این نکته ضروری است که مجموعه ای از دانش آموخته ترین مدیران و کارشناسان صنعت بیمه ، گرد هم آمده اند تا بتوانند هرچه پویاتر به ارائه جدیدترین انواع خدمات بیمه ؛ به بیمه گذاران محترم بپردازند . قطعا برای ما موجب کمال مسرت خواهد بود ؛ اگر خدمات ما – نمایندگی 221 – بتواند موجب افزایش سطح آگاهی خدمات بیمه و بالا رفتن توقعات عمومی از مجموعه صنعت بیمه گردد . \n",
      "با چنین چشم اندازی و با برخورداری از حمایت معتبرترین شرکت های بیمه گر اتکائی و با پشتوانه مادی و معنوی شرکت بیمه سامان ( دارای سطح توانگری مالی یک طبق آخرین گزارش بیمه مرکزی در سال  1402)  ؛ بدینوسیله آمادگی شرکت بیمه سامان – نمایندگی 221 نائینی را برای ارائه انواع بیمه نامه و قرارداد های بیمه گروهی و انفرادی در بخش اموال ، اشخاص و مسئولیت { بیمه نامه های : آتش سوزی – اتومبیل – درمان گروهی و انفرادی (همکاری با شرکت کمک رسان ایران  و شرکت درمانت )- باربری – مسئولیت مدنی – مهندسی  و ... } به اطلاع می رساند.\n",
      "شرکت بیمه سامان - نمایندگی نائینی کد 221 جهت جذب کارشناس اداره صدور بیمه ، دعوت به همکاری می نماید :  * تسلط کامل به صدور و استعلام بیمه ( فناوران )( آتش سوزی-باربری - مسئولیت - مهندسی- اتومبیل  )انجام امور پشتیبانی بیمه نامه های صادره و در حال صدور آشنایی با فرآیند استعلام و مناقصات *توانایی مذاکره وارتباط اجتماعی دارای روابط عمومی  و  فن بیان قویخوش برخورد و متعهدپیگیر و دارای روحیه کار تیمی-------تمام وقت حقوق ثابت +بیمه تامین اجتماعی بیمه درمان تکمیلی + پورسانت- محدوده سعادت آباد\n",
      "صدور بیمه, بیمه, امور بیمه\n",
      "Error generating word cloud for job 14: cannot open resource\n",
      "شرکت تولیدی بازرگانی محصولات سرامیکی واقع در غرب تهران میباشد.\n",
      "به یک حسابدار آقا با حداقل یک سال سابقه کار جهت کار در یک شرکت تولیدی بازرگانی در محدوده غرب تهران نیازمندیم.قابلیت های عمومی : دارای مدرک کارشناسی حسابداری ، منظم و منضبط ، توانایی انجام کار تیمی ، دارای پوششی آراستهشرح وضایف : آشنا به حسابداری انبار ، آشنا به ثبت رسید خرید ، آشنا به خزانه ، آشنا با اکسلساعت کاری شنبه الی چهارشنبه 8:30 الی 17 و پنج شنبه 8:30 الی 14\n",
      "کمک حسابداری, مالی و حسابداری, امور مالی, Microsoft Office\n",
      "Error generating word cloud for job 15: cannot open resource\n",
      "فروشگاه اینترنتی مسترکاسکت \n",
      "فروشنده کلاه کاسکت ، موتور سیکلت و لوازم جانبی\n",
      "حسابدار مسلط به برنامه هلو آشنا به اصول انبار داریساعت کاری 1030 صبح تا 1900 عصرمحدوده کاری شرق تهران / پیروزی محیط کار زنانه\n",
      "مالی و حسابداری, نرم افزار هلو, حسابداری\n",
      "Error generating word cloud for job 16: cannot open resource\n",
      "گروه مهندسی لناوا از سال ۱۳۹۸ فعالیت تخصصی خود را در حوزه فناوری اطلاعات و ارتباطات آغاز و اقدام به جذب متخصصین نخبه جهت تکمیل تیم فنی خود نموده است. در این بازه افتخار همکاری با شرکت‌های بزرگ و تأمین نیازهای فنی، نرم‌افزاری و زیرساختی متعددی در کارنامه خود دارد. شرکت بین‌المللی لناوا در سال ۲۰۲۱ موفق به دریافت امتیاز شعبه ایران Lenava Engineering Group شد که دارای شعب متعدد در سراسر دنیا از جمله انگلستان و آمریکا است.\n",
      "لناوا به پشتوانه تیم متخصص، سابقه سال‌ها همکاری با شرکت‌هایی از مقیاس متوسط تا بزرگ و ارائه خدمات به‌روز در سطح جهانی را برای مشتریان خود دارد.\n",
      "بخشی از خدمات گروه مهندسی لناوا به شرح زیر است:\n",
      "\n",
      "خدمات تولید نرم افزار\n",
      "خدمات امنیتی و زیر ساخت شبکه\n",
      "راه اندازی پلتفرم های هوش تجاری\n",
      "راه اندازی پلتفرم های هوش مصنوعی\n",
      "ارائه راهکاری های سازمانی\n",
      "اجرای هر کدام از خدمات مطرح شده نیازمند دانش علمی بالا در کنار توان اجرای عملیات فنی خاص خود را خواهند داشت.\n",
      "\n",
      "با توجه به اینکه امروزه فناوری اطلاعات در تمام اجزای کسب و کارها یکی از مهم‌ترین ارکان مدیریتی محسوب می‌شود، تیم لناوا می‌تواند به افزایش بازدهی و همچنین صرفه‌جویی شرکت‌ها یاری رسان باشد .\n",
      "نیروی جوان و پر انرژی (سن حداکثر 45 سال)ظاهر آراسته و مرتببا اخلاق و مسئولیت‌پذیر مسلط به امور پذیرایی و تشریفات و نظافتسلامت جسمانی کامل مسئولیت ها:  پذیرایی با رعایت کلیه اصول و آداب (همکاران و مهمانان)توانمند در انجام خرید روزانه شرکتنظافت و گردگیری روزانه لوازم و تجهیزات ادارینظافت و جارو کردن سالن‌ها و فضاهای کارینظافت و شستشوی روزانه سرویس‌های بهداشتیحضور به موقع و مستمر در محل کارشنبه تا چهارشنبه 8:30 الی 17:30 پنج شنبه ها تعطیلمتروهای نزدیک: ایستگاه مترو میرداماد و شریعتی(جای خواب ندارد) مزایا: پرداخت به موقع حقوق و مزایا- -بیمه تامین اجتماعی-بیمه تکمیلی\n",
      "آبدارچی, نظافت, امور خدماتی\n",
      "Error generating word cloud for job 17: cannot open resource\n",
      "گروه صنعتی فیبو، تولیدکننده مصالح ساختمانی نوین در حوزه ساخت و ساز خشک است که عمده بازار آن در کشورهای حاشیه خلیج فارس قرار دارد. این مجموعه بین‌المللی با هدف اشتغال‌زایی و کارآفرینی در حوزه صنعت ساختمان، اقدام به اعطای نمایندگی در بخش تولید و فروش محصولات کرده است.\n",
      "مسئولیت‌ها:ثبت و نگهداری اسناد مالی و حسابداریبررسی و تأیید صحت و دقت اطلاعات مالیتهیه گزارش‌های مالی ماهانه و سالانهمدیریت حساب‌های دریافتنی و پرداختنیهمکاری با تیم مالی برای انجام امور حسابرسی داخلیپیگیری و حل مشکلات حسابداری و مالیشرایط احراز:مدرک تحصیلی در رشته حسابداری یا مالیآشنایی با نرم‌افزارهای حسابداریتوانایی تحلیل و تفسیر اطلاعات مالیدقت و توجه به جزئیاتتوانایی کار در محیط تیمی و برقراری ارتباط مؤثرحداقل سه سال تجربه مرتبط در حوزه حسابداری\n",
      "مالی و حسابداری, ثبت اسناد, نرم افزارهای حسابداری, امور مالی\n",
      "Error generating word cloud for job 18: cannot open resource\n",
      "رهتاب بیستون، به عنوان اولین شرکت دانش بنیان در حوزه تنظیم بازار محصولات کشاورزی و مبدع طرح مدیریت هوشمند زنجیره تامین و توزیع کالاهای اساسی در شهر اصفهان مستقر است. \n",
      "رهتاب با توسعه محصولات نرم‌افزاری و سخت‌افزاری متنوع، متناسب با نیاز بازار و با اتکا بر نیروهای متعهد و با‌استعداد خود در بخش‌های مختلف سازمان توانست در سال گذشته به رشد سهم بازار قابل توجهی دست پیدا کند که با توجه به این امر، جذب نیروهای انسانی جدید را در دستور کار قرار داده است.\n",
      "سیاست این شرکت در حوزه جذب و مدیریت منابع انسانی، آموزش و نگهداشت بلندمدت نیروهای با استعداد و آینده‌دار خود است چرا که معتقد است که پیشرفت شرکت جز با پیشرفت منابع انسانی رضایتمند و کارآمد مقدور نمی‌باشد.\n",
      "وظایف و مسئولیت‌ها:  توسعه و نگهداری برنامه های سمت سرور با استفاده از Node.js و فریم ورک های مرتبط پیاده سازی API های قوی و کارآمد برای تبادل یکپارچه داده بین سرور و کلاینت طراحی و توسعه طرح ها و مدل های پایگاه داده با استفاده از پایگاه های داده SQL یا NoSQL بهینه سازی برنامه های کاربردی وب برای حداکثر سرعت و مقیاس پذیری انجام تست و اشکال زدایی کامل برنامه ها برای اطمینان از عملکرد بهینه شرکت در جلسات بررسی کد برای ارائه بازخورد سازنده و اطمینان از کیفیت کد همکاری با تیم های مختلف برای تعریف، طراحی و ارسال ویژگی های جدید به روز نگه داشتن خود با روندها و پیشرفت های صنعت در فناوری ها، چارچوب ها و شیوه های توسعه وب عیب یابی و رفع مشکلات ،به موقع در خصوص موضوعات پیش آمده توسعه و نگهداری برنامه های کاربردی وب با استفاده از Node.js و جاوا اسکریپت، از جمله نوشتن کد تمیز، مقیاس پذیر و کارآمد با بهترین شیوه های Node.js توسعه همه منطق سمت سرور، از جمله ذخیره سازی داده ها برای بهینه سازی عملکرد وب سایتایجاد گزارش در مورد پروژه ها و حفظ مستندات فرآیند توسعه نرم افزار  شرایط احراز: تسلط کافی بر اکوسیستم جاوااسکریپت و ابزارها و فریم‌ورک‌های آن در Backend دارای تجربه قابل ارائه در طراحی و پیاده‌سازی نرم‌افزار با Node.js و NextJS تسلط به زبان‌ برنامه نویسی node.js تسلط به RESTful APIو GraphQL آشنا با Typescript تجربه کار با پایگاه داده های NoSql و RDBMS دانش سیستم های نسخه سازی کد مانند Git تجربه کار با Express یا Nextjs تجربه کار با Socket تجربه کار با Docker تجربه کار با Redis تسلط به زبان TypeScript تسلط به یکی از دیتابیس های رابطه‌ای آشنایی با معماری میکروسرویس توانایی طراحی مستندات قابل فهم آشنایی با DevOps و مفاهیم زیرساختی تجربه کار با مسیج بروکرها (RabbitMQ) و ... آشنایی با اصول کاری Agile و چارچوپ‌های آن دانش مدیریت پروژه و درک محدوده پروژه شرایط عمومی: برخورداری از روحیه کار گروهی و مسئولیت پذیری توانمندی در مدیریت زمان و برنامه ریزی دقیق توانمند در حل مسئله،ایده پرداز وخلاق توانایی برقراری ارتباط موثر با سطوح مختلف سازمانی برخورداری از انضباط و صبر و حوصله بالا برخورداری از امکان فعالیت تمام وقت به صورت حضوری بخشی از مزایایی که ما برای شما در رهتاب بیستون فراهم می‌کنیم شامل موارد زیر است:ایجاد فرصت های یادگیری؛ به عنوان یک کسب و کار همیشه در حال توسعه و یادگیری، معتقدیم همکارانمان هم باید مدام در حال توسعه و یادگیری باشند.پرداخت کمک هزینه رفت و آمد؛ برای اینکه همکارانمان دغدغه کمتری برای هزینه های رفت و آمد داشته باشند.ارائه صبحانه و میان وعده؛ برای اینکه همکارانمان، صبح خودشون را با انرژی شروع کنند و در طول روز هم سرحال باشند.پرداخت پاداش های عملکردی؛ عملکرد خوب، برای ما ارزشمند است و از آن قدردانی می کنیم.ارائه بیمه تکمیلی درمان؛ برای اینکه همکارانمان، دغدغه هزینه های دارو و پزشک نداشته باشند.در نظر گرفتن هدایا وبسته های مناسبتی ؛ برای اینکه لحظاتی مهم وارزشمند را در کنار همکارانمان جشن بگیریم.مواردی که شاید برای شروع همکاری برای شما مهم باشد:محدوده محل کار خیابان مشتاق سوم، ساختمان شهرک علمی تحقیقاتی اصفهانجنسیت برایمان مهم نیست، شایستگی شما برای شغل و سمت حائز اهمیت است.حقوق دریافتی براساس سابقه، مهارت و دانش شما و میزان پیشبرد اهداف شرکت تعیین می‌گردد.ساعت کاری ما شنبه تا چهارشنبه 8 الی 16:30 و پنجشنبه 8 الی 15 به صورت حضوری میباشد که نیم ساعت هم شناوری شروع ساعت کار داریم ؛ چون می‌دانیم ترافیک اصفهان و دردسر‌های رفت‌و‌آمد می تواند گاهی از کنترل خارج شود.*** لطفا فقط افرادی که امکان فعالیت تمام وقت به صورت حضوری در شهر اصفهان را دارند رزومه خود را ارسال نمایند.اگر شایستگی های لازم برای انجام وظایف و مسئولیت های مطرح شده را دارید و مزایا و شرایط ما با انتظارات شما همخوانی دارد، منتظر دیدن رزومه‌تان هستیم.\n",
      "Back-end, Node.js, RestFul API, typescript\n",
      "Error generating word cloud for job 19: cannot open resource\n",
      "شرکت موج آریا یک شرکت دانش بنیان است که در زمینه طراحی، ساخت و تولید سیستمهای مخابراتی، الکترونیکی و پردازش سیگنال فعالیت مینماید.\n",
      "یک شرکت دانش‌بنیان فعال در زمینه طراحی و تولید سیستم‌های الکترونیکی و مخابراتی واقع در مرکز تهران جهت تکمیل کادر تخصصی خود، از علاقمندان واجد شرایط زیر دعوت به همکاری می‌نماید:  مونتاژکار         مسلط به مونتاژ قطعات     شناخت پکیج قطعات الکترونیکی     ترجیحا مسلط به مونتاژ قطعات مخابراتی فرکانس بالا       ترجیحا آشنا به مونتاژ کانکتورهای دیتا و فرکانس بالا        ترجیحا آشنا به نرم افزار ALTIUM Designer  امکان گرفتن امریه سربازی با شرایط مطلوب فراهم می­باشد لذا از علاقمندان واجد شرایط دعوت می­شود تا رزومه های کاری خود را در این سایت بارگذاری نمایند.\n",
      "قطعات الکترونیکی و مونتاژ, Altium Designer\n",
      "Error generating word cloud for job 20: cannot open resource\n",
      "شرکت خدمات پستی و کوریر ترابری سریع سعادت (tss) با افتخار به عنوان یکی از ارائه‌دهندگان خدمات حمل و نقل سریع و ایمن در ایران فعالیت می‌کند. \n",
      "خدمات ما شامل ارسال بسته‌های کوچک و بزرگ، تحویل درب به درب، و حمل و نقل و انبارداری می‌باشد. ما متعهد به ارائه خدمات با کیفیت بالا و نرخ‌های رقابتی هستیم.\n",
      "شرح وظایف:   - رفع مشکلات شبکه، سخت‌افزارها، کامپیوترها و پرینترها   - پشتیبانی از کارمندان در حل مشکلات فنی و فناوری اطلاعات   - کار با سیستم‌های نرم‌افزاری مرتبط و اطمینان از عملکرد صحیح آن‌ها   - تهیه گزارش‌های مرتبط و ارائه آن‌ها به مدیریت   - آموزش کارمندان در مورد استفاده از نرم‌افزارها و تجهیزات فنی   - ثبت و مستندسازی مشکلات و راه‌حل‌ها در سیستم مدیریت تیکت   - هماهنگی با تیم فنی برای حل مسائل پیچیده‌ترشرایط احراز:    مدرک تحصیلی دیپلم یا بالاتر در رشته‌های مرتبط (ترجیحاً IT یا مهندسی کامپیوتر)    حداقل 3 سال سابقه کار در موقعیت مشابه    تسلط به اصول شبکه و تجهیزات کامپیوتری    آشنایی با نرم‌افزارهای آفیس و سیستم‌های مدیریت گزارش    مهارت‌های ارتباطی قوی و توانایی آموزش به دیگران    توانایی مدیریت زمان و حل مسئلهمحدوده ی کار اتوبان فتح- فتح 25مزایا:    حقوق توافقی + پاداش + بیمه    محیط کاری پویا و همکاری تیمی    حقوق و مزایای رقابتی    آموزش‌های تخصصی در طول دوره کاری\n",
      "Help Desk, شبکه, عیب یابی, +NETWORK\n",
      "Error generating word cloud for job 21: cannot open resource\n",
      "گروه ایده‌کاوان یه سازمان دانش‌بنیانه که آفریننده و مالک محصول‌های مختلفیه: بهترینو، ویترین و دوباره.\n",
      " تمام این محصول‌ها، و محصول‌های تازه‌ای که در راه هستن، دور یه ماموریت واحد شکل گرفته‌ان: توانمندسازی کسب‌وکارهای کوچیک و متوسط با انحصارزدایی از قدرت فناورانه و کمک کردن به این کسب‌وکارها برای این که دیده بشن و بتونن خدمات بهتری به مردم ارائه کنن.کسب‌وکارهای کوچیک و متوسط برای تجربه کردن فعالیت در فضای دیجیتال چالش‌های زیادی دارن؛ ما با راهکارهایی که در اختیارشون می‌ذاریم این چالش‌ها رو از سر راه برمی‌داریم و به رشدشون کمک می‌کنیم.خود ما هم یکی از سریع‌ترین رشدها رو، اون‌هم به شکل ارگانیک و محصول‌محور، در اکوسیستم استارت‌آپی ایران تجربه کرده‌ایم که نتیجه‌ی مستقیم عملکرد تیم مستعد و پرتلاش‌مون بوده.خوشحالیم که تونستیم فضایی بسازیم که آدم‌های توانمند و اثرگذار کار کردن در ایده‌کاوان رو انتخاب کنن و در کنار هم ارزش خلق کنیم.\n",
      "ما در گروه ایده‌کاوان ابزارهای تکنولوژیک رو برای افزایش فروش و مدیریت عملیات داخلی کسب‌وکارها در اختیار مدیرانشون قرار می‌دیم. در همین راستا به دنبال هم‌تیمی‌ پرانرژی، باتجربه و نتیجه‌گرا برای جایگاه شغلی کارشناس ارشد شبکه‌های اجتماعی هستیم تا با کمک همکار جدیدمون، از طریق اینستاگرام و تلگرام، روی رشد صفحات بهترینو تمرکز کنیم و آگاهی از برند بهترینو رو در این شبکه‌های اجتماعی رشد بدیم.«بهترینو» یه پلتفرمه که کسب‌وکارهای هر منطقه رو به کاربران معرفی می‌کنه.وقتی شما برای پیداکردن یه کسب‌وکار در یه منطقه توی گوگل سرچ می‌کنید، مثلا «آموزشگاه خیاطی در آزادی»، صفحه‌ای از سایت بهترینو رو می‌بینید که توی اون لیستی از کسب‌وکارهای همون منطقه روی نقشه به شما نشون داده می‌شه. کسب‌وکارها بر اساس امتیازی که دارن رتبه‌بندی شده‌ن و شما می‌تونید اطلاعات تماس، ساعت کاری، نظرات و عکس‌های اون کسب‌وکار رو ببینید. از طرف دیگه کسب‌وکارها هم می‌تونن از بهترینو برای دیده‌شدن، جذب مشتری و تبلیغات استفاده کنن. فعالیت‌‌ها و کارهای مربوط به این جایگاه:طراحی و اجرای استراتژی رشد پیج اینستاگرام: تحلیل وضعیت فعلی پیج، تعریف اهداف بلندپروازانه و معنادار و تدوین برنامه برای رشد تعداد دنبال‌کنندگان (Followers) و نرخ تعاملمدیریت پروژه‌های سوشال مدیا: هدایت پروژه‌ها از مرحله ایده‌پردازی تا اجرا و ارزیابی نتایج، با تضمین کیفیت و تعهد به زمان‌های توافق شدهتدوین برنامه محتوایی و سناریوهای خلاقانهمدیریت کمپین‌های تبلیغاتی اینستاگرامی: هدف‌گذاری، طراحی تبلیغ، بودجه‌بندی و بهینه‌سازی کمپین‌های تبلیغاتیتشکیل و رهبری تیم: ایجاد، مدیریت و انگیزه‌دهی به تیم، هماهنگی با سایر تیم‌ها (بازاریابی، محصول و فروش)تحلیل عملکرد شبکه‌های اجتماعی و ارائه گزارش‌های هفتگی و ماهانهتوانمندی‌ها و شایستگی‌های مورد نیاز برای این جایگاه:تسلط به الگوریتم‌های رشد در اینستاگرام و سایر شبکه‌های اجتماعیآشنایی عمیق با کپی‌رایتینگ و تبلیغ‌نویسیآشنایی با ابزارهای گرافیکی و طراحیآشنایی با ابزارهای گزارش‌گیری و تحلیلی شبکه‌های اجتماعیتوانمندی در تشکیل، رهبری تیم و کار تیمی و برقراری ارتباط موثر با اعضای تیم  توانمندی در حل مسئلهحداقل ۳ سال تجربه موفق در مدیریت و اجرای پروژه‌های سوشال مدیایادگیری سریع و قابلیت تطبیق با تغییرات سریع در کسب‌وکارپیش‌بردن پروژه‌ها از صفر تا صد با دقت و کیفیت بالامسئولیت‌پذیر و متعهد نسبت به تمامی پروژه‌ها و اهداف تعیین‌شدهداشتن تجربه‌های زیر مزیت محسوب می‌شه:- تجربه کار در کسب‌وکارهای پلتفرمی- تجربه ایجاد و توسعه تیم سوشال مدیاساعت‌کاری:شنبه تا چهارشنبه از ساعت ۹ تا ۱۸:۳۰این موقعیت به صورت حضوری و تمام‌وقت در غرب تهران (با دسترسی آسان به ایستگاه متروی دانشگاه شریف و دکتر حبیب‌الله) تعریف شده. اگه به این حوزه علاقه‌مندین، رزومه‌تون رو برامون ارسال کنین!مزایا:‌ پرداخت حقوق به‌موقع و رعایت مسائل قانونی قرارداد کار که وظیفهٔ‌ ماست. امکان استفاده از بیمه تکمیلی و صبحانه فراهمه. ایجاد تجربه کار در محیطی محترم و امن و پر از رشد و یادگیری هم ارزشیه که به‌ش پایبندیم.\n",
      "شبکه های اجتماعی, کمپین‌های تبلیغاتی, تولید محتوا, اینستاگرام, سناریو نویسی\n",
      "Error generating word cloud for job 22: cannot open resource\n",
      "شرکت شیمی دارویی نوترون یکی از بزرگترین تولیدکنندگان مواد شیمیایی آزمایشگاهی میباشد که با تیمی با بیش از پانزده سال تجربه در صنایع شیمیایی دارویی آرایشی بهداشتی آزمایشگاهی آب و فاضلاب وآزمایشگاه های پاتوبیولوژی رنج متنوعی ازمواد و استاندارها را تهیه و با بالا ترین کیفیت به بازار عرضه کرده است.\n",
      "به یک حسابدار باتجربه  تمام وقت با بیمه و مزایا و شرایط زیر؛ محدوده میدان انقلاب نیازمندیم:جمع‌آوری و مدیریت داده‌های اصلی مالی.آشنایی کامل با برنامه سپیدارثبت صورت‌های مالی و اسناد صورتحساب و پیروی از دستورالعمل‌ها.نگهداری سوابق با به‌روزرسانی فاکتورها، بدهی‌ها و اعتبارات.ارائه گزارش به مدیریت در مورد گزارشات حسابداری و کلیه مسائل مربوطه.اطمینان از رعایت استانداردها و خط‌مشی‌های حسابداری.نظارت بر هر گونه حساب‌های پرداختنی و دریافتنی  آشنا به امور مالیاتی و حسابرسی  رده سنی حداکثر 35 سالساعت کاری از شنبه تا چهارشنبه 9 الی 17:30و پنجشنبه 9 تا 13\n",
      "مالی و حسابداری, نرم افزار سپیدار, امور مالیاتی\n",
      "Error generating word cloud for job 23: cannot open resource\n",
      "گروه ایده‌کاوان یه سازمان دانش‌بنیانه که آفریننده و مالک محصول‌های مختلفیه: بهترینو، ویترین و دوباره.\n",
      " تمام این محصول‌ها، و محصول‌های تازه‌ای که در راه هستن، دور یه ماموریت واحد شکل گرفته‌ان: توانمندسازی کسب‌وکارهای کوچیک و متوسط با انحصارزدایی از قدرت فناورانه و کمک کردن به این کسب‌وکارها برای این که دیده بشن و بتونن خدمات بهتری به مردم ارائه کنن.کسب‌وکارهای کوچیک و متوسط برای تجربه کردن فعالیت در فضای دیجیتال چالش‌های زیادی دارن؛ ما با راهکارهایی که در اختیارشون می‌ذاریم این چالش‌ها رو از سر راه برمی‌داریم و به رشدشون کمک می‌کنیم.خود ما هم یکی از سریع‌ترین رشدها رو، اون‌هم به شکل ارگانیک و محصول‌محور، در اکوسیستم استارت‌آپی ایران تجربه کرده‌ایم که نتیجه‌ی مستقیم عملکرد تیم مستعد و پرتلاش‌مون بوده.خوشحالیم که تونستیم فضایی بسازیم که آدم‌های توانمند و اثرگذار کار کردن در ایده‌کاوان رو انتخاب کنن و در کنار هم ارزش خلق کنیم.\n",
      "ما در گروه ایده‌کاوان ابزارهای تکنولوژیک رو برای افزایش فروش و مدیریت عملیات داخلی کسب‌وکارها در اختیار مدیرانشون قرار می‌دیم. در تیم بهترینو به دنبال هم‌تیمی‌‌های توانمند در جایگاه اکانت منیجر هستیم. «بهترینو» یه پلتفرمه که کسب‌وکارهای هر منطقه رو به کاربران معرفی می‌کنه.وقتی شما برای پیداکردن یه کسب‌وکار در یه منطقه توی گوگل سرچ می‌کنید، مثلا «آموزشگاه خیاطی در آزادی»، صفحه‌ای از سایت بهترینو رو می‌بینید که توی اون لیستی از کسب‌وکارهای همون منطقه روی نقشه به شما نشون داده می‌شه. کسب‌وکارها بر اساس امتیازی که دارن رتبه‌بندی شده‌ن و شما می‌تونید اطلاعات تماس، ساعت کاری، نظرات و عکس‌های اون کسب‌وکار رو ببینید. از طرف دیگه کسب‌وکارها هم می‌تونن از بهترینو برای دیده‌شدن، جذب مشتری و تبلیغات استفاده کنن. فعالیت‌‌ها و کارهای مربوط به این جایگاه:برقراری ارتباط اولیه در جهت تکمیل صفحه کاربری مشتریان و آشنایی بیشتر با بهترینو اضافه کردن تعدادی مشتری جدید به صورت ماهانهتحلیل روند رشد مشتریان و برنامه‌ریزی برای موفقیت آن‌ها ارائه پیشنهادهای کلیدی و کاربردی برای بهبود نتیجه مشتریان بهترینوپاسخ‌گویی به پیام و تماس‌های مشتریان در اسرع وقتارائۀ گزارش‌های مدون و مکتوب به مدیر فروشارائۀ بازخوردهای کاربردی از طرف مشتریان و ارائۀ راه حل برای بهبود محصولتشخیص مسائل، مشکلات و دغدغه‌های مشتریان و رفع آن‌هاپیگیری، تمدید، ارتقا قراردادهای مشتریانمعرفی تبلیغات کلیکی بهترینو و شارژ مجدد پنل مشتریانتوانمندی‌ها و شایستگی‌های مورد نیاز برای این جایگاه: مسئولیت‌پذیری بالا منظم در محیط کار و پیگیری امور مشتریانتوانمندی راهبری و مذاکره در جلسه با مشتریانداشتن سابقه حداقل یک سال در زمینه فروش و پشتیبانی آنلاینآشنایی به اصول و فنون مذاکره و توانایی عقد قرارداد با مشتریان کلیدیآشنایی با صنعت پشتیبانی و فروش آنلاین و کسب‌وکارهای محلی تجربه کار با سیستم‌های ارتباط با مشتری (CRM) دارای روحیۀ همکاری تیمی و دقت در کار انعطاف‌پذیری کافی و تطبیق‌پذیری بالاعلاقه‌مند به یادگیری و به کارگیری تکنیک‌های فروشاین موقعیت به صورت حضوری و تمام‌وقت در غرب تهران (با دسترسی آسان به ایستگاه متروی دانشگاه شریف و دکتر حبیب‌الله) تعریف شده. اگه به این حوزه علاقه‌مندین، رزومه‌تون رو برامون ارسال کنین!مدل درآمدی: حقوق ثابت + پورسانت + پاداش عملکردمزایا:‌ پرداخت حقوق به‌موقع و رعایت مسائل قانونی قرارداد کار که وظیفهٔ‌ ماست. امکان استفاده از بیمه تکمیلی و صبحانه فراهمه. ایجاد تجربه کار در محیطی محترم و امن و پر از رشد و یادگیری هم ارزشیه که به‌ش پایبندیم.\n",
      "Account Management, ارتباط با مشتریان و پشتیبانی, ارتباط با مشتری, امورمشتریان, Microsoft Office, پشتیبانی\n",
      "Error generating word cloud for job 24: cannot open resource\n",
      "گروه ایده‌کاوان یه سازمان دانش‌بنیانه که آفریننده و مالک محصول‌های مختلفیه: بهترینو، ویترین و دوباره.\n",
      " تمام این محصول‌ها، و محصول‌های تازه‌ای که در راه هستن، دور یه ماموریت واحد شکل گرفته‌ان: توانمندسازی کسب‌وکارهای کوچیک و متوسط با انحصارزدایی از قدرت فناورانه و کمک کردن به این کسب‌وکارها برای این که دیده بشن و بتونن خدمات بهتری به مردم ارائه کنن.کسب‌وکارهای کوچیک و متوسط برای تجربه کردن فعالیت در فضای دیجیتال چالش‌های زیادی دارن؛ ما با راهکارهایی که در اختیارشون می‌ذاریم این چالش‌ها رو از سر راه برمی‌داریم و به رشدشون کمک می‌کنیم.خود ما هم یکی از سریع‌ترین رشدها رو، اون‌هم به شکل ارگانیک و محصول‌محور، در اکوسیستم استارت‌آپی ایران تجربه کرده‌ایم که نتیجه‌ی مستقیم عملکرد تیم مستعد و پرتلاش‌مون بوده.خوشحالیم که تونستیم فضایی بسازیم که آدم‌های توانمند و اثرگذار کار کردن در ایده‌کاوان رو انتخاب کنن و در کنار هم ارزش خلق کنیم.\n",
      "ما در گروه ایده‌کاوان ابزارهای تکنولوژیک را به منظور افزایش فروش و مدیریت عملیات داخلی کسب‌وکارها، در اختیار مدیرانشان قرار می‌دهیم. در همین راستا در جهت تکمیل تیم فروش دوباره به دنبال هم‌تیمی‌ توانمند برای جایگاه Account Manager هستیم.«دوباره» یه نرم‌افزار ابری یا تحت وبه که به کسب‌وکارها امکان می‌ده باشگاه مشتریان خودشون رو راه‌اندازی کنن تا فروششون رو بیشتر کنن. کافیه صاحب کسب‌وکار، شمارۀ مشتری‌های خودش رو توی پنل آنلاین «دوباره» ثبت کنه تا به کمک «دوباره» به مشتری‌های خودش کد تخفیف یا اعتبار هدیه اتوماتیک بده، پیامک‌های نظرسنجی بفرسته و لیست مشتری‌هاش رو به صورت منظم و دسته‌بندی‌شده در اختیار داشته باشه.فعالیت‌‌ها و کارهای مربوط به این جایگاه:تحلیل روند رشد مشتریان و برنامه‌ریزی برای موفقیت آن‌ها و تعامل اثربخش با مشتریان کلیدیارائۀ پیشنهاد و معرفی محصول متناسب با نیاز مشتریتکمیل پروفایل مشتریان جدیدارائۀ بازخوردهای کاربردی از طرف مشتریان و ارائۀ راه حل برای بهبود محصولتشخیص مسائل، مشکلات و دغدغه‌های مشتریان و رفع آن‌هاحفظ صد درصدی مشتریان و تلاش در راستای وفادارسازی حداکثری در مشتریان توجه و استفاده از داده‌ها و مولفه‌های کلیدی عملکرد در کمپین‌های مشتریان ارائه پیشنهادها و راهکارهای موثر و پربازده به مشتریان به منظور دست‌یابی به اهداف مشتری و توسعه روابطرفع مسائل و ابهامات مشتریان و رسیدگی به‌موقع و سریع به نارضایتی‌ها و شکایات این مشتریانمدیریت گردش مالی، امور قرارداد و پیگیری شارژ، مطالبات و واریزی‌های مشتریانتوجه و دستیابی به اهداف فروش ماهانه و فصلی و KPIهای تعریف شده در هر OKRتوانمندی‌ها و شایستگی‌های مورد نیاز برای این جایگاه: مسئولیت‌پذیری بالا علاقه‌مندی و درک مناسب از حوزه بازاریابی و بازاریابی دیجیتالتوانمندی راهبری و مذاکره در جلسه با مشتریان شناخت رقبا منظم در محیط کار و پیگیری امور مشتریانآشنایی با نرم‌افزارهای آفیس سابقه حداقل 2 سال در زمینه پشتیبانی و اکانتینگ آشنایی با صنعت پشتیبانی و فروش آنلاین و کسب‌وکارهای محلی تجربه کار با سیستم‌های ارتباط با مشتری (CRM) دارای روحیۀ همکاری تیمی و دقت در کار این موقعیت به صورت حضوری و تمام‌وقت در غرب تهران (با دسترسی آسان به ایستگاه متروی دانشگاه شریف و دکتر حبیب‌الله) تعریف شده. اگه به این حوزه علاقه‌مندین، رزومه‌تون رو برامون ارسال کنین!مدل درآمدی: حقوق ثابت + پورسانت + پاداش عملکردمزایا:‌ پرداخت حقوق به‌موقع و رعایت مسائل قانونی قرارداد کار که وظیفهٔ‌ ماست. امکان استفاده از بیمه تکمیلی و صبحانه فراهمه. ایجاد تجربه کار در محیطی محترم و امن و پر از رشد و یادگیری هم ارزشیه که به‌ش پایبندیم.\n",
      "Account Management, CRM, KPI, ارتباط با مشتریان و پشتیبانی\n",
      "Error generating word cloud for job 25: cannot open resource\n",
      "مجموعه روانشناسی ما به دنبال نیروی فروش پرانرژی و متعهد برای گسترش خدمات و محصولات خود است. اگر شما فردی هستید که به روانشناسی علاقه دارید و در ارتباط با مشتریان توانمند هستید، ما مشتاقانه منتظر شما هستیم.\n",
      "شرایط و مهارت‌های مورد نیاز:- تجربه کاری در فروش- توانایی برقراری ارتباط موثر با مشتریان- علاقه‌مند به حوزه روانشناسی و توسعه فردی- روحیه تیمی و توانایی کار در محیط پویا- مهارت‌های مذاکره و متقاعدسازیمزایا:- حقوق ثابت به همراه پورسانت- محیط کاری دوستانه و حرفه‌ای- فرصت‌های آموزش و پیشرفت شغلی- امکان حضوریاگر شما آماده‌اید تا بخشی از تیم ما شوید، رزومه خود را برای ما  ارسال کنید.ما منتظر شما هستیم تا با همکاری همدیگر، به سوی موفقیت‌های بزرگ قدم برداریم!\n",
      "اصول و فنون مذاکره, فروش تلفنی, فروش و بازاریابی\n",
      "Error generating word cloud for job 26: cannot open resource\n",
      "شرکت ویتریول در زمینه ساختمانی و فروش املاک در ترکیه و قبرس شمالی و همچنین اقامت مونته نگرو  و اقامت تحصیلی در کشورهای روسیه ،ترکیه و قبرس شمالی فعالیت میکند\n",
      "۱-استخدام ۴ همکار که سابقه کارشناس فروش و بازاریابی  تلفنی ( کال سنتر ) را در زمینه فروش املاک و کارهای اقامتی داشته باشند۲- شرایط احراز:سابقه فروش تلفنی–راوابط عمومی عالی-راونشناسی شناخت مشتری- روحیه کار تیمی – مسلط به آفیس (Word، Excel، Powerpoint و۳-در روز حداقل ۵۰ تماس و روزانه حداقل ۱۰ تماس موفق باید داشند۴-پنجاه تماس شامل یک روز B2c و یک روز B2Bمزایا حقوق توافقی + پورسانتآموزش توسط شرکت در هفته های اول انجام میشودپورسانت ۵ فروش اول ۲ درصد از قیمت ملک و به صورت پله کانی تا ۳.۵ در صد قابل ارتقاع\n",
      "فروش و بازاریابی, اصول و فنون مذاکره, فروش تلفنی\n",
      "Error generating word cloud for job 27: cannot open resource\n",
      "مجموعه بازرگانی لیان سیکا به عنوان کارگزار رسمی گمرک ایران فعالیت خود را از سال 1389 به منظور کمک به بازرگانان و صاحبان کالا و ارائه راهکارهای گمرکی در زمینه ترخیص کالا، واردات و صادرات با استفاده از کارشناسان مجرب و متخصص آغاز نموده است و در حال حاضر با حضور فعال در کلیه گمرکات کشور در حال خدمت رسانی به بازرگانان و صنایع می باشد.\n",
      "شرکت بازرگانی لیان سیکا  واقع در تهران  بلوار میرداماد جهت  تکمیل  کادر خود از افراد واجد  شرایط زیر  دعوت به  همکاری  می نماید: کارشناس خزانه دار نوع همکاری: تمام وقتتوانایی ها و شرایط احراز:تسلط کافی به  نرم افزار آفیس و نرم افزارهای حسابداریصدور اسناد حسابداری دریافت ، پرداخت و دستور پرداخت های روزانهآشنا به قوانین بانکی، چک ، انواع تسهیلات و ضمانت نامه های بانکیکنترل تمامی تراکنش های مالی و مغایرت گیری بانک ها تسلط به امور تنخواه گردان و تهیه گزارش روزانه نقدینگیدارای روحیه کار تیمی و تعامل با سایر اعضای تیم مالیمتعهد، امانتدار، دقیق ، منظم و مسئولیت پذیرحداکثر سن 35 سالدر صورت تمایل به پیشرفت در تخصص و جایگاه اجتماعی خود ، منتظر همکاری با شما هستیمحقوق در عنوان شغلی مذکور با توجه به مهارت های شما و  سنجش شما در مصاحبه ی حضوری ، در نظر گرفته می شود .سابقه کار در شرکت های بازرگانی مزیت محسوب می شودتسهیلات و مزایا:بیمه تامین اجتماعی از روز اولبیمه تکمیلیصبحانهمحیط کاری پویا و دوستانهارائه آموزش‌های تخصصی متناسب با سطح افراد (امور آموزش رایگان است)ساعت کاری:·         شنبه تا چهارشنبه 8 الی 16:30·         پنجشنبه ها 8 الی 12:30\n",
      "خزانه داری, مغایرت گیری, مالی و حسابداری\n",
      "Error generating word cloud for job 28: cannot open resource\n",
      "سرزمین هوشمند یک شرکت دانش‌بنیان و با سهامداری بانک ملت است که در زمینه توسعه زیرساخت‌ها و خدمات نرم‌افزاری از سال 1384 فعالیت می‌کند.\n",
      "\n",
      "ما در یک خانواده 250 نفری در محیطی صمیمی و خلاق در کنار هم می‌کوشیم تا پروژه‌های متنوع مجموعه را با بهره‌گیری از فناوری های روز برای ارتقای زندگی و کار پیش ببریم.\n",
      "\n",
      "بنیادی‌ترین کارکرد برند \"سرزمین هوشمند\" در فضای اطلاعات و ارتباطات، خلق زیرساخت‌های سخت‌افزاری و نرم‌افزاری است که منجر می‌شود یک فرآیند سرویس‌دهی در حوزه‌ی فناوری اطلاعات، از ابتدا تا پایان، در هوشمندانه‌ترین حالت خود انجام شود\n",
      "ما یک شرکت نوآور در حوزه فینتک و بلاکچین هستیم که تمرکز اصلی‌مان بر توسعه محصولات مالی بر بستر هوش مصنوعی است. هدف ما ایجاد راهکارهای پیشرفته‌ای است که بتوانند همزمان با تحول در صنعت مالی، مدل‌های کسب‌وکار هوشمند و پایدار را به بازار ارائه دهد. با استفاده از آخرین فناوری‌های بلاکچین و هوش مصنوعی، ما به دنبال خلق آینده‌ای هستیم که خدمات مالی دیجیتال را به سطح جدیدی ارتقا دهد و فرصت‌های تازه‌ای را برای کاربران و کسب‌وکارها فراهم کند.مسئولیــت‌هـــای اصـلـی:مدیریت کامل چرخه محصول: هدایت کلیه مراحل محصول از Discovery تا Delivery و بهینه‌سازی آن، با تمرکز بر محصولات فینتکی بلاکچینیطراحی استراتژی محصول: طراحی استراتژی محصول و نقشه راه براساس تحلیل بازار و نیازهای کاربران، با هدف هماهنگی کامل با اهداف کسب‌وکارطراحی و پیش‌بینی مدل کسب‌وکار: طراحی و ایجاد مدل‌های کسب‌وکار پایدار برای محصولات بلاکچین جدید، با در نظر گرفتن درآمدزایی و رشد بازارهمکاری بین تیمی: مدیریت و هماهنگی بین تیم‌های فنی، طراحی، و بازاریابی برای اطمینان از اجرای موفقیت‌آمیز محصول و دستیابی به اهداف تعیین‌شدهتحلیل رقبا و بازار: رصد مستمر روندهای صنعت، شناسایی فرصت‌های جدید و ارائه پیشنهادات برای بهبود و نوآوری محصولاتنوشتن مستندات کسب و کاری  و PRD محصول: طراحی وایرفریم و تهیه مستندات فنی و کاربری برای هدایت تیم توسعه و ذینفعان.مهارت‌ها و تجربه مورد نیاز:حداقل ۳ سال تجربه مدیریت محصول، ترجیحاً در حوزه مرتبط با بلاکچین و ارزهای دیجیتالتجربه موفق در راه‌اندازی و مدیریت محصولات فین‌تکی یا بلاکچینیتوانایی نوشتن مستندات محصول به شکلی روان و دقیقمهارت‌های رهبری قوی و سابقه موفق در هدایت تیم‌های توسعه و مدیریت پروژهتوانایی برقراری ارتباط موثر و تاثیرگذاری بر ذینفعان در سطوح مختلفمهارت‌های حل مسئله و تحلیل دادهآشنایی کامل با فناوری بلاکچین، رمزارزها و تحولات جدید در حوزه فین‌تک مزایا:ساعات کاری منعطف و امکان دورکاریفرصت رهبری پروژه‌های پیشرفته فین‌تک و بلاکچین در شرکتی پویا و آینده‌نگر.محیط کاری حمایتی و تیمی که در آن نقش شما مستقیماً در موفقیت شرکت تاثیرگذار استحقوق و مزایای رقابتی از جمله تسهیلات وام\n",
      "product management, Blockchain, توسعه محصول\n",
      "Error generating word cloud for job 29: cannot open resource\n",
      "شرکت آرتان تجارت تورال سنا\n",
      "زمینه کاری شرکت: واردات و صادرات\n",
      "محصولات و خدمات: \n",
      "- خدمات بازرگانی شامل واردات، صادرات، حمل و ترخیص کالا\n",
      "ثبت اسناد مالینرم افزار هلوآشنا به سامانه مؤدیان آشنا به سامانه بیمه تامین اجتماعیحقوق دستمزدمزایا:بیمهناهارامکان افزایش حقوق در صورت عملکرد بهتر\n",
      "حسابداری, امور مالی, نرم افزار حسابداری هلو\n",
      "Error generating word cloud for job 30: cannot open resource\n",
      "شرکت  فراگرصنعت  فعال در زمینه های چاپ و تولید لیبل های امنیتی و صنعتی با بیش از بیست سال سابقه در این حوضه فعال است\n",
      "ما در شرکت فراگرصنعت دنبال همکاری میگردیم که بتونه در زمینه فروش کمک ما باشه برای توسعه سهم بازارخوشحال میشیم شما رو در تیم خودمون داشته باشیم.شرح وظایف:اشنایی با CRMپیگیری مشتریان جدید و قدیم  برقراری ارتباط موثر با مشتریان و ایجاد روابط پایدارفن بیان قوی و روابط عمومی بالامزایای همکاری با ما:حقوق ثابت + پورسانت خوببیمه تامین اجتماعیساعت کاری منعطفآموزش حین کار در حیطه فروشمحدوده شرکت: خیابان طالقانی و ایرانشهرساعت کاری: شنبه تا چهارشنبه 10 صبح تا 18پنج شنبه ها تعطیلاگر فعال، پرانرژی و دنبال رشد هستید جای شما در فراگرصنعت خالی هست.\n",
      "اصول و فنون مذاکره, فروش و بازاریابی, استراتژی فروش\n",
      "Error generating word cloud for job 31: cannot open resource\n",
      "مدیاژ یک شرکت جوان و پویا در زمینه فروش آنلاین محصولات آرایشی و بهداشتی است. مجموعه مدیاژ به پیشرفت و ترقی اعتقاد زیادی دارد و خواهان این است که در محیطی امن و خلاق، امکان پیشرفت سازمانی و فردی را فراهم کند.\n",
      "فروشگاه اینترنتی مُدیاژ یکی از پلتفرم‌های آنلاین در حوزه لوازم آرایشی و مراقبتی و بهداشتی، به دنبال یک کارشناس ارشد تولید محتوا باانگیزه و خلاق است که توانایی‌های استراتژیک و عملیاتی خود را برای ارتقاء برند و افزایش فروش به‌کار گیرد.شرح وظایف:تولید و مدیریت محتوای متنی برای سایت، شبکه‌های اجتماعی و خبرنامه‌هانوشتن توضیحات جذاب و کاربردی برای محصولات آرایشی٬ مراقبتی و بهداشتیایجاد مقالات مرتبط با زیبایی، مراقبت از پوست و مو، و ترندهای جدید آرایشیبهینه‌سازی محتوا بر اساس اصول SEO برای بهبود رتبه سایت در موتورهای جستجوهمکاری با تیم بازاریابی برای طراحی و اجرای کمپین‌های تبلیغاتی آنلاینتحلیل و بررسی عملکرد محتوا و ارائه راهکارهایی برای بهبود آنشرایط احراز:تسلط به اصول و تکنیک‌های تولید محتوا و کپی‌رایتینگآشنایی با صنعت لوازم آرایشی و بهداشتی و علاقه‌مند به زیبایی و مراقبت‌های پوستیتجربه در نوشتن متون بهینه‌سازی شده برای موتورهای جستجو (SEO)آشنایی با شبکه‌های اجتماعی و اصول بازاریابی محتواتوانایی مدیریت همزمان چندین پروژه و دقت به جزئیاتتوانایی ارائه‌ی گزارش‌های منظم از عملکردداشتن روحیه تیمی و مهارت‌های ارتباطی قویتسلط به زبان انگلیسیشرایط همکاری: تمام وقت و حضوریما مشتاق همکاری با افراد خلاق و با انگیزه هستیم!\n",
      "تولید محتوا, مدیریت محتوا, شبکه های اجتماعی, کپی رایتینگ\n",
      "Error generating word cloud for job 32: cannot open resource\n",
      "۱- کلینیک روانشناسی و مشاوره واقع در سعادت آباد،\n",
      "۲- روان درمانی فردی بزرگسالان،\n",
      "۳- مشاوره ازدواج، زوج و خانواده درمانی،\n",
      "۴- مشاوره تحصیلی و شغلی، \n",
      "۵- ارزیابی روان،\n",
      "6- انجام انواع تست های شخصیت، استعداد یابی و ازدواج،\n",
      "7- برگزاری کارگاه های تخصصی و عمومی روانشناسی،\n",
      "شرح شغل و وظایفپاسخگویی به تماس های ورودی و خروجیپیگیری مراجعانشناسایی و جذب مشتریان جدید از طریق روش‌های مختلف مانند بازاریابی تلفنی، بازاریابی آنلاین،.ارائه مشاوره به مشتریان پاسخگویی به سوالات مشتریان در مورد خدمات کلینیک، ارائه اطلاعات دقیق در مورد درمان‌ها و هزینه‌ها.انجام مذاکرات با مشتریان برای تعیین بهترین برنامه درمانی و توافق بر روی هزینه‌ها.پیگیری روند درمان مشتریان اطمینان از رضایت مشتریان از خدمات ارائه شده و حل هرگونه مشکل احتمالی.تهیه گزارش‌های فروش ارائه گزارش‌های منظم از عملکرد فروش و فعالیت‌های بازاریابیمذاکره براساس تعداد تارگت و محدوده تعیین شددستیابی به اهداف فروش و گزارش نتیجه آن ها به مدیر مربوطهپیگیری مشتریان و رسیدگی به نیازها و درخواست های ایشان بصورت حضوری و تلفنیهماهنگی و کار تیمی سازنده با سایر تیم های سازمان جهت دستیابی به نتایج بهترو جستجوی روش های نوین برای پروموت کردن سازمان و محصولات آنجست وجوی فعالانه در زمینه فرصت های بهتر برای فروش، و ارائه پیشنهاد بررسی داده ها و تهیه گزارشات مکرر در زمینه مالی و فروشمذاکره کردن با هدف معاملات سودمند و رسیدگی به شکایات و اعتراضاتصبر و حوصله :توانایی برخورد با افراد با مشکلات مختلف و ارائه پاسخ‌های مناسب.همدلی, توانایی درک احساسات و نیازهای مشتریان.مسئولیت‌پذیری توانایی انجام وظایف محوله به نحو احسن و در موعد مقرر.انرژی و انگیزه داشتن روحیه مثبت و تلاش برای دستیابی به اهداف.مهارت‌های ارتباطی قویارائه راه حل‌های مناسب برای مشکلات آن‌هاتوانایی کار با نرم‌افزارهای رایانه‌ای تسلط بر نرم‌افزارهای آفیس  CRM و سایر ابزارهای مرتبط با فروش و بازاریابیروحیه کار تیمیانعطاف‌پذیری و توانایی مدیریت زمانعلاقه به حوزه سلامت و روانساعت کاریشنبه تا چهارشنبه 11 تا 20پنج شنبه 11 تا 17\n",
      "اصول و فنون مذاکره, فروش و بازاریابی, فروش, ارتباط با مشتری\n",
      "Error generating word cloud for job 33: cannot open resource\n",
      "بیش از 10 سال است که رسانۀ تجارت نوین در حوزۀ فناوری‌های دیجیتال فعالیت می‌کند. تجربۀ این سال‌ها به ما نشان داده است که رسانه‌های دیجیتال می‌توانند نقشی کلیدی در افزایش کیفیت زندگی همۀ ما ایرانی‌ها ایفا کنند. دقیقاً همین نگاه بود که باعث شد ما وارد عرصۀ بازاریابی دیجیتال شویم. به این امید که بتوانیم نقشی کوچک در بهبود و توسعۀ رسانه‌های دیجیتال فارسی داشته باشیم\n",
      "حسابدار فعال و با انرژی جهت حسابداری، دقت بالا و وقت شناس، انضباط در رفتار و محل کار،  قدرت برقراری کار گروهی، دارای انگیزه یادگیری، قابلیت عملکرد دقیق و کامل در شرایط پرفشار کاری    مسلط به امور حسابداری و  توانایی ثبت و بایگانی اسناد حسابداری توانایی کار با ورد و اکسل و قابلیت گزارش گیری آشنا به امور ارزش افزوده و معاملات فصلی آشنا به قوانین بیمه و مالیات و انجام امور بیمه ای و مالیاتی آشنا به امور بانک و تنخواه و مغایرت گیری توانایی صدور و بایگانی فاکتورهای خرید و فروش انجام امور اداری دفتری و امور پرسنلی و قراردادها آشنایی با نرم افزار حسابداری رایورز امتیاز محسوب می شود. تایم کاری شنبه تا چهارشنبه ۰۸:۰۰ الی ۱۶:۳۰ - پنج شنبه ۰۸:۰۰ الی ۱۲:۰۰\n",
      "مالی و حسابداری, مغایرت گیری, قوانین مالیاتی, تنخواه گردانی\n",
      "Error generating word cloud for job 34: cannot open resource\n",
      "شرکت در زمینه بازرگانی و مدیریت پروژه های صنعتی فعالیت می نماید.\n",
      "یک شرکت معتبر در تهران میدان شیخ بهایی به یک نفر مهندس صنایع با شرایط زیر نیازمند است: 1- دارای حداقل 4 سال سابقه کنترل پروژه باشد. 2- مسلط به زبان انگلیسی 3-ترجیحا سابقه کار در پروژه کنسانتره مس داشته باشد. 4- روابط عمومی بالا و پوشش و ظاهر مناسب ، فعال و متعهد باشد. 5-ساعت کار: شنبه تا چهارشنبه از ساعت 8:30 الی 16:30 و پنجشنبه 8:30 الی 12:30\n",
      "مهندسی صنایع, کنترل پروژه\n",
      "Error generating word cloud for job 35: cannot open resource\n",
      "‎کانتمپو يكى از زیر مجموعه های شرکت (گروه مبلمان غفارى) تولید کننده مبلمان، صنایع چوبی و نماینده انحصاری RocheBobois فرانسه در ایران میباشد که توسط خانواده غفاری در سال 1355 تاسیس گردید. كانتمپو از فعالان و پیشروان صاحب سبک طراحی و تولید مبلمان و صنایع چوبی شناخته می شود.\n",
      "استخدام فروشنده مبلمان آشنا به معماری داخلی(کانتمپو)گروه مبلمان غفاری ( کانتمپو ) جهت تکمیل کادر فروش خود به فروشندگانی با تجربه در طراحی داخلی با مهارت‌های ارتباطی بالا در فروش و یا افراد با تجربه در فروشگاه های سطح بالا نیازمند میباشد.شرایط کلی:-حداقل ۲ سال سابقه کار مرتبط در فروشگاه های سطح-مسلط به برنامه اتوکد و اکسل-عدم داشتن سوء پیشینه و اعتیاد-آشنایی با طراحی داخلی-مستعد و علاقه مند به حرفه فروشندگی و طراحی داخلی-ظاهری آراسته -حداکثر سن : 45-روزهای کاری: شنبه تا پنجشنبه-ساعت کاری: از ۹:۳۰ تا ۱۸:۰۰-موقیعت : عباس آباد، خیابان بهشتی، شوروم کانتمپو-لطفا فقط رزومه عکس دار ارسال نمایید-پرداخت حقوق به صورت ثابت ماهانه به اضافه پورسانت* در صورت علاقه رزومه خود را ارسال نمایید.\n",
      "فروشندگی, اصول و فنون مذاکره, فروش و بازاریابی\n",
      "Error generating word cloud for job 36: cannot open resource\n",
      "رهتاب بیستون، به عنوان اولین شرکت دانش بنیان در حوزه تنظیم بازار محصولات کشاورزی و مبدع طرح مدیریت هوشمند زنجیره تامین و توزیع کالاهای اساسی در شهر اصفهان مستقر است. \n",
      "رهتاب با توسعه محصولات نرم‌افزاری و سخت‌افزاری متنوع، متناسب با نیاز بازار و با اتکا بر نیروهای متعهد و با‌استعداد خود در بخش‌های مختلف سازمان توانست در سال گذشته به رشد سهم بازار قابل توجهی دست پیدا کند که با توجه به این امر، جذب نیروهای انسانی جدید را در دستور کار قرار داده است.\n",
      "سیاست این شرکت در حوزه جذب و مدیریت منابع انسانی، آموزش و نگهداشت بلندمدت نیروهای با استعداد و آینده‌دار خود است چرا که معتقد است که پیشرفت شرکت جز با پیشرفت منابع انسانی رضایتمند و کارآمد مقدور نمی‌باشد.\n",
      "وظایف و مسئولیت‌ها:ترجمه استوری‌بردهای برنامه وتبدیل موارد استفاده آن به برنامه‌های کاربردی طراحی، ساخت و نگهداری کدهای C# کارآمد،تمییز،مقیاس پذیر، قابل استفاده مجدد و قابل اعتمادکمک به حفظ کیفیت کد، سازماندهی و خودکارسازیبرنامه ریزی و گزارش فعالیت ها برای ایجاد سیستم های مدیریتیانجام بررسی هایی برای شناسایی گلوگاه ها،ریسک ها و مشکلات فنی مرتبط با پروژه ها، سپس پیشنهاد وارائه یک راه حل استراتژیک به روز رسانی،هماهنگی و پیشرفت با تیم PM و سایر توسعه دهندگانایجاد، ادغام و تست ویژگی های جدید در برنامه های مختلفایجاد پلتفرم های تست خودکار و تست های واحدارائه پشتیبانی فنی به ذینفعان در سازمان  و راهنمایی برای اعضای تیم.گزارش وضعیت و پیشرفت پروژه به اعضای ارشد تیمایجاد اپلیکیشن های داخلی با استفاده از فریم ورک دات نتاستفاده از بازخورد کاربران برای ایجاد نسخه های بعدی هر توسعهتضمین امنیت با اجرای اقدامات لازم و رعایت استانداردهای تعریف شده صنعتتوسعه و پیاده سازی نرم افزارهای کاربردی با استفاده از  C#و چارچوب .NETتجزیه و تحلیل نیازهای کاربر و الزامات نرم افزاری برای تعیین امکان سنجی طراحی در محدودیت های زمانی و هزینه ایشرایط احراز:تسلط به فناوری هایی مانند MVC/C#، JavaScript و پایگاه داده های SQLدانش سیستم های نسخه سازی کد مانند Gitتسلط به RESTful APIو GraphQLدانش یکپارچه سازی مداومآشنایی با پایگاه های داده رابطه ای و SQLتجربه توسعه خدمات وب SOAP)،REST)آشنایی با الگوهای همزمانی در سی شارپدرک خوب از فناوری های پایگاه داده(پایگاه داده رابطه ای و NoSQL) ترجیحاً در SQL Server و MongoDBآشنایی با  pipelines CI/CD و DevOpsدرک خوبی از معماری و الگوهای طراحیآشنایی با اصول کاری Agile و چارچوپ‌های آندانش مدیریت پروژه ودرک محدوده پروژهآشنایی نسبی با میکروسرویس هاآشنایی با Design Pattern ها و Unit Test مزیت محسوب می‌شودشرایط عمومی:برخورداری از روحیه کار گروهی و مسئولیت پذیریتوانمندی در مدیریت زمان و برنامه ریزی دقیقتوانمند در حل مسئله،ایده پرداز وخلاقتوانایی برقراری ارتباط موثر با سطوح مختلف سازمانیبرخورداری از انضباط و صبر و حوصله بالابرخورداری از امکان فعالیت تمام وقت به صورت حضوریبخشی از مزایایی که ما برای شما در رهتاب بیستون فراهم می‌کنیم شامل موارد زیر است:ایجاد فرصت های یادگیری؛ به عنوان یک کسب و کار همیشه در حال توسعه و یادگیری، معتقدیم همکارانمان هم باید مدام در حال توسعه و یادگیری باشند.پرداخت کمک هزینه رفت و آمد؛ برای اینکه همکارانمان دغدغه کمتری برای هزینه های رفت و آمد داشته باشند.ارائه صبحانه و میان وعده؛ برای اینکه همکارانمان، صبح خودشون را با انرژی شروع کنند و در طول روز هم سرحال باشند.پرداخت پاداش های عملکردی؛ عملکرد خوب، برای ما ارزشمند است و از آن قدردانی می کنیم.ارائه بیمه تکمیلی درمان؛ برای اینکه همکارانمان، دغدغه هزینه های دارو و پزشک نداشته باشند.در نظر گرفتن هدایا وبسته های مناسبتی ؛ برای اینکه لحظاتی مهم وارزشمند را در کنار همکارانمان جشن بگیریم.مواردی که شاید برای شروع همکاری برای شما مهم باشد:محدوده محل کار خیابان مشتاق سوم، ساختمان شهرک علمی تحقیقاتی اصفهانجنسیت برایمان مهم نیست، شایستگی شما برای شغل و سمت حائز اهمیت است.حقوق دریافتی براساس سابقه، مهارت و دانش شما و میزان پیشبرد اهداف شرکت تعیین می‌گردد.ساعت کاری ما شنبه تا چهارشنبه 8 الی 16:30 و پنجشنبه 8 الی 15 به صورت حضوری میباشد که نیم ساعت هم شناوری شروع ساعت کار داریم ؛ چون می‌دانیم ترافیک اصفهان و دردسر‌های رفت‌و‌آمد می تواند گاهی از کنترل خارج شود.*** لطفا فقط افرادی که امکان فعالیت تمام وقت به صورت حضوری در شهر اصفهان را دارند رزومه خود را ارسال نمایند.اگر شایستگی های لازم برای انجام وظایف و مسئولیت های مطرح شده را دارید و مزایا و شرایط ما با انتظارات شما همخوانی دارد، منتظر دیدن رزومه‌تان هستیم.\n",
      "Back-end, #C, Javascript, RestFul API\n",
      "Error generating word cloud for job 37: cannot open resource\n",
      "کلینیک فیزیوتراپی ارتوپدی تخصصی بیمارستان پیامبران\n",
      "سوابق کاری مرتبطآشنایی با سامانه بیمارستانی HISپاسخدهی و نوبت دهی به مراجعین حضوری و تلفنیثبت نسخ الکترونیکثبت معرفی نامه بیمه تکمیلی بیمارانپذیرش بیماران و تشکیل پروندهبرقراری ارتباط موثر با بیماران جهت جذب مراجعه کنندگان\n",
      "امور پذیرش, پذیرش مراجعین, Microsoft Office\n",
      "Error generating word cloud for job 38: cannot open resource\n",
      "شرکت بما ایران، شرکت ساختمانی\n",
      "هلدینگ (قطعه‌سازی خودرو)\n",
      "1- منظم و بادقت2- ثبت و کنترل اسناد مالی اعم از بانک ها ، تنخواه ، هزینه ها ، خزانه ، خرید ، فروش و ..3- مغایرت گیری بانک ها و طرف حساب ها4- آشنا به قوانین بیمه تامین اجتماعی ، سامانه مودیان ، امور مالیاتی و ارزش افزوده و ...5- آشنا به نرم افزار سپیدار و اکسل6- دارای حداقل مدرک تحصیلی کارشناسی در رشته حسابداری 7- نوع قرارداد تمام وقت – رزومه دانشجویان مورد پذیرش نمی باشد8- همکاری تمام وقت با ساعت کاری 8 الی 17:30 در صورت لزوم  19:30 پنج شنبه ها تعطیل9- سرویس ایاب و ذهاب ، صبحانه و نهار ، بیمه از روز اول 10- محل خدمت : جاده مخصوص کرج کیلومتر 16 حتما ذکر شود\n",
      "حسابداری, ثبت اسناد مالی, Microsoft Office, نرم افزار سپیدار\n",
      "Error generating word cloud for job 39: cannot open resource\n",
      "خدای خوب من مجموعه ای است که در زمینه رشد و توسعه فردی و بیزینس کوچینگ فعالیت دارد.\n",
      "این مجموعه از سال 1401 شروع به کار کرده و به لطف خدا با استقبال خوب مخاطبین از ایران و سایر کشورها همراه شده است\n",
      "شرح وظایف :پاسخگویی به سوالات مخاطبین شرایط احراز: روحیه کار تیمی دارای قدرت مذاکره و فن بیانآموزش پذیری وقت شناسی و منظم بودن مزایا :رشد شخصیتیدسترسی به آموزش های رشد و توسعه فردی و بیزینسیکمک هزینه شرکت در دوره های فروش و غیرهامکان کسب درآمد تا مبلغ 70 میلیون تومنپورسانت پلکانی شرکت در کارگاه‌های آموزشیامکان ارتقای شغلیساعت کاری:9 صبح تا 18 عصر\n",
      "فروش تلفنی, اصول و فنون مذاکره, فروش و بازاریابی\n",
      "Error generating word cloud for job 40: cannot open resource\n",
      "آژانس دیجیتال مارکتینگ ایران وب لایف، مجموعه‌ای از متخصصان حرفه‌ای، خلاق و پرتلاش را گرد هم آورده تا تمام آنچه که کسب‌وکارها برای فعالیت و رشد در فضای دیجیتال نیاز دارند را در اختیارشان قرار دهد و در کنار هم به اهداف مشترک دست یابند.\n",
      "ایران وب لایف همان جایی است که خلاقیت شکوفا می‌شود! به ما بپیوندید تا در کنار تیمی حرفه‌ای و خلاق، روی پروژه‌های متنوع و چالش‌برانگیز کار کنید. ما به شما فرصت می‌دهیم تا مهارت‌های خود را ارتقا دهید و در مسیر رشد حرفه‌ای خود گام بردارید.\n",
      "آژانس دیجیتال مارکتینگ ایران وب لایف در جهت تکمیل تیم سئو خود از همکاران جوان، باانگیزه، خلاق و مستعد برای کار با پروژه‌های حقیقی به صورت حضوری و تمام وقت دعوت به همکاری می‌کند.پیش‌نیازها:برخورداری از روحیه کار تیمیتسلط به مفاهیم سئو و تولید محتواتوانایی نگارش محتوای خلاقانه و مفیدآشنایی با انواع شیوه تولید محتوا بر اساس نوع کسب و کارآشنایی کامل با کیورد ریسرچ و ابزارهای این حوزه مانند Keyword Planner ،KWFinder و ... تسلط کامل به سئو داخلی و تکنیکالآشنایی کامل با ابزارهای تست سرعت سایت تسلط کامل به سئو خارجی، استراتژی‌های لینک بیلدینگ و رپورتاژ آگهی تسلط به ابزارهای آنالیز وبسایت‌ها از قبیل Search Console ،Google Analytics و ... تسلط به ابزارهای سئو و آنالیز سایت مانند Screaming Frog ،Semrush ،Ahrefs و ...قدرت و خلاقیت در حل مسئله علاقه‌مندی‌های کارشناس:علاقه‌مند به حوزه وب، تکنولوژی و بخصوص سئو و رقابتعلاقه‌مند به یادگیریعلاقه‌مند به حوزه دیجیتال مارکتینگ و برندینگعلاقه‌مند به کار با شبکه‌های اجتماعی مختلف علاقه‌مند به نگارشساعت کاری:شنبه تا چهارشنبه: 9:30 تا 18 و پنجشنبه 9:30 تا 13:30\n",
      "SEO, Google Analytics, Google Search Console, Ahrefs\n",
      "Error generating word cloud for job 41: cannot open resource\n",
      "آژانس دیجیتال مارکتینگ ایران وب لایف، مجموعه‌ای از متخصصان حرفه‌ای، خلاق و پرتلاش را گرد هم آورده تا تمام آنچه که کسب‌وکارها برای فعالیت و رشد در فضای دیجیتال نیاز دارند را در اختیارشان قرار دهد و در کنار هم به اهداف مشترک دست یابند.\n",
      "ایران وب لایف همان جایی است که خلاقیت شکوفا می‌شود! به ما بپیوندید تا در کنار تیمی حرفه‌ای و خلاق، روی پروژه‌های متنوع و چالش‌برانگیز کار کنید. ما به شما فرصت می‌دهیم تا مهارت‌های خود را ارتقا دهید و در مسیر رشد حرفه‌ای خود گام بردارید.\n",
      "آژانس دیجیتال مارکتینگ ایران وب لایف در تیم سئو خود از همکاران جوان، با انگیزه، خلاق و مستعد برای کار با پروژه های حقیقی دعوت به همکاری میکند.لطفا توجه بفرمایید: این دوره کارآموزی به هیچ وجه به معنای آموزش مفاهیم اولیه سئو نیست و فرد متقاضی میبایست علاوه بر علاقه، دوره یا تجربه فعالیت سئو یا حداقل شناخت کامل مفاهیم پایه سئو داخلی و خارجی را داشته باشد.توانایی تولید محتوای سئو شده جزو اصلی ترین پیش نیاز هاست.پیش  نیاز هاتسلط به مفاهیم اولیه سئو و تولید محتواتسلط حدودی به گوگل سرچ کنسولتسلط حدودی به گوگل آنالیتیکسترجیح با افرادی است که دوره آموزشی سئو را گذرانده باشند.توانایی نگارش خلاقانه و مفید (نه صرفا ترجمه یا تغییر لحن محتوای رقبا)قدرت و خلاقیت در حل مسئلهعلاقه مندی های کارآموزعلاقه مند به حوزه وب، تکنولوژی و بخصوص سئو و رقابتعلاقه مند به حوزه دیجیتال مارکتینگ علاقه مند به تیم ورکینگ علاقه مند به کار با شبکه های اجتماعی مختلف علاقه مند به نگارشآموزش های حین کارآشنایی با انواع شیوه تولید محتوا بر اساس نوع کسب و کار کار با ابزار های آنالیز وبسایت ها از قبیل گوگل آنالیتیکس، سرچ کنسول و ... آموزش نگارش مقالات موثرآموزش لینک سازی داخلی و خارجی وبسایتآموزش حرفه ای لینک سازی و پلن دهیآموزش HTML و CSS در حد لازم برای سئوی وب اختصاصیو سایر مبانی مرتبط با حوزه SEO و مارکتینگ مشخصات دورهکارآموز حین دوره از حقوق توافقی برخوردار است.مدت زمان کار آموزی حداکثر 3 ماه خواهد بود.پس از دوره در صورت تمایل و توافق طرفین با رعایت تمام ضوابط، کارآموز استخدام خواهد شد.دوره کارآموزی در محل شرکت ایران وب لایف خواهد بود.*** کارآموز بر روی یک یا چند پروژه حقیقی مشغول خواهد بود.*** پس از دوره در صورت عدم تمایل و یا توافق بر ادامه همکاری امکان ارائه گواهی ممهور به مهر شرکت برای کارآموز وجود دارد.*** حتما قبل از ارسال رزومه حوزه فعالیت شرکت را بررسی کنید. ***\n",
      "کارآموزی, SEO\n",
      "Error generating word cloud for job 42: cannot open resource\n",
      "موسسه راهی به سوی نور یک موسسه غیر دولتی بسترساز ،توانمند ، مستقل و خود جوش که در عرصه پیشگیری از آسیبهای اجتماعی فعالیت میکند.\n",
      "این موسسه با سابقه بیش از 20 سال متشکل از کارشناسان، متخصصین و درمانگران که درنوع خود بهترین هستند به کارگرفته شده اند.\n",
      "ما در مجموعه ی راهی بسوی نور و جهت تکمیل کادر تیم تبلیغاتی نیاز به یک خانم کارشناس شبکه های اجتماعی داریم.مهارت ها:اشنا به الگوریتم اینستاگرامکپشن نویسیسناریو نویسیتعامل در شبکه های اجتماعیاگر در این زمینه تجربه و یا توانمندی دارید ما منتظر رزومه شما هستیم\n",
      "شبکه های اجتماعی, کپشن نویسی, تولید محتوا\n",
      "Error generating word cloud for job 43: cannot open resource\n",
      "افرا موسسه‌ای است که برای تقویت زیرساخت‌های نیکوکاری و تبدیل آن به نیرویی تغییرآفرین در جامعه تأسیس شده است. چشم‌انداز ما ساختن دنیایی است که در آن نیکوکاری نه فقط یک عمل خیر، بلکه راهی برای حل عمیق‌ترین مشکلات اجتماعی باشد. با تلفیق دانش مدیریت موسسه‌های مختلف و تجربه تخصصی در حوزه نیکوکاری، به این سازمان‌ها کمک می‌کنیم تا شفافیت و کارآمدی خود را بهبود بخشند و نیکوکاران را به مسیرهایی هدایت کنیم که تأثیری پایدار و واقعی داشته باشند..\n",
      "\n",
      "در افرا، ارزش‌هایی مانند ممتاز بودن، ارزش‌آفرینی، شفافیت، مسئولیت‌پذیری، وفای به تعهدات، کار تیمی، یادگیری و یاددهی و چابکی، چراغ راه ما هستند. ما به تیمی از افراد پرتلاش و موفقیت‌طلب افتخار می‌کنیم که در محیطی پویا و صمیمانه هر روز به دنبال یادگیری و پیشرفت هستند. اگر شما نیز فردی هستید که با اشتیاق برای موفقیت و اثرگذاری قدم برمی‌دارید، افرا همان جایی است که می‌توانید رشد کنید و بخشی از تغییری بزرگ برای جامعه باشید.\n",
      "کارشناس توسعه ارتباطات در مؤسسه افرا مسئولیت برقراری و حفظ ارتباط با موسسه‌های نیکوکاری و نگهداری، تکمیل و به‌روزرسانی داده‌ها در بانک اطلاعات افرا بر عهده دارد. این موقعیت شغلی نیازمند توانایی برقراری ارتباط مؤثر با موسسه‌ها، جمع‌آوری اطلاعات دقیق و به‌روزرسانی منظم داده‌ها است. کارشناس توسعه ارتباطات در افرا باید بتواند نیازها و خواسته‌های موسسه‌های خیریه و نیکوکاری را درک کند و اطلاعات لازم را به صورت کارآمد مدیریت کند. ویژگی‌های شخصیتیتوانایی برقراری ارتباط و موثر: توسعه، برقراری و حفظ ارتباط موثر و حرفه‌ای با موسسه‌های نیکوکاری (با روش‌های ارتباطی مختلف) و اعضای تیم داخلیصبور و متعهد:  توانایی حفظ آرامش و ادامه دادن کارها حتی در شرایط فشار و پیچیدگی. همراهی با دیگر اعضای تیم در پیشبرد امورات کلی موسسه.با دقت و جزئی‌نگر: دقت به جزئیات و اطمینان از صحت اطلاعات وارد شده در بانک اطلاعاتی.حساس به نیازهای مخاطب: توانایی درک و پاسخگویی به نیازهای موسسه‌های نیکوکاری به‌طور مناسب.یادگیرنده و جستجوگرتوانمند در حل مسئله، مدیریت زمان و همکاری تیمیمهارت‌ها:آشنایی با اصول مدیریت اطلاعات: توانایی سازماندهی و مدیریت داده‌ها به‌صورت کارآمد.مهارت‌های کامپیوتری: آشنایی با نرم‌افزار  Excelتوانایی جستجوی اینترنتی: توانایی یافتن و جمع‌آوری اطلاعات به‌صورت آنلاین.مهارت‌های نوشتاری: توانایی تدوین و نگارش انواع گزارشات، مکاتبات و اسناد مرتبط.وظایف این شغل:جمع‌آوری اطلاعات:ارائه برنامه و اجرای برای بهینه‌سازی و توسعه بانک اطلاعات افرا.به‌روزرسانی بانک اطلاعات افرا: بررسی و به‌روزرسانی اطلاعات موسسه‌های نیکوکاری در بانک اطلاعاتی.پایش اطلاعات: اطمینان از صحت و دقت اطلاعات موجود در بانک اطلاعاتی.ارتباط مستمر با موسسه‌ها: برقراری ارتباط منظم با موسسه‌های نیکوکاری برای حفظ ارتباط کارآمد و به‌روزرسانی بانک اطلاعاتی.تهیه گزارش: تهیه گزارش‌های منظم از وضعیت بانک اطلاعاتی و ارائه به مدیران مؤسسه.پشتیبانی: ارائه مشاوره و پاسخ به سؤالات موسسه‌های نیکوکاری در خصوص خدمات افرا.این موقعیت شغلی برای افرادی مناسب است که علاقه‌مند به همکاری با سازمان‌های مردم نهاد و نیکوکاری بوده و دارای مهارت‌های ارتباطی قوی هستند. کارشناس توسعه ارتباطات در مؤسسه افرا نقش حیاتی در بهبود فرآیندهای مدیریت اطلاعات و ارتقاء کیفیت خدمات ارائه شده به موسسه‌های نیکوکاری ایفا می‌کند.مشخصات مورد نیاز:داشتن حداقل مدرک کارشناسیبازه سنی زیر ۳۵ سالداشتن سابقه فعالیت در سازمان‌های مردم‌نهاد مزیت محسوب می‌شود.فارغ‌التحصیلان گرایش‌های مختلف علوم اجتماعی و علوم ارتباطات برای این موقعیت شغلی مناسب هستند.کار تمام‌وقت و به شکل حضوری از ساعت ۸:۳۰ تا ۱۷:۳۰محدوده خیابان بهشتی – قائم مقام\n",
      "ارتباطات, excel, نگارش\n",
      "Error generating word cloud for job 44: cannot open resource\n",
      "فروشگاه اینترنتی تیک تاک گالری، در زمینه فروش ساعت مچی اصل فعالیت می کند.\n",
      "تیک تاک گالری یک تیم جوان و با انگیزه برای رشد و ترقی است. این مجموعه در زمینه انواع ساعت فعالیت می‌کند. در نظر داریم در جهت بهبود فرآیند فروش و انبار یک نیروی پر انرژی جدید به تیم اضافه کنیم. مسئولیت پذیری، دقت و نظم در کار از مهمترین ویژگی‌های فرد ایده آل شغل است. اگر مسئولیت پذیر، دقیق و منظم هستید اینجا جایی برای رشد است.وظایف:بسته بندی و ارسال محصولاتپشتیبانی فروشفروش برای مراجعات حضوری و تلفنیمهارت‌های تخصصی:فن بیان و توانایی متقاعد سازیهمدلی و برطرف کردن نیاز های مشتریانمهارت‌های فردی:توانایی همکاری گروهیمسئولیت پذیرینظم و انضباطمزایا- بیمه- پورسانت بدون سقف- امکان ارتقا شغلیساعت کاریساعت کاری تمام وقت شنبه تا پنج شنبه از ساعت ۹ تا ۱۸\n",
      "اصول و فنون مذاکره, فروش و بازاریابی, فروشندگی, ارتباط با مشتری\n",
      "Error generating word cloud for job 45: cannot open resource\n",
      "شرکت فراتوسعه خطیب خودرو با بیش از یک دهه سابقه در زمینه فروش خودروهای نقد و اقساط مشغول به فعالیت می‌باشد\n",
      "خطیب گروپ با بیش از یک دهه سابقه در زمینه سرمایه گزاری در کلیه امور آماده همکاری با افراد واجد شرایط ذیل  میباشد:حداقل مدرک کارشناسی در رشته حسابداری از دانشگاه معتبر حداقل 5 سال سابقه مدیریت مالی مسلط به نرم افزار حسابداری سپیدارمهارت عملی در کلیه امور مالی مسلط بر قوانین مالیاتی و ارزش افزوده تلاش برای بهبود فرآیندها و ساختارها و رویه های موجود در واحد مالیروحیه کار تیمیتوانایی رهبریمهارت برقراری ارتباط موثرتهیه گزارش های صورت های مالی ،اظهارنامه مالیاتی ،گزارش های مالی مربوط به بودجه، بدهی های حساب مطالبات حساب و هزینه هامدیریت تمام عملیات حسابداری، اطمینان از یکپارچگی دقت و انطباق با خط مشی ها ، قوانین و مقررات شرکتحقوق از 20/000/000 تومان الی 50/000/000 مزایای همکاری با خطیب گروپ: بیمه تامین اجتماعی ، نهار گرم ، پارکینگ رایگان مجموعه حیات سبز ساعت کاری: شنبه الی چهارشنبه 8:15 الی 17 پنجشنبه ها 8:15 الی 14\n",
      "مالی و حسابداری, سرپرستی, نرم افزار سپیدار, امور مالی\n",
      "Error generating word cloud for job 46: cannot open resource\n",
      "معرفی شرکت شباهنگ میثاق جنوب \n",
      "شرکت شباهنگ میثاق جنوب با نام تجاری «رهی‌نو»، سامانه آنلاین خدمات گردشگری در ایران است. ما در رهی‌نو هر سفر را راهی جدید برای کسب تجربه‌های جدید می‌دانیم و همیشه در تلاشیم تا گردشگران و مسافران را به بهترین شکل همراهی کنیم. \n",
      "وب‌سایت www.raheeno.com سامانه جامع رزرواسیون هتل‌های ایران برای سفرهای شخصی و سازمانی و ارائه‌دهنده خدمات CIP فرودگاه امام خمینی (ره) است. تیم پشتیبانی رهی‌نو پل ارتباطی ما با مسافرانی است که «همراهی» در سفر را یک ارزش می‌دانند. \n",
      "\n",
      "فرهنگ کاری شرکت شباهنگ میثاق جنوب \n",
      "گروهی پرتلاش از افراد باانگیزه همره رهی‌نو شده‌اند تا در مسیر توسعه خود و سازمانشان در کنار هم باشند. اینجا «سفر»، فقط یک ابزارِ شغلی نیست. «سفر» بخشی از انگیزه و اشتیاق ما به زندگی است. \n",
      "تیم‌های تخصصی رهی‌نو در فضایی صمیمی و منعطف، برای رسیدن به اهداف سازمان همکاری می‌کنند. رفاقت، به‌روز بودن، خلاقیت و پشتکار مهم‌ترین ویژگی و مهارت‌های همکاران ماست. \n",
      "\n",
      "مزایای همکاری با شرکت شباهنگ میثاق جنوب \n",
      "رهی‌نو مسیری روبه‌پیشرفت است. همکاران ما با ورود به اینجا، سفری در مسیر رشد را آغاز می‌کنند. \n",
      "• ساعت کار منعطف (شناوری شروع و پایان کار) \n",
      "• بیمه تکمیلی درمان \n",
      "• تسهیلات سفر (پایین‌تر از قیمت بازار) \n",
      "• برگزاری دوره‌های آموزشی \n",
      "• جلسات مشاوره رایگان با روانشناس مجموعه \n",
      "• کمک‌هزینه (با بررسی شرایط) \n",
      "• هدایای مناسبتی \n",
      "و...\n",
      "مرکز رزواسیون خدمات گردشگری رهی نومدیر مارکتینگ در محیطی شاداب , پویا و پر نشاطسابقه کار در زمینه مارکتینگ و زیر شاخه های مرتبطحقوق و مزایای عالی با نشاط و پویا و علاقمند به صنعت گردشگری و هتلداری دارای روحیه همکاری تیمی و تعامل مدبرانه و حرفه ای با اعضای تیم\n",
      "مدیریت مارکتینگ, فروش و بازاریابی, مارکتینگ\n",
      "Error generating word cloud for job 47: cannot open resource\n",
      "هسته مركزی شركت رایان پرداز كاوش در سال 1378با هدف فعالیت و مشاركت در عرصه انفورماتیك كشور شكل گرفته و با توسعه شركت در سال 1381 به ثبت رسید. اكنون با سپری شدن بیش از 20 سال از آغاز فعالیت این شركت، مفتخریم كه سهمی در ایجاد ارزش افزوده برای كارفرمایان، مشتریان و صنعت فناوری اطلاعات داشته ایم.\n",
      "این شرکت توانسته است با اتکا به خداوند متعال و استفاده از کارشناسان خبره در صنعت فناوری اطلاعات به مدت 9 سال، رتبه \"1\"  را در زمینه ارائه نرم افزارهای سفارش مشتری توسط شورای عالی انفورماتیك اخذ نماید.\n",
      "شرکت\" رایان پرداز کاوش\" با بیست سال سابقه و داشتن حدود یکصد پرسنل تمام وقت دارای رتبه یک از سازمان مدیریت است، همچنین قرار گرفتن در زمره شرکت‌‎های دانش بنیان و دارا بودن پروانه بهره برداری از وزارت صنعت، معدن، تجارت در حوزه تولید نرم افزار می باشد.ما در رایان پرداز کاوش برای توسعه و بهسازی سرمایه‌های انسانی، مشتری مداری و ارائه خدمات در بهترین و کوتاه‌ترین زمان ممکن ارزش زیادی قائلیم، با حضور در شرکت رایان پرداز کاوش در محیطی حرفه‌ای و دوستانه در طول یکسال مهارت‌هایی فراتر از دانش کاری مورد نیاز فرا می‌گیرید و با امنیت شغلی بالا می‌توانید در بازه‌های زمانی طولانی مدت به همکاری خود ادامه دهید.ما مشتاقانه منتظر شما به عنوان همکار جدید در کارشناس ارشد تحلیل و طراحی نرم افزار هستیم.مهارت‌های اصلی تحلیلی و طراحی: تسلط بر مفاهیم مربوط به تحلیل و طراحی سیستم‌های نرم‌افزاری تسلط به مدلسازی فرآیندها (Business Process Modeling) تسلط به طراحی مدل داده (Data Modeling)توانایی تحلیل و پیاده سازی مدل‌ها و الگوریتم‌های پیچیده تسلط به سناریو نویسی و ایجاد Use Case توانایی تولید و طراحی Prototype تسلط به اصول مستندسازی پروژه‌های نرم‌افزاری آشنایی با حوزه های مرتبط با اطلاعات مکانی ، سنجش از راه دور و فرایندهای مرتبط با املاک مزیت محسوب می شود مهارت‌های فنی: تسلط بر مفاهیم پایگاه داده‌های رابطه‌ای (Relational Databases) آشنایی با مفاهیم تجربه کاربری (UX) تسلط به نرمافزارهای تحلیل و طراحی مانند EA, Axure, Figma و Vusial Paradigm تسلط به مفاهیم و ابزارهای اسکرام (Scrum) شایستگی‌های رفتاری مورد نیاز:مهارت‌های تیمی و ارتباطیتوانایی درک و تشخیص درست نیازهای مشتریان به منظور اعمال آنها در نرم‌افزار توانایی درک مطالب شفاهی و کتبی و تبدیل آن به زبان قابل فهم برای تیم توسعه دهندهرویکرد خلاقانه در حل مسئله توانایی تحمل فشار کاری و انجام به موقع پروژه‌هاشرایط کاری:بیمه تامین اجتماعی از بدو استخدامبیمه تکمیلیپرداخت حقوق بطور منظم و بدون تاخیرروزهای کاری شنبه الی چهارشنبه ساعات کاری از 8 صبح الی 17:20 با 30 دقیقه شناوریمحل کار: محدوده خیابان قائم مقام- مطهری (نزدیک به متروهای مفتح و میرزای شیرازی)\n",
      "طراحی نرم افزار, تحلیل نرم افزار, مستندسازی\n",
      "Error generating word cloud for job 48: cannot open resource\n",
      "فیروزه دیجیتال، دروازه شما به سوی آرامش مالی است. ما به عنوان یک شرکت پیشرو در فناوری مالی در ایران، مفتخریم که با گروه مالی فیروزه و شرکت های زیرمجموعه آن همکاری نزدیک داریم و به عنوان بازوی فناورانه ای که خدمات و ارائه های آنها را توانمند می کند، عمل می کنیم. از طریق همکاری و هم افزایی در داخل گروه، هدف ما ارائه راه حل های ارزشمندتر و جامع تر به مشتریان است.\n",
      "Job DescriptionWe are seeking a skilled and experienced Backend Developer to join our team. As a Backend Developer, you will play a crucial role in building and maintaining the server-side of our applications. Your primary responsibilities will include troubleshooting, improving, and optimizing the performance of our backend systems. You will collaborate with front-end developers, system administrators, web designers, and customers to create efficient and attractive web applications. Additionally, you will be responsible for implementing data protection and security protocols to ensure the integrity of our systems.Roles & Responsibilities:- Compiling and analyzing data, processes, and codes to troubleshoot problems and identify areas for improvement    - Collaborating as a team with front-end developers, system administrators, and web designers to create a functional and attractive web application- Debugging and documenting code that runs on the web server- Implementing data protection and security protocols- Being responsible for optimizing the speed, efficiency, and scalability of applications- Building optimal backend applications that are performing at scale- Developing back-end components to improve responsiveness and overall performance- Coordinating with internal teams to understand user requirements and provide technical solutions- Overseeing the compliance of applications with established quality standardsRequirements:- Bachelor's degree in computer programming, computer science, or a related field- Experience in coding in Python (Django FastAPI) on production-level code for more than 3 years- Strong understanding of the web development cycle and programming techniques and tools- Excellent project and time management skills- Knowledge of Git- Knowledge of Postgres, MySQL, MongoDB, Redis- Knowledge of Docker- Knowledge of Microservices- Self-management skills with a pragmatic mindset focused on continuous improvement- Ability to find pragmatic and innovative solutions to complex problemsIf you are a highly skilled Backend Developer with a passion for creating high-performing web applications, we would like to hear from you. Join our team and contribute to the development of innovative solutions that meet our customers' needs.\n",
      "Python, Git, developer, MySQL\n",
      "Error generating word cloud for job 49: cannot open resource\n",
      "شرکت رسام گستر راویس به عنوان نمونه موفق و شناخته شده در امور واردات و توزیع محصولات شیمیایی و به ویژه مواد اولیه محصولات غذایی، صنایع دارویی و بهداشتی و ... در سال‌های اخیر افتخار همکاری با تولیدکنندگان فعال در این صنعت را داشته است. این شرکت با تکیه بر نیروی انسانی متخصص و همکاری مستقیم با تولیدکنندگان داخلی و خارجی (پتروشیمی‌ها و تولیدکنندگان محصولات شیمیایی) و به صورت تخصصی در واردات انواع اسید سیتریک، تری سدیم سیترات و اسید فسفریک از برندهای معتبر بین المللی در جلب رضایتمندی مشتریان کوشیده است.\n",
      "سابقه کاری در حوزه فروش مواد اولیه صنایع غذایی و صنعتی مزایا:بیمهپاداش و پورسانتسنوات و عیدیمحیطی امن و بدون استرس . آشنا با CRM. بازاریابی و فروش. پیگیری مشتریان. جمع آوری و‌تکمیل اطلاعات مشتریان. ارائه گزارش روزانه و‌ماهیانه از وضعیت تماس ها و مشتریان. قدرت بیان بالا، روابط اجتماعی قوی و روحیه بالا جهت یادگیری و پیشرفتمحدوده پاسدارانساعات کاری : شنبه تا چهارشنبه 8 صبح تا 4:00 عصر پنجشنبه ها تعطیلنزدیک به مترو\n",
      "فروش و بازاریابی, اصول و فنون مذاکره, CRM\n",
      "Error generating word cloud for job 50: cannot open resource\n",
      "شرکت فناوران اطلاعات وستا با چشم انداز بهبود سطح شادی و رفاه بشریت با استفاده از فناوری در سال 1386 تاسیس شد.\n",
      "در راستای چشم انداز و با تکیه بر تجربه بنیانگذار ها،  وستا، فناوری اطلاعات با گرایش طراحی و تولید نرم افزار را بعنوان حوزه فعالیت خود انتخاب کرد.\n",
      "\n",
      "یک دهه تجربه در طراحی و تولید نرم افزار، مشاوره در پیاده سازی راه حل های نرم افزار و ارائه سرویس مبتنی بر ابر به مشتریان سازمانی، صنعتی، دانشگاهی، شرکت های خصوصی و آموزشگاهها باعث کسب تجربه ای گرانقدر برای وستا شده است.\n",
      "\n",
      "با استفاده از تجربه اندوخته شده و توانمندی نیروی با انگیزه، متعهد و متخصص خود تاکنون توانسته ایم اهداف شرکت را با موفقیت پیش برده و خود را به عنوان یکی از شرکت های پیشرو در زمینه های فعالیت خود معرفی کنیم ومحصولات و خدماتی متناسب با نیاز های مشتری ارائه کنیم.  \n",
      "\n",
      "مابه  آینده روشن تر شرکت امیدواریم و در این راستا گام برمیداریم.\n",
      "در حال حاضر نیاز داریم تا به همکاران خوب و با تعهدمان، یک نفر آقا با سمت کارشناس پشتیبانی در قالب همکاری  امریه سربازی اضافه کنیم.جهت واجد شرایط شدن برای استفاده از امریه سربازی در شرکت های دانش بنیان نیاز است که فرد درخواست کننده امتیازهای لازم را کسب کند. یکی از مواردی که باعث کسب امتیاز می شود، حضور در لیست بیمه شرکت دانش بنیانی است که فرد درخواست کننده از طریق آن شرکت، درخواست امریه سربازی را می دهد. به همین دلیلی، حداقل مدت زمان همکاری با شرکت دانش بنیان برای درخواست کننده که به تازگی از تحصیل فارغ شده است، در حدود 12 تا 16ا ماه است، تا بتواند امتیاز لازم در قسمت بیمه تامین اجتماعی را کسب کند.افراد واجد شرایط پس از پذیرفته شدن درخواست امریه سربازی، دوره آموزشی سربازی را طی خواهند کرد و  مابقی مدت زمان سربازی خود را در شرکت دانش بنیان مشغول به کار خواهند شد. این افراد در پایان مدت زمان سربازی، کارت پایان خدمت سربازی را دریافت می کنند.جهت کسب اطلاعات بیشتر از شرایط استفاده از تسهیلات امریه سربازی در شرکت های دانش بنیان، لطفا قسمت مرتبط را در سایت شرکت های دانش بنیان، مطالعه کنید.                                      درخواست کنندگان محترم امریه سربازی دقت کنند که :دوره تحصیلی کارشناسی (لیسانس) را تمام کرده باشید.نباید خدمت سربازی رفته باشید. نباید برای رفتن به خدمت سربازی غیبت داشته باشید.نباید کارت معافیت سربازی گرفته باشید.از مدت زمان فراغت از تحصیل شما حدود 12 تا 15 ماه باقی مانده باشد تا بتوانید با فعالیت در یک شرکت دانش بنیان، امتیاز بیمه تامین اجتماعی را کسب کنید. البته در صورت کمبود زمان فراغت از تحصیل در صورت استخدام توسط شرکت، با هماهنگی شرکت میتوانید ادامه تحصیل دهید تا امتیاز های لازم را حین تحصیل و با فعالیت در شرکت، کسب کنید.در صورتیکه متقاضی امریه سربازی سابقه کاری کمی داشته باشد، یا اصلا سابقه ای نداشته باشد ولی دانش پایه و روحیه یادگیری خوبی داشته باشد، شرکت شرایط کارآموزی و سپس همکاری با بیمه تامین اجتماعی را برای او فراهم خواهد کرد. برای ما خیلی مهم است که بتوانیم در یک محیط با احترام، صمیمت داشته باشیم و به مشتریان عزیزمان در استفاده از محصولات و خدمات شرکت کمک کنیم.شرح مسئولیت : کارشناس پشتیبانی و استقرار نرم افزارشرایط استخدام اتمام دوره تحصیلی کارشناسی (لیسانس) نرفتن به خدمت سربازی (چون این فرصت شغلی مناسب افرادی است که میخواهند از تسهیلات امریه خدمت سربازی استفاده کنند)نداشتن غیبت برای رفتن به خدمت سربازینداشتن کارت معافیت سربازی داشتن فرصت حدود 12 تا 15ماه بعد از فراغت از تحصیل در دوره کارشناسی (لیسانس) به جهت اینکه این مدت زمان برای فعالیت شما در شرکت دانش بنیان نیاز است تا امتیاز لازم برای استفاده از تسهیلات امریه را کسب کنید.داشتن حداقل 6 ماه سابقه کار مرتبطداشتن تحصیلات در رشته کامیپوتر، فناوری اطلاعات یا رشته های مرتبطمهارت های لازم : ترجیحاً آشنا به SQL Serverترجیحاً آشنا به مفاهیم شبکه و سرور دارای صبر و حوصله برای پیگیری موارد اعلامی مشتری و پاسخ گویی به اودارای پشتکار و مشتاق برای یادگیریشرایط و ساعت کاری:از ساعت 8:00 الی 17:00 شنبه تا چهار شنبه استفاده از بیمه تامین اجتماعیانجام فعالیت کاری در برخی از پنج شنبه ها در قالب ماهیانه یا دوره ای به صورت حضوری یا دورکاریمزایا:استفاده از بیمه تکمیلی فضای کاری مناسب برای یادگیری و پیشرفت\n",
      "پشتیبانی نرم افزار, استقرار نرم افزار, Sql Server\n",
      "Error generating word cloud for job 51: cannot open resource\n",
      "شرکت آیبک طب پارس در سال 1386 در تهران تاسیس شده است و در حوزه تجهیزات چشم پزشکی (لنز های جراحی- عدسی-فریم عینک) فعالیت دارد و وارد کننده برند HOYA در ایران می باشد.\n",
      "ما یک شرکت پیشرو در حوزه چشم پزشکی و اپتیک هستیم که برای دفتر  تهران ، نیاز به یک همکار در بخش تراش عدسی عینک داریم  . محدوده شرکت : تهران-خیابان هفت تیربه دنبال شخصی هستیم که :آشنا با ویندوز آشنا به آفیس آشنا به امور فنی (تمام امور مربوطه آموزش داده می شود)آقا و خانم\n",
      "لابراتوار, آفیس, امور فنی\n",
      "Error generating word cloud for job 52: cannot open resource\n",
      "حصین در سال ۱۳۸۳ تأسیس شد و امروز بعنوان یک هلدینگ بیش از ۸۰۰ نفر همکار فعال در ۱۲ شرکت زیرمجموعه دارد: \n",
      "مایکت (فروشگاه بازی و نرم‌افزار اندرویدی)، طاقچه (کتابفروشی و کتابخانهٔ الکترونیکی و صوتی)، مگنت (شبکهٔ هوشمند تبلیغات)، ترابرنت (سامانهٔ هوشمند حمل‌ونقل کالا)، مانی (سکوی یکپارچهٔ خدمات مالی)، گوشی‌شاپ (فروشگاه آنلاین دستگاه‌های دیجیتال)، فخیم (واردات و خدمات پس از فروش گوشی‌های هوشمند)، آلین (خدمات مرتبط با پایانه‌های فروش)، حصین‌گستر (تولیدکنندهٔ کارت‌های هوشمند و دستگاه پوز)، رادین (ارائه‌دهندهٔ راهکارهای نرم‌افزاری و سخت‌افزاری به سازمان‌ها در ارتباط با امنیت فناوری اطلاعات، بانک و پرداخت، اپراتورهای مخابراتی، هوش مصنوعی، بلاک‌چین)، ویرا سرویس (خدمات پس از فروش تلفن همراه) و فناوران اکسون (پخش و فروش B2B موبایل). \n",
      "به لطف حضور همکاران متعهد و پویا، گروه حصین در زوایای مختلف رشد کرده و بر آنیم که این مسیر توسعه پرتلاطم ولی ارزشمند را با قدرت بیشتر ادامه دهیم، از این‌رو از پیوستن افراد با انگیزه و متخصص در موقعیت‌های شغلی متنوع استقبال می‌کنیم. \n",
      "به تیم ما بپیوندید و بخشی از یک مسیر پرارزش باشید که در آن مشارکت شما تاثیر واقعی بر روی زندگی جامعه دارد.\n",
      "ما در ترابرنت (یکی از زیرمجموعه‌های گروه حصین) ،جای خالی یک همکار با انرژی و پیگیر رو حس می‌کنیم که با تکیه بر مهارت‌های ارتباطی و روابط عمومی قوی که داره، خیال ما رو از بابت مسئولیت‌های زیر راحت کنه:یافتن و ارائه پاسخ‌های شخصی سازی‌شده به مشتریان مختلف؛ارائه پیشنهاد در راستای بهبود خدمات واحد امور مشتریان؛مدیریت تماس‌های ورودی و خروجی مشتریان؛ثبت اطلاعات در پنل مربوط پیگیری؛ثبت شکایات کاربران و پیگیری آن؛  پیگیری تیکت‌های ارسالی کاربران. فرد مورد نظر ما صاحب چنین مهارت‌ها و ویژگی‌هایی خواهد بود:دارای مهارت های ارتباطی، کلامی و نوشتاری جهت انجام بهینه مسئولیت‌های محوله؛توانایی انجام چند کار به طور هم‌زمان و توانمند در مدیریت زمان؛دانش و تجربه کافی در به‌کارگیری نرم‌افزارهای CRM؛تسلط بر مجموعه نرم‌افزاری مایکروسافت آفیس؛داشتن روحیه کار تیمی و انعطاف‌پذیری بالا؛مسلط به اصول مدیریت ارتباط با مشتریان؛تاب آوری و سرسختی.شرایط همکاری:امکان دورکاری بعد از دوره سه ماه آزمایشیروز کاری شنبه تا چهارشنبه: 7 الی 16 یا 8 الی 17و 13 الی 22پنجشنبه‌‌ها: 8 الی 13، 8 الی 14؛ 10 الی 16 و 16 الی 22جمعه‌ها: 8 الی 15پنجشنبه و جمعه‌ها به صورت چرخشی و با یک روز تعطیلی پنجشنبه یا جمعه.اگر چنین ویژگی‎‌هایی رو در خودت سراغ داری، این جای خالی متعلق به شماست.\n",
      "امورمشتریان, مرکز تماس, پاسخگویی به مشتریان, ارتباط با مشتریان و پشتیبانی, Microsoft Office\n",
      "Error generating word cloud for job 53: cannot open resource\n",
      "شرکت فیدورا که در زمینه واردات ابزارآلات و تولید دستکش‌های صنعتی فعالیت دارد.\n",
      "یک فضای امن آرام\n",
      "همراه با آموزش در زمینه رشد شخصی و رشد بیزینسی محیا کرده است.\n",
      "که در زمینه تولید دستکش‌های صنعتی و واردات ابزارآلات فعالیت دارد.برای تکمیل تیم فروش خود نیاز به چند همکار که سابقه فروش تلفنی داشته باشند را دارد.شرکت برای پیشرفت نیروهای خود از مشاور قوی و آموزش‌های خوب استفاده میکند.همچنین شرکت پورسانت فروش هم گذاشته که نیروها بتوانند حقوق خیلی خوبی دریافت کنند.توجه داشته باشید که همکارانی که بتوانند در هفته دو تا سه روز ویزیت حضوری انجام بدهند میتوانند برای هر روز انجام دادن ویزیت حضوری حق ماموریت هم دریافت نمایند.\n",
      "فروش, اصول و فنون مذاکره, فروش تلفنی\n",
      "Error generating word cloud for job 54: cannot open resource\n",
      "در «کران»، ما فراتر از تمرکز بر بازده مالی عمل می‌کنیم؛ ما به رشد و بهینه‌سازی همه‌جانبه شرکت‌های تحت سرمایه‌گذاری‌مان متعهد هستیم. هدف ما ایجاد تأثیرات معنادار است که به مرزهای مالی محدود نمی‌شود. تعهد ما به کیفیت بالا در خدمات و ساخت روابط پایدار و قوی با شرکای خود کاملاً مشهود است. به‌عنوان تیمی با دانش و تخصص در حوزه‌های مختلف کسب‌وکار، ما به‌خوبی با هدف توسعه ایران و پیشبرد آن متحد شده‌ایم و به دنبال ایجاد مشارکت‌هایی هستیم که به شکوفایی کشور کمک کند.\n",
      "مسئولیت‌ها:کمک در تهیه صورت‌های مالی و گزارش‌هاثبت تراکنش‌های مالی در نرم‌افزار حسابداری و اطمینان از صحت و درستی آن‌هاتطبیق صورت‌حساب‌های بانکی و سایر سوابق مالیکمک در فعالیت‌های پایان ماه و پایان سال مالیحمایت از فعالیت‌های حساب‌های پرداختنی و دریافتنی، شامل پردازش فاکتورها و پرداخت‌هاکمک در حفظ سوابق مالی و اطمینان از رعایت استانداردهای حسابداریهمکاری با تیم‌های چند رشته‌ای برای اطمینان از گزارش‌دهی مالی دقیقکمک در تهیه جداول حسابرسی و مالیاتی مورد نیازصلاحیت ها :تحصیلات در رشته‌های حسابداری, مالی یا رشته‌های مشابه و مرتبطتوجه قوی به جزئیات و دقتتوانایی مدیریت چندین وظیفه و رعایت مهلت‌هاتسلط بر Microsoft Excel و سایر نرم‌افزارهای حسابداریآشنایی با اصول و روش‌های حسابداری توانایی کار موثر در محیط تیمیمهارت‌های ارتباطی و بین‌فردی قوی\n",
      "حسابداری, امور مالی, امور مالیاتی\n",
      "Error generating word cloud for job 55: cannot open resource\n",
      "با نزدیک به 20 سال تجربه در تولید نرم افزار، افتخار این رو داریم که در این سال ها به هزاران کسب و کار و سازمان کمک کردیم تا سریعتر پیشرفت کنند و به اهداف خود برسند.\n",
      "در دانا پرداز ما اعتقاد داریم کسب و کارها استحقاق این رو دارند که از بهترین نرم افزارها استفاده کنند، نرم افزارهایی که موجب ساده تر شدن کارها و رشد سازمانها بشه. با این نگرش، ما موفقیت خودمون رو در گرو موفقیت مشتریان مون میدونیم و هدف اصلی خودمون رو تلاش برای موفقیت مشتریان خود قرار دادیم.\n",
      " ما در حال حاضر به دنبال همکاران خلاق، با استعداد و با انگیزه هستیم که بتوانند در توسعه و بهبود محصولات به ما کمک کنند. اگر شما هم علاقه مند به پیوستن به تیم دانا پرداز هستید، منتظر رزومه شما هستیم.\n",
      "شرکت دانا پرداز به منظور توسعه تیم فنی خود، از افراد پرانرژی، متعهد و با انگیزه در زمینه طراحی و توسعه نرم‌افزار دعوت به همکاری می‌کند. اگر شما به دنبال فرصتی برای کار در یک تیم پیشرو، خلاق و متعهد در زمینه تولید نرم‌افزارهای حرفه‌ای هستید، این موقعیت شغلی برای شماست! محل کار: حضوری (محدوده غرب تهران)مهارت‌های کلیدی مورد نیاز:تسلط بسیار بالا به #C و مفاهیم شی‌گرایی (OOP).تجربه عملی در توسعه با ASP.NET Core و WebAPI.درک عمیق از اصول و مفاهیم Domain Driven Design (DDD).تسلط به SQL و T-SQL و آشنایی با طراحی و بهینه‌سازی دیتابیس‌های رابطه‌ای.آشنایی کامل با RESTful API و توانایی توسعه APIهای مقیاس‌پذیر و با عملکرد بالا.توانایی تحلیل نیازمندی‌ها و ارائه راه‌حل‌های فنی.مهارت در Debugging و رفع مشکلات پیچیده.تسلط به  Entity Framework (EF Core) تسلط به JSON و XMLتجربه کار با  SignalR و Multithreadingتجربه در امنیت نرم‌افزار و پیاده‌سازی بهترین روش‌های امنیتیآشنایی با موارد زیر به عنوان امتیاز مثبت در نظر گرفته می‌شود:آشنایی با DevOps و فرآیندهای CI/CDتسلط به الگوهای طراحی و اصول SOLIDتجربه توسعه Progressive Web Applications (PWA)شرایط و ویژگی‌های مورد نیاز:حداقل 4 سال سابقه کار تخصصی در برنامه‌نویسیتوانایی کار تیمی و مشارکت فعال در پروژه‌های پیچیدهتعهد به کیفیت بالا، دقت و انضباط در انجام وظایفمهارت در حل مسائل و چالش‌های فنی پیچیدهعلاقه‌مند به یادگیری مداوم و ارتقاء سطح دانش فنی خودخلاقیت و استعداد بالا در توسعه راهکارهای نوآورانهپایبند به اصول حرفه‌ای و اخلاقی در تعاملات سازمانی اگر به دنبال شغلی با چالش‌های هیجان‌انگیز، محیطی حرفه‌ای و پویا هستید، این فرصت مناسب شماست. به تیم ما بپیوندید و در پروژه‌های تأثیرگذار و نوآورانه همکاری کنید!\n",
      "Back-end, RestFul API, OOP, Git, C#, Entity Framework, .NET Core\n",
      "Error generating word cloud for job 56: cannot open resource\n",
      "تحقیق و توسعه خدمات و محصولات ما از سال ۱۳۸۲ خورشیدی (معادل ۲۰۰۳ میلادی) شروع شد. «اینوتکس ایران» به صورت رسمی از مهر ۱۳۸۳ (۲۰۰۴) اعلام موجودیت کرد. ما به عنوان یک شرکت‌دانش بنیان مراحل پیش رشد، رشد و بلوغ خود را در مراکز رشد و پارک‌های فناوری طی کردیم و گام به گام، فناوری‌ها و خدمات خود را توسعه دادیم. در تمام این سال‌ها در راستای توسعه و بلوغ فناوری‌های نرم‌افزاری و خدمات مشاوره‌ای کوشیدیم و در ارائه خدمات حرفه‌ای به مشتریان و جامعه، لحظه‎‌ای دریغ نکردیم. پیشینه اینوتکس ایران، چالش‌ها، فرصت‌ها، پیروزی‌ها و شکست‌هایی را به تصویر می‌کشد که از دل آن هویت سازمان ما بیرون می‌آید.\n",
      "\n",
      "محدوده شرکت:\n",
      "میرداماد، فاصله تا ایستگاه مترو 2 دقیقه\n",
      "\n",
      "ساعت کاری:\n",
      "روزهای شنبه تا چهار شنبه از ساعت 08:00 صبح الی 17:00 عصر و پنج شنبه ها نیز تعطیل می باشد.\n",
      "ما در تیم حرفه‏ ای شرکت خود به دنبال جذب همکار در موقعیت شغلی \"کارشناس پشتیبانی و استقرار نرم افزار\" هستیم، اگر به کار کردن با یک تیم حرفه‌ای علاقه‌مندید و شرایط زیر را دارید، منتظر شما هستیم.مهارت‌های مورد نیاز:نصب و راه اندازی نرم افزارارایه آموزش ادمین و کاربری نرم افزار در صورت نیازدریافت توصیه های لازم در خصوص محیط نصب نرم افزارتوانایی پاسخگویی به سوالات مشتریان در خصوص عملکرد نرم افزارتوانایی رسیدگی به درخواست های مشتریان در مورد اشکالات یا بهبودهای بالقوه نرم افزارتوانایی ارسال گزارش های دوره ای خدمات پشتیبانی نرم افزار برای مشتریانتوانایی انجام تنظیمات و اشکال یابی در محیط SQL Serverنگهداری و راهبری سیستم ثبت درخواست های پشتیبانیتوانایی ارائه خدمات پشتیبانی تلفننی، حضوری و تیکتینگتوانایی به روزرسانی مستندات پشتیبانی نرم افزارشرایط مهم احراز:تسلط به مفاهیم Network+داشتن گواهینامه MCTS یا تجربه کار با پایگاه داده SQL Serverداشتن گواهینامه MCSA یا آشنایی به مفاهیم شبکه و تجربه کار با ویندوز سرورآشنا به IIS و راه اندازی های مربوطهعلاوه بر شرایط احراز، داشتن موارد زیر امتیاز محسوب می گردد:آشنایی با مفاهیم مدیریت دانش Knowledge Managementداشتن گواهینامه LPIC1  یا دانش کار با سیستم عامل لیونکسداشتن دانش یا تجربه کار با پایگاه داده PostgreSQLآشنایی با وب سرویس و مفاهیم API و کار با Postmanویژگی های فردی:بازه سنی بین 20 تا 30 سالبرخورداری از روحیه کارگروهی و مهارت در برقراری ارتباط مؤثر با دیگرانتوانایی فراگیری دانش حوزه‌ای که پشتیبانی می‌کند و زمینه‌های مرتبط با آن و به روزآوری مداوم دانش خودتوانایی رسیدگی همزمان به وظایف گوناگونداشتن دقت بالا و توجه به جزئیاتتوانایی برگزاری و اداره جلسات آموزشیتوانایی انجام مستندسازی و ارائه گزارش به مدیرانمزایا:پرداخت پاداش های موردیمحیط کاری شرکت خلاق، پویا، حرفه‌ای، صمیمی و همراه با همکارانی جوان و پر انرژی است.فرصت‌های یادگیری و رشد حرفه‌ای و جذب در آینده\n",
      "پشتیبانی نرم افزار, استقرار نرم افزار, Sql Server\n",
      "Error generating word cloud for job 57: cannot open resource\n",
      "ما در تیم خود به دنبال یک معمار خلاق و با انگیزه هستیم. در شرکت ما، صداقت، صبوری و احترام اصولی است که بر آن استواریم و به تعامل سازنده با همکارانمان ارزش می‌دهیم.\n",
      "**آگهی استخدام مهندس معمار جوان**آیا به دنبال محیطی پویا و مسیرهای حرفه‌ای جدید هستید؟ ما در تیم خود به دنبال یک معمار خلاق و با انگیزه هستیم. در شرکت ما، صداقت، صبوری و احترام اصولی است که بر آن استواریم و به تعامل سازنده با همکارانمان ارزش می‌دهیم.اگر به طراحی داخلی و خارجی علاقه دارید و خلاقیت، آشنایی با ضوابط و تسلط به نرم‌افزارهای سه‌بعدی و دوبعدی بخشی از توانمندی‌های شماست و حدودا ۵ تا ۷ سال در این زمینه تجربه دارید، خوشحال می‌شویم شما را در کنار خود داشته باشیم.منتظر ارسال نمونه کارها و سوابق شما هستیم تا بیشتر با شما، توانمندی‌ها و دیدگاه‌هایتان آشنا شویم.\n",
      "طراحی معماری, AutoCad, 3DMAX, revit\n",
      "Error generating word cloud for job 58: cannot open resource\n",
      "این شرکت در حوزه واردات، بازرگانی و پخش قطعات یدکی خودرو فعالیت میکند.\n",
      "شرکت ثمین خودرو پویا، پیشرو در زمینه فروش قطعات خودرو و تجهیزات تعمیرگاهی، به دنبال یک کارشناس فروش تلفنی حرفه‌ای و متعهد برای پیوستن به تیم فروش خود می‌باشد.وظایف و مسئولیت‌ها:برقراری تماس با مشتریان و ارائه مشاوره تخصصی در خصوص محصولات و خدماتشناسایی و تحلیل نیازهای مشتریان به منظور ارائه راهکارهای بهینه  پیگیری و انجام فرآیند فروش و ثبت سفارشات  حفظ و گسترش روابط با مشتریان کلیدی و ایجاد فرصت‌های جدید فروشتهیه گزارش‌های دقیق و منظم از فعالیت‌ها و نتایج فروش به مدیریتشرایط احراز:  حداقل مدرک تحصیلی کارشناسی   تجربه حداقل ۲ سال در فروش تلفنیتوانایی قوی در برقراری ارتباط و ایجاد ارتباط مؤثر با مشتریانآشنایی به نرم‌افزارهای CRM روحیه کار تیمی مزایای همکاری:حقوق و مزایای رقابتیبیمه تأمین اجتماعی و مزایای جانبیفرصت‌های آموزشی و توسعه مهارت‌های فردیمحیط کاری حرفه‌ای و پویا با فرصت رشدپورسانت عالی\n",
      "فروش, اصول و فنون مذاکره, فروش تلفنی\n",
      "Error generating word cloud for job 59: cannot open resource\n",
      "به جمعِ ما در بامبو خوش اومدی! اینجا جاییه که هوش مصنوعی و سفرهای رویایی با هم یکی می‌شن تا یه چیز تازه و خفن تو صنعت مسافرت به وجود بیاد. تازه کاریم، اما هدفهامون بلندپروازانه‌ست و دوست داریم با کمک فناوری‌های جدید، لبخند رو روی لب مسافرامون بنشونیم. فکر می‌کنی تو هم می‌تونی تو این ماجراجویی شریک باشی؟\n",
      "\n",
      "دنبال چهره‌های جدیدی می‌گردیم که عاشق چالش باشن، دلشون بخواد یه چیزهای جدید یاد بگیرن و تو یه فضای پر از حمایت و دوستی پیشرفت کنن. اینجا تو بامبو، می‌تونی کنار افرادی باشی که همه‌شون دنبال ساختن چیزهای باحال و معنادارن.\n",
      "ما در بامبو، دنبال یه متخصص در حوزه‌ی Back-End هستیم! آیا آماده‌ای تا با مهارت‌های کدنویسی‌ت، دنیای تکنولوژی رو متحول کنی؟ اگه تمایل داری توی یه محیط پر انرژی و خلاق کار کنی، ما مشتاق آشنایی با تو هستیم!ما دنبال چه کسی هستیم؟تسلط بر Node.js: با بیش از ۳ سال تجربه در استفاده از Node.js و فریم‌ورک‌های محبوب مثل Express و NestJS، تو باید بتونی با این محیط، پروژه‌های بزرگ رو رهبری کنی.مهارت در کار با دیتابیس‌ها: مسلط به MongoDB، PostgreSQL و استفاده از ORM ها مثل Mongoose و Sequelize برای تعامل اثربخش با دیتابیس‌ها.تسلط در کار با میکروسرویس‌ها و Docker: تجربه کار با معماری میکروسرویس‌ها و استفاده از Docker برای ایجاد محیط‌های توسعه قابل پیکربندی و یکپارچه.توسعه API‌های قدرتمند: تخصص در ساخت API‌های RESTful و GraphQL و توانایی ایجاد ارتباطات مؤثر بین سرور و کلاینت.مهارت‌های تست و TDD: تسلط بر تست‌های خودکار و توسعه محصول با استفاده از رویکرد TDD برای اطمینان از کد بدون نقص.مهارت‌های امنیتی: دانش در زمینه امنیت وب و توانایی اعمال بهترین شیوه‌ها برای حفاظت از برنامه‌ها و داده‌های حساس.تجربه کار با Redis و Elasticsearch: استفاده از Redis برای مدیریت حافظه نهان و Elasticsearch برای جست‌وجو و تجزیه و تحلیل داده‌ها بصورت بهینه.خب، حالا چرا ما؟محیط کاری پویا و دوستانه، جایی که هر ایده‌ای ارزشمندهجایی که برای رشد حرفه‌ای و یادگیری مستمر، فرصت زیادی وجود دارهتیمی که در بهترین و سخت‌ترین لحظات کنارتن و حمایتت می‌کننپروژه‌های چالش‌برانگیز که هر روزت رو پر از هیجان و یادگیری می‌کننشرایط کاری انعطاف‌پذیر، چون ما به تعادل بین کار و زندگی اهمیت می‌دیمچالش‌های فنی که هر روز مغزت رو ورزش می‌دن و تو رو تشنه‌ی یادگیری بیشتر می‌کننآزادی در مسئولیت‌پذیریه، مسئولیت‌پذیری هم در آزادیهدر عصری که دورکاری نه‌تنها یک امکان بلکه یک مزیته، ما به تو این فرصت رو می‌دیم که هر جای دنیا که هستی، به تیم ما بپیوندی. با این شیوه کاری، می‌تونی تعادل بین کار و زندگی شخصی‌ت رو به بهترین شکل حفظ کنی.آماده‌ای به جمع ما بپیوندی؟ارسال رزومه و نامه‌ای که نشون‌دهنده‌ی انگیزه و علاقه‌مندی تو به این حوزه‌ست، اولین قدم تو برای اومدن به جمع ماست. منتظر شنیدن از تو و داستان‌های تکنولوژیکی که پشت سر گذاشتی هستیم!پس، بیا و بخشی از داستان تکنولوژیکی ما شو. با ما هر روز به سمت آینده‌ای روشن‌تر قدم برمی‌داری.\n",
      "Node.js, Back-end, PostgreSQL, MongoDB\n",
      "Error generating word cloud for job 60: cannot open resource\n",
      "\n",
      "Successfully processed 60 jobs\n",
      "Word clouds saved in: C:\\Users\\saeid\\jobinja_processed_data\\wordclouds\n",
      "Processed file saved to: C:\\Users\\saeid\\jobinja_processed_data\\jobs_with_wordclouds.json\n",
      "\n",
      "Processing Summary:\n",
      "--------------------------------------------------\n",
      "Total jobs processed: 60\n",
      "Jobs with word clouds: 0\n",
      "\n",
      "Sample processed job:\n",
      "ID: 1\n",
      "Company: pishro-sanat-arshit-1\n",
      "Title: استخدام مسئول دفتر (مسلط به انگلیسی-خانم)\n",
      "Has word cloud: No\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from hazm import Normalizer, word_tokenize\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "class JobinjaDataProcessor:\n",
    "    def __init__(self, save_dir: str = \"jobinja_data\"):\n",
    "        \"\"\"\n",
    "        Initialize the processor\n",
    "        \"\"\"\n",
    "        self.save_dir = Path(save_dir.strip().replace('\\x0b', '').replace('\\x07', ''))\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Create directory for word clouds\n",
    "        self.wordcloud_dir = self.save_dir / \"wordclouds\"\n",
    "        self.wordcloud_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Initialize Persian text processor\n",
    "        self.normalizer = Normalizer()\n",
    "        \n",
    "        # Persian stopwords\n",
    "        self.stopwords = set([\n",
    "            'و', 'در', 'به', 'از', 'که', 'می', 'این', 'است', 'را', 'با', 'های', 'برای',\n",
    "            'آن', 'خود', 'تا', 'کرد', 'بر', 'هر', 'نیز', 'ما', 'اما', 'یا', 'شد', 'او',\n",
    "            'ها', 'هم', 'شده', 'کند', 'من', 'باید', 'دارد', 'دیگر', 'همه', 'شود', 'یک'\n",
    "        ])\n",
    "\n",
    "    def create_circular_mask(self, width, height):\n",
    "        \"\"\"Create circular mask for word cloud\"\"\"\n",
    "        center = (int(width/2), int(height/2))\n",
    "        radius = min(width, height) // 2\n",
    "        Y, X = np.ogrid[:height, :width]\n",
    "        dist_from_center = np.sqrt((X - center[0])**2 + (Y - center[1])**2)\n",
    "        mask = dist_from_center <= radius\n",
    "        return mask\n",
    "\n",
    "    def preprocess_persian_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Preprocess Persian text for word cloud\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "            \n",
    "        # Normalize text\n",
    "        text = self.normalizer.normalize(text)\n",
    "        \n",
    "        # Tokenize\n",
    "        words = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords and short words\n",
    "        words = [w for w in words if w not in self.stopwords and len(w) > 1]\n",
    "        \n",
    "        return ' '.join(words)\n",
    "\n",
    "    def generate_wordcloud(self, text: str, job_id: int) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Generate word cloud for Persian text\n",
    "        \n",
    "        Returns:\n",
    "            Base64 encoded image string\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not text:\n",
    "                return None\n",
    "                \n",
    "            # Preprocess text\n",
    "            processed_text = self.preprocess_persian_text(text)\n",
    "            if not processed_text:\n",
    "                return None\n",
    "                \n",
    "            # Reshape Persian text\n",
    "            processed_text = arabic_reshaper.reshape(processed_text)\n",
    "            processed_text = get_display(processed_text)\n",
    "            \n",
    "            # Create word cloud\n",
    "            mask = self.create_circular_mask(400, 400)\n",
    "            wordcloud = WordCloud(\n",
    "                width=400,\n",
    "                height=400,\n",
    "                background_color='white',\n",
    "                mask=mask,\n",
    "                font_path='Vazir.ttf',  # Make sure to have this Persian font file\n",
    "                max_words=100\n",
    "            ).generate(processed_text)\n",
    "            \n",
    "            # Save image\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Save to BytesIO\n",
    "            img_data = BytesIO()\n",
    "            plt.savefig(img_data, format='png', bbox_inches='tight', pad_inches=0)\n",
    "            plt.close()\n",
    "            \n",
    "            # Convert to base64\n",
    "            img_data.seek(0)\n",
    "            img_base64 = base64.b64encode(img_data.read()).decode()\n",
    "            \n",
    "            # Save individual word cloud image\n",
    "            wordcloud_path = self.wordcloud_dir / f\"wordcloud_{job_id}.png\"\n",
    "            wordcloud.to_file(str(wordcloud_path))\n",
    "            \n",
    "            return img_base64\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating word cloud for job {job_id}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_company_name(self, url: str) -> str:\n",
    "        \"\"\"Extract company name from job URL\"\"\"\n",
    "        try:\n",
    "            pattern = r'companies/([^/]+)/jobs'\n",
    "            match = re.search(pattern, url)\n",
    "            return match.group(1) if match else \"unknown\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting company name from URL {url}: {e}\")\n",
    "            return \"unknown\"\n",
    "\n",
    "    def process_job(self, job: Dict, index: int) -> Dict:\n",
    "        \"\"\"Process a single job entry\"\"\"\n",
    "        # Extract company name\n",
    "        company_name = self.extract_company_name(job.get('url', ''))\n",
    "        \n",
    "        # Generate word cloud\n",
    "        company_desc = job.get('معرفی شرکت', '')\n",
    "        print(company_desc)\n",
    "        \n",
    "        \n",
    "        job_desc = job.get(\"شرح موقعیت شغلی\",'')\n",
    "        print(job_desc)\n",
    "        skills = job.get(\"مهارت‌های مورد نیاز\",'')\n",
    "        print(skills)\n",
    "        \n",
    "        wordcloud_base64 = self.generate_wordcloud(company_desc, index)\n",
    "        \n",
    "        # Create processed job dict\n",
    "        processed_job = {\n",
    "            'id': index,\n",
    "            'company_name': company_name,\n",
    "            'wordcloud': wordcloud_base64,\n",
    "            **job\n",
    "        }\n",
    "        \n",
    "        return processed_job\n",
    "\n",
    "    def process_file(self, input_file: str, output_file: str = None):\n",
    "        \"\"\"Process a JSON file\"\"\"\n",
    "        try:\n",
    "            # Validate input file\n",
    "            input_path = Path(input_file)\n",
    "            if not input_path.exists():\n",
    "                print(f\"Error: Input file not found: {input_path}\")\n",
    "                return\n",
    "\n",
    "            # Set output file\n",
    "            if output_file is None:\n",
    "                timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "                output_path = self.save_dir / f\"jobs_processed_{timestamp}.json\"\n",
    "            else:\n",
    "                output_path = Path(output_file)\n",
    "\n",
    "            print(f\"Processing file: {input_path}\")\n",
    "\n",
    "            # Read input file\n",
    "            with open(input_path, 'r', encoding='utf-8') as f:\n",
    "                jobs = json.load(f)\n",
    "\n",
    "            # Process jobs\n",
    "            processed_jobs = []\n",
    "            total_jobs = len(jobs)\n",
    "            \n",
    "            print(f\"\\nProcessing {total_jobs} jobs...\")\n",
    "            for i, job in enumerate(jobs, 1):\n",
    "                print(f\"Processing job {i}/{total_jobs}\", end='\\r')\n",
    "                processed_job = self.process_job(job, i)\n",
    "                processed_jobs.append(processed_job)\n",
    "\n",
    "            # Save processed jobs\n",
    "            output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(processed_jobs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            print(f\"\\nSuccessfully processed {len(processed_jobs)} jobs\")\n",
    "            print(f\"Word clouds saved in: {self.wordcloud_dir}\")\n",
    "            print(f\"Processed file saved to: {output_path}\")\n",
    "            \n",
    "            # Print sample and stats\n",
    "            self.print_summary(processed_jobs)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing file: {e}\")\n",
    "            print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "    def print_summary(self, jobs: List[Dict]):\n",
    "        \"\"\"Print summary of processed jobs\"\"\"\n",
    "        print(\"\\nProcessing Summary:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Count jobs with word clouds\n",
    "        jobs_with_clouds = sum(1 for job in jobs if job.get('wordcloud'))\n",
    "        print(f\"Total jobs processed: {len(jobs)}\")\n",
    "        print(f\"Jobs with word clouds: {jobs_with_clouds}\")\n",
    "        \n",
    "        # Print sample job\n",
    "        if jobs:\n",
    "            print(\"\\nSample processed job:\")\n",
    "            job = jobs[0]\n",
    "            print(f\"ID: {job['id']}\")\n",
    "            print(f\"Company: {job['company_name']}\")\n",
    "            print(f\"Title: {job.get('عنوان شغلی', 'No title')}\")\n",
    "            print(f\"Has word cloud: {'Yes' if job.get('wordcloud') else 'No'}\")\n",
    "\n",
    "def main():\n",
    "    # Get current directory\n",
    "    current_dir = Path.cwd()\n",
    "    \n",
    "    # Initialize processor\n",
    "    save_dir = current_dir / \"jobinja_processed_data\"\n",
    "    processor = JobinjaDataProcessor(save_dir=str(save_dir))\n",
    "    \n",
    "    # Set file paths\n",
    "    input_file = current_dir / \"JobInja\" / \"v11\" / \"all_jobs.json\"\n",
    "    output_file = save_dir / \"jobs_with_wordclouds.json\"\n",
    "    \n",
    "    print(\"Starting job processing...\")\n",
    "    processor.process_file(\n",
    "        input_file=str(input_file),\n",
    "        output_file=str(output_file)\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3667a101-1dfc-4fa4-9147-a669e3016e06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs file urls.txt not found. Using example URL...\n",
      "Processing batch 1/1 (60 URLs)\n",
      "Saved JSON data to JobInja\\v11\\batch_1.json\n",
      "Saved CSV data to JobInja\\v11\\batch_1.csv\n",
      "Saved JSON data to JobInja\\v11\\all_jobs.json\n",
      "Saved CSV data to JobInja\\v11\\all_jobs.csv\n",
      "\n",
      "Scraping completed:\n",
      "Total jobs scraped: 60\n",
      "Results saved in: JobInja/v11\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "from typing import Dict, List, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "\n",
    "class JobinjaScraper:\n",
    "    def __init__(self, save_dir: str = \"output\", delay: float = 1.0, max_workers: int = 5):\n",
    "        \"\"\"\n",
    "        Initialize the scraper\n",
    "        \n",
    "        Args:\n",
    "            save_dir: Directory to save output files\n",
    "            delay: Time to wait between requests (seconds)\n",
    "            max_workers: Maximum number of concurrent threads\n",
    "        \"\"\"\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.delay = delay\n",
    "        self.max_workers = max_workers\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        # Create save directory if it doesn't exist\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def get_page_content(self, url: str) -> Optional[str]:\n",
    "        \"\"\"Fetch the page content from the given URL\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching page {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_job_info(self, html_content: str, url: str) -> Dict:\n",
    "        \"\"\"Extract job information from HTML content\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        job_info = {'url': url}  # Include URL in the results\n",
    "\n",
    "        # Extract job title\n",
    "        title_div = soup.select_one('div.c-jobView__titleText h1')\n",
    "        if title_div:\n",
    "            job_info['عنوان شغلی'] = title_div.text.strip()\n",
    "\n",
    "        # Extract information from the first info box\n",
    "        info_box = soup.select_one('ul.c-jobView__firstInfoBox')\n",
    "        if info_box:\n",
    "            info_items = info_box.select('li.c-infoBox__item')\n",
    "            for item in info_items:\n",
    "                title = item.select_one('h4.c-infoBox__itemTitle')\n",
    "                value = item.select_one('div.tags span.black')\n",
    "                if title and value:\n",
    "                    key = title.text.strip()\n",
    "                    job_info[key] = value.text.strip()\n",
    "\n",
    "        # Extract company introduction\n",
    "        company_intro_title = soup.find('h4', class_='o-box__title', string='معرفی شرکت')\n",
    "        if company_intro_title:\n",
    "            company_intro = company_intro_title.find_next('div', class_='o-box__text')\n",
    "            if company_intro:\n",
    "                job_info['معرفی شرکت'] = company_intro.text.strip()\n",
    "\n",
    "        # Extract job description\n",
    "        job_desc_title = soup.find('h4', class_='o-box__title', string='شرح موقعیت شغلی')\n",
    "        if job_desc_title:\n",
    "            job_desc = job_desc_title.find_next('div', class_='o-box__text')\n",
    "            if job_desc:\n",
    "                job_info['شرح موقعیت شغلی'] = job_desc.text.strip()\n",
    "\n",
    "        # Extract bottom info box information\n",
    "        bottom_info_box = soup.select('ul.c-infoBox.u-mB0 li.c-infoBox__item')\n",
    "        for item in bottom_info_box:\n",
    "            title = item.select_one('h4.c-infoBox__itemTitle')\n",
    "            values = item.select('div.tags span.black')\n",
    "            if title and values:\n",
    "                key = title.text.strip()\n",
    "                if len(values) > 1:\n",
    "                    job_info[key] = ', '.join(val.text.strip() for val in values)\n",
    "                else:\n",
    "                    job_info[key] = values[0].text.strip()\n",
    "\n",
    "        return job_info\n",
    "\n",
    "    def scrape_job(self, url: str) -> Optional[Dict]:\n",
    "        \"\"\"Scrape a single job posting\"\"\"\n",
    "        html_content = self.get_page_content(url)\n",
    "        if html_content:\n",
    "            job_info = self.extract_job_info(html_content, url)\n",
    "            time.sleep(self.delay)  # Respect the site by waiting between requests\n",
    "            return job_info\n",
    "        return None\n",
    "\n",
    "    def scrape_jobs(self, urls: List[str], batch_size: int = 100) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Scrape multiple job postings concurrently with batch processing\n",
    "        \n",
    "        Args:\n",
    "            urls: List of job posting URLs\n",
    "            batch_size: Number of URLs to process in each batch\n",
    "        Returns:\n",
    "            List of dictionaries containing job information\n",
    "        \"\"\"\n",
    "        all_results = []\n",
    "        total_batches = (len(urls) + batch_size - 1) // batch_size\n",
    "\n",
    "        for batch_num in range(total_batches):\n",
    "            start_idx = batch_num * batch_size\n",
    "            end_idx = min((batch_num + 1) * batch_size, len(urls))\n",
    "            batch_urls = urls[start_idx:end_idx]\n",
    "\n",
    "            print(f\"Processing batch {batch_num + 1}/{total_batches} ({len(batch_urls)} URLs)\")\n",
    "\n",
    "            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "                futures = [executor.submit(self.scrape_job, url) for url in batch_urls]\n",
    "                batch_results = []\n",
    "                for future in futures:\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        batch_results.append(result)\n",
    "\n",
    "            # Save batch results\n",
    "            if batch_results:\n",
    "                batch_filename = f\"batch_{batch_num + 1}\"\n",
    "                self.save_batch(batch_results, batch_filename)\n",
    "                all_results.extend(batch_results)\n",
    "\n",
    "        return all_results\n",
    "\n",
    "    def save_batch(self, data: List[Dict], filename_prefix: str):\n",
    "        \"\"\"Save batch results to both JSON and CSV\"\"\"\n",
    "        # Save to JSON\n",
    "        json_path = self.save_dir / f\"{filename_prefix}.json\"\n",
    "        self.save_to_json(data, json_path)\n",
    "\n",
    "        # Save to CSV\n",
    "        csv_path = self.save_dir / f\"{filename_prefix}.csv\"\n",
    "        self.save_to_csv(data, csv_path)\n",
    "\n",
    "    def save_to_json(self, data: List[Dict], filepath: Path):\n",
    "        \"\"\"Save results to JSON file\"\"\"\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Saved JSON data to {filepath}\")\n",
    "\n",
    "    def save_to_csv(self, data: List[Dict], filepath: Path):\n",
    "        \"\"\"Save results to CSV file\"\"\"\n",
    "        if not data:\n",
    "            print(\"No data to save\")\n",
    "            return\n",
    "\n",
    "        # Get all unique keys from all dictionaries\n",
    "        fieldnames = set()\n",
    "        for item in data:\n",
    "            fieldnames.update(item.keys())\n",
    "        fieldnames = sorted(list(fieldnames))\n",
    "\n",
    "        with open(filepath, 'w', encoding='utf-8', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(data)\n",
    "        print(f\"Saved CSV data to {filepath}\")\n",
    "\n",
    "def main():\n",
    "    # Example usage with custom save directory\n",
    "    save_dir = \"JobInja/v11\"  # You can change this to your preferred directory\n",
    "    \n",
    "    # Load URLs from file\n",
    "    urls_file = \"urls.txt\"  # Create this file with your URLs\n",
    "    try:\n",
    "        with open(urls_file, 'r', encoding='utf-8') as f:\n",
    "            urls = [line.strip() for line in f if line.strip()]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"URLs file {urls_file} not found. Using example URL...\")\n",
    "        urls  = [\n",
    "  \"https://jobinja.ir/companies/pishro-sanat-arshit-1/jobs/Apb6/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D9%85%D8%B3%D8%A6%D9%88%D9%84-%D8%AF%D9%81%D8%AA%D8%B1-%D9%85%D8%B3%D9%84%D8%B7-%D8%A8%D9%87-%D8%A7%D9%86%DA%AF%D9%84%DB%8C%D8%B3%DB%8C-%D8%AE%D8%A7%D9%86%D9%85-%D8%AF%D8%B1-%D8%B4%D8%B1%DA%A9%D8%AA-%D9%BE%DB%8C%D8%B4%D8%B1%D9%88%D8%B5%D9%86%D8%B9%D8%AA-%D8%A2%D8%B1%D8%B4%DB%8C%D8%AA\",\n",
    "  \"https://jobinja.ir/companies/verge/jobs/ApET/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-senior-front-end-developer-%D8%AF%D8%B1-%DA%A9%D9%84%D8%A7%D8%AF%DB%8C%D9%86%D8%A7%D8%AA%D9%88%D8%B1\",\n",
    "  \"https://jobinja.ir/companies/aiwaweb/jobs/A6Xo/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-%D8%B3%D8%A6%D9%88-%DA%A9%D8%B1%D8%AC-%D8%AF%D8%B1-%D8%A2%DB%8C%D9%88%D8%A7-%D9%88%D8%A8\",\n",
    "  \"https://jobinja.ir/companies/wizidar-1/jobs/AmwA/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D9%81%DB%8C%D9%84%D9%85%D8%A8%D8%B1%D8%AF%D8%A7%D8%B1-%D9%88-%D8%AA%D8%AF%D9%88%DB%8C%D9%86%DA%AF%D8%B1-%D8%A2%D9%82%D8%A7-%D8%AF%D8%B1-%D9%88%DB%8C%D8%B2%DB%8C%D8%AF%D8%A7%D8%B1\",\n",
    "  \"https://jobinja.ir/companies/directam/jobs/A40w/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-devops-%D8%A7%D8%B5%D9%81%D9%87%D8%A7%D9%86-%D8%AF%D8%B1-%D8%AF%D8%A7%DB%8C%D8%B1%DA%A9%D8%AA%D9%85\",\n",
    "  \"https://jobinja.ir/companies/nizva-1/jobs/A7IG/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D9%85%D8%AF%DB%8C%D8%B1-%D9%81%D8%B1%D9%88%D8%B4-%D8%AE%D8%A7%D9%86%D9%85-%D8%AF%D8%B1-%D9%86%DB%8C%D8%B2%D9%88%D8%A7\",\n",
    "  \"https://jobinja.ir/companies/nizva-1/jobs/Am4F/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D9%81%D8%B1%D9%88%D8%B4%D9%86%D8%AF%D9%87-%D8%AE%D8%A7%D9%86%D9%85-%D8%AF%D8%B1-%D9%86%DB%8C%D8%B2%D9%88%D8%A7\",\n",
    "  \"https://jobinja.ir/companies/autokhatib/jobs/AmWZ/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D9%88%DA%A9%DB%8C%D9%84-%D9%BE%D8%A7%DB%8C%D9%87-%DB%8C%DA%A9-%D8%AF%D8%A7%D8%AF%DA%AF%D8%B3%D8%AA%D8%B1%DB%8C-%D8%AF%D8%B1-%D8%A7%D8%AA%D9%88-%D8%AE%D8%B7%DB%8C%D8%A8\",\n",
    "  \"https://jobinja.ir/companies/veerayco/jobs/A7y2/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D9%85%D9%87%D9%86%D8%AF%D8%B3-%D8%A8%D8%B1%D9%82-%D9%88-%D8%A7%D9%84%DA%A9%D8%AA%D8%B1%D9%88%D9%86%DB%8C%DA%A9-%D8%A2%D9%82%D8%A7-%D9%85%D8%B4%D9%87%D8%AF-%D8%AF%D8%B1-%D9%88%DB%8C%D8%B1%D8%A7%DB%8C%DA%A9%D9%88\",\n",
    "  \"https://jobinja.ir/companies/auto-bi-nazir/jobs/A7hw/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D8%B3%D8%B1%D9%BE%D8%B1%D8%B3%D8%AA-%D8%B9%D9%85%D9%84%DB%8C%D8%A7%D8%AA-%D9%81%D8%B1%D9%88%D8%B4-%D8%AF%D8%B1-%D9%85%D9%87%D8%A7%D9%85-%D8%AE%D9%88%D8%AF%D8%B1%D9%88-%D8%A8%DB%8C-%D9%86%D8%B8%DB%8C%D8%B1\",\n",
    "  \"https://jobinja.ir/companies/auto-bi-nazir/jobs/AvwM/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-%D9%81%D8%B1%D9%88%D8%B4-%D8%AE%D9%88%D8%AF%D8%B1%D9%88-%D8%AF%D8%B1-%D9%85%D9%87%D8%A7%D9%85-%D8%AE%D9%88%D8%AF%D8%B1%D9%88-%D8%A8%DB%8C-%D9%86%D8%B8%DB%8C%D8%B1\",\n",
    "  \"https://jobinja.ir/companies/safiran-gasht-1/jobs/Avia/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%A2%D9%85%D9%88%D8%B2-%D9%81%D8%B1%D9%88%D8%B4-%D8%AA%D9%88%D8%B1-%D8%A7%D8%B1%D9%88%D9%BE%D8%A7-%D8%AE%D8%A7%D9%86%D9%85-%D8%AF%D8%B1-%D8%B3%D9%81%DB%8C%D8%B1%D8%A7%D9%86-%DA%AF%D8%B4%D8%AA\",\n",
    "  \"https://jobinja.ir/companies/artanburstap/jobs/AvPM/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-%D9%81%D8%B1%D9%88%D8%B4-%D8%A7%D9%85%D9%88%D8%B1-%D9%85%D9%87%D8%A7%D8%AC%D8%B1%D8%AA%DB%8C-%D8%AE%D8%A7%D9%86%D9%85-%D8%AF%D8%B1-%D8%A2%D8%B1%D8%AA%D8%A7%D9%86-%D9%85%D8%B4%D8%A7%D9%88%D8%B1%D8%A7%D9%86-%D9%85%D9%87%D8%A7%D8%AC%D8%B1\",\n",
    "  \"https://jobinja.ir/companies/saman-insurance-221/jobs/AjJT/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-%D8%B5%D8%AF%D9%88%D8%B1-%D8%A8%DB%8C%D9%85%D9%87-%D8%AF%D8%B1-%D8%A8%DB%8C%D9%85%D9%87-%D8%B3%D8%A7%D9%85%D8%A7%D9%86-%D9%86%D8%A7%D8%A6%DB%8C%D9%86%DB%8C-%DA%A9%D8%AF-221\",\n",
    "  \"https://jobinja.ir/companies/aras-16/jobs/Ap3o/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D8%AD%D8%B3%D8%A7%D8%A8%D8%AF%D8%A7%D8%B1-%D8%A2%D9%82%D8%A7-%D8%AF%D8%B1-%D8%A7%D8%B7%D9%84%D8%B3-%D9%BE%D8%A7%D8%B3%D8%A7%D8%B1%DA%AF%D8%A7%D8%AF-%D8%AF%D9%85%D8%A7%D9%88%D9%86%D8%AF-%D8%A7%D8%B1%D8%B3\",\n",
    "  \"https://jobinja.ir/companies/mrkasket/jobs/A4l5/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-%D8%AD%D8%B3%D8%A7%D8%A8%D8%AF%D8%A7%D8%B1%DB%8C-%D8%AE%D8%A7%D9%86%D9%85-%D8%AF%D8%B1-%D9%85%D8%B3%D8%AA%D8%B1%DA%A9%D8%A7%D8%B3%DA%A9%D8%AA\",\n",
    "  \"https://jobinja.ir/companies/lenava-engineering-group-ltd/jobs/A7kk/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D8%A2%D8%A8%D8%AF%D8%A7%D8%B1%DA%86%DB%8C-%D9%88-%D9%86%D8%B8%D8%A7%D9%81%D8%AA%DA%86%DB%8C-%D8%AE%D8%A7%D9%86%D9%85-%D8%AF%D8%B1-%DA%AF%D8%B1%D9%88%D9%87-%D9%85%D9%87%D9%86%D8%AF%D8%B3%DB%8C-%D9%84%D9%86%D8%A7%D9%88%D8%A7\",\n",
    "  \"https://jobinja.ir/companies/fibo-industrial-group/jobs/AGe1/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D9%85%DA%A9-%D8%AD%D8%B3%D8%A7%D8%A8%D8%AF%D8%A7%D8%B1-%D8%AF%D8%B1-%DA%AF%D8%B1%D9%88%D9%87-%D8%B5%D9%86%D8%B9%D8%AA%DB%8C-%D9%81%DB%8C%D8%A8%D9%88\",\n",
    "  \"https://jobinja.ir/companies/rahtab-bistoon/jobs/AZnq/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D8%A8%D8%B1%D9%86%D8%A7%D9%85%D9%87-%D9%86%D9%88%DB%8C%D8%B3-mid-level-node-js-back-end-%D8%A7%D8%B5%D9%81%D9%87%D8%A7%D9%86-%D8%AF%D8%B1-%D8%B1%D9%87%D8%AA%D8%A7%D8%A8-%D8%A8%DB%8C%D8%B3%D8%AA%D9%88%D9%86\",\n",
    "  \"https://jobinja.ir/companies/ariawave/jobs/Aawt/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D9%85%D9%88%D9%86%D8%AA%D8%A7%DA%98%DA%A9%D8%A7%D8%B1-%D8%AF%D8%B1-%D9%85%D9%88%D8%AC-%D8%A2%D8%B1%DB%8C%D8%A7\",\n",
    "  \"https://jobinja.ir/companies/tss/jobs/AvPP/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-helpdesk-%D8%A2%D9%82%D8%A7-%D8%AF%D8%B1-%D8%AA%D8%B1%D8%A7%D8%A8%D8%B1%DB%8C-%D8%B3%D8%B1%DB%8C%D8%B9-%D8%B3%D8%B9%D8%A7%D8%AF%D8%AA\",\n",
    "  \"https://jobinja.ir/companies/behtarino/jobs/AmXZ/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-senior-social-media-specialist-%D8%AF%D8%B1-%DA%AF%D8%B1%D9%88%D9%87-%D8%A7%DB%8C%D8%AF%D9%87-%DA%A9%D8%A7%D9%88%D8%A7%D9%86\",\n",
    "  \"https://jobinja.ir/companies/neutron-pharmachemical-co/jobs/AZnI/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D8%AD%D8%B3%D8%A7%D8%A8%D8%AF%D8%A7%D8%B1-%D8%A7%D8%B1%D8%B4%D8%AF-%D8%AF%D8%B1-%D8%B4%DB%8C%D9%85%DB%8C-%D8%AF%D8%A7%D8%B1%D9%88%DB%8C%DB%8C-%D9%86%D9%88%D8%AA%D8%B1%D9%88%D9%86\",\n",
    "  \"https://jobinja.ir/companies/behtarino/jobs/A7QM/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-account-manager-%D8%AF%D8%B1-%DA%AF%D8%B1%D9%88%D9%87-%D8%A7%DB%8C%D8%AF%D9%87-%DA%A9%D8%A7%D9%88%D8%A7%D9%86\",\n",
    "  \"https://jobinja.ir/companies/behtarino/jobs/A75W/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-account-manager-%D8%AF%D8%B1-%DA%AF%D8%B1%D9%88%D9%87-%D8%A7%DB%8C%D8%AF%D9%87-%DA%A9%D8%A7%D9%88%D8%A7%D9%86\",\n",
    "  \"https://jobinja.ir/companies/drmed/jobs/A4hu/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D9%85%D9%86%D8%AF-%D9%81%D8%B1%D9%88%D8%B4-%D8%AA%D9%84%D9%81%D9%86%DB%8C-%D8%AF%D8%B1-%D8%AF%DA%A9%D8%AA%D8%B1%D9%85%D8%AF\",\n",
    "  \"https://jobinja.ir/companies/vitriol/jobs/AvgO/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-%D9%81%D8%B1%D9%88%D8%B4-%D9%88-%D8%A8%D8%A7%D8%B2%D8%A7%D8%B1%DB%8C%D8%A7%D8%A8%DB%8C-%D8%AA%D9%84%D9%81%D9%86%DB%8C-%D8%AF%D8%B1-%D9%88%DB%8C%D8%AA%D8%B1%DB%8C%D9%88%D9%84\",\n",
    "  \"https://jobinja.ir/companies/lian-sika-trading-and-financial-services/jobs/A4PL/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-%D8%AE%D8%B2%D8%A7%D9%86%D9%87-%D8%AF%D8%A7%D8%B1%DB%8C-%D8%AF%D8%B1-%D8%A8%D8%A7%D8%B2%D8%B1%DA%AF%D8%A7%D9%86%DB%8C-%D9%84%DB%8C%D8%A7%D9%86-%D8%B3%DB%8C%DA%A9%D8%A7\",\n",
    "  \"https://jobinja.ir/companies/sls/jobs/A427/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-product-manager-blockchain-%D8%A2%D9%82%D8%A7-%D8%AF%D8%B1-%D8%B1%D8%A7%D9%87%DA%A9%D8%A7%D8%B1-%D8%B3%D8%B1%D8%B2%D9%85%DB%8C%D9%86-%D9%87%D9%88%D8%B4%D9%85%D9%86%D8%AF\",\n",
    "  \"https://jobinja.ir/companies/artan-tejarat-toral-sana-co/jobs/ApQn/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D8%AD%D8%B3%D8%A7%D8%A8%D8%AF%D8%A7%D8%B1-%D8%AF%D8%B1-%D8%A2%D8%B1%D8%AA%D8%A7%D9%86-%D8%AA%D8%AC%D8%A7%D8%B1%D8%AA-%D8%AA%D9%88%D8%B1%D8%A7%D9%84-%D8%B3%D9%86%D8%A7\",\n",
    "  \"https://jobinja.ir/companies/faragarsanat/jobs/AmWG/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-%D9%81%D8%B1%D9%88%D8%B4-%D8%AE%D8%A7%D9%86%D9%85-%D8%AF%D8%B1-%D9%81%D8%B1%D8%A7%DA%AF%D8%B1%D8%B5%D9%86%D8%B9%D8%AA\",\n",
    "  \"https://jobinja.ir/companies/modiage/jobs/A4rE/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-%D8%AA%D9%88%D9%84%DB%8C%D8%AF-%D9%85%D8%AD%D8%AA%D9%88%D8%A7-%D8%AF%D8%B1-%D9%85%D8%AF%DB%8C%D8%A7%DA%98\",\n",
    "  \"https://jobinja.ir/companies/araminta-psychological-clinic/jobs/A7Pm/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D9%85%D9%86%D8%AF-%D9%81%D8%B1%D9%88%D8%B4-%D8%AE%D8%A7%D9%86%D9%85-%D8%AF%D8%B1-%DA%A9%D9%84%DB%8C%D9%86%DB%8C%DA%A9-%D8%B1%D9%88%D8%A7%D9%86%D8%B4%D9%86%D8%A7%D8%B3%DB%8C-%D8%A2%D8%B1%D8%A7%D9%85%DB%8C%D9%86%D8%AA%D8%A7\",\n",
    "  \"https://jobinja.ir/companies/novinholdings/jobs/Av8s/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-%D8%AD%D8%B3%D8%A7%D8%A8%D8%AF%D8%A7%D8%B1%DB%8C-%D8%AF%D8%B1-%D9%87%D9%84%D8%AF%DB%8C%D9%86%DA%AF-%D9%86%D9%88%DB%8C%D9%86\",\n",
    "  \"https://jobinja.ir/companies/kdd-group/jobs/CQeb/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-%D9%85%D9%87%D9%86%D8%AF%D8%B3%DB%8C-%D8%B5%D9%86%D8%A7%DB%8C%D8%B9-%D8%AF%D8%B1-%DA%A9%D8%A7%D8%B1%D9%88%D9%86-%D8%AF%D8%B2-%D8%AF%D8%B4%D8%AA\",\n",
    "  \"https://jobinja.ir/companies/contempo-ghaffari-furniture-group-1/jobs/CHnG/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D9%81%D8%B1%D9%88%D8%B4%D9%86%D8%AF%D9%87-%D9%85%D8%A8%D9%84%D9%85%D8%A7%D9%86-%D8%AF%D8%B1-%DA%A9%D8%A7%D9%86%D8%AA%D9%85%D9%BE%D9%88-%DA%AF%D8%B1%D9%88%D9%87-%D9%85%D8%A8%D9%84%D9%85%D8%A7%D9%86-%D8%BA%D9%81%D8%A7%D8%B1%DB%8C\",\n",
    "  \"https://jobinja.ir/companies/rahtab-bistoon/jobs/AZyQ/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D8%A8%D8%B1%D9%86%D8%A7%D9%85%D9%87-%D9%86%D9%88%DB%8C%D8%B3-mid-level-c-sharp-back-end-%D8%A7%D8%B5%D9%81%D9%87%D8%A7%D9%86-%D8%AF%D8%B1-%D8%B1%D9%87%D8%AA%D8%A7%D8%A8-%D8%A8%DB%8C%D8%B3%D8%AA%D9%88%D9%86\",\n",
    "  \"https://jobinja.ir/companies/ava-salamat-paya/jobs/AvkX/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D9%85%D8%B3%D8%A6%D9%88%D9%84-%D9%BE%D8%B0%DB%8C%D8%B1%D8%B4-%D8%AE%D8%A7%D9%86%D9%85-%D8%AF%D8%B1-%D8%A2%D9%88%D8%A7%DB%8C-%D8%B3%D9%84%D8%A7%D9%85%D8%AA-%D9%BE%D8%A7%DB%8C%D8%A7\",\n",
    "  \"https://jobinja.ir/companies/baryar/jobs/AmsC/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-%D8%AD%D8%B3%D8%A7%D8%A8%D8%AF%D8%A7%D8%B1%DB%8C-%D8%AF%D8%B1-%D8%A8%D9%85%D8%A7-%D8%A7%DB%8C%D8%B1%D8%A7%D9%86\",\n",
    "  \"https://jobinja.ir/companies/khoday-khoob/jobs/A4qD/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D9%81%D8%B1%D9%88%D8%B4%D9%86%D8%AF%D9%87-%D8%AA%D9%84%D9%81%D9%86%DB%8C-%D8%AE%D8%A7%D9%86%D9%85-%DA%A9%D8%B1%D8%AC-%D8%AF%D8%B1-%D9%85%D8%AC%D9%85%D9%88%D8%B9%D9%87-%D8%AE%D8%AF%D8%A7%DB%8C-%D8%AE%D9%88%D8%A8-%D9%85%D9%86\",\n",
    "  \"https://jobinja.ir/companies/iran-web-life/jobs/Av6X/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-%D8%B3%D8%A6%D9%88-seo-%D8%AF%D8%B1-%D8%A7%DB%8C%D8%B1%D8%A7%D9%86-%D9%88%D8%A8-%D9%84%D8%A7%DB%8C%D9%81\",\n",
    "  \"https://jobinja.ir/companies/iran-web-life/jobs/A4zE/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%A2%D9%85%D9%88%D8%B2-%D8%B3%D8%A6%D9%88-%D8%AF%D8%B1-%D8%A7%DB%8C%D8%B1%D8%A7%D9%86-%D9%88%D8%A8-%D9%84%D8%A7%DB%8C%D9%81\",\n",
    "  \"https://jobinja.ir/companies/the-way-to-light-institute/jobs/A45C/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-%D8%B4%D8%A8%DA%A9%D9%87-%D9%87%D8%A7%DB%8C-%D8%A7%D8%AC%D8%AA%D9%85%D8%A7%D8%B9%DB%8C-%D8%AE%D8%A7%D9%86%D9%85-%D8%AF%D8%B1-%D9%85%D9%88%D8%B3%D8%B3%D9%87-%D8%B1%D8%A7%D9%87%DB%8C-%D8%A8%D9%87-%D8%B3%D9%88%DB%8C-%D9%86%D9%88%D8%B1\",\n",
    "  \"https://jobinja.ir/companies/afra-10/jobs/AUp0/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-%D8%AA%D9%88%D8%B3%D8%B9%D9%87-%D8%A7%D8%B1%D8%AA%D8%A8%D8%A7%D8%B7%D8%A7%D8%AA-%D9%88-%D8%A8%D8%A7%D9%86%DA%A9-%D8%A7%D8%B7%D9%84%D8%A7%D8%B9%D8%A7%D8%AA-%D8%AF%D8%B1-%D8%AD%D8%A7%D9%85%DB%8C%D8%A7%D9%86-%D9%86%D9%88%D8%A7%D9%86%D8%AF%DB%8C%D8%B4-%D8%A7%D9%81%D8%B1%D8%A7\",\n",
    "  \"https://jobinja.ir/companies/ticktackgallery/jobs/AplT/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D9%81%D8%B1%D9%88%D8%B4%D9%86%D8%AF%D9%87-%D8%AF%D8%B1-%D8%AA%DB%8C%DA%A9-%D8%AA%D8%A7%DA%A9-%DA%AF%D8%A7%D9%84%D8%B1%DB%8C\",\n",
    "  \"https://jobinja.ir/companies/autokhatib/jobs/A7NH/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D8%B3%D8%B1%D9%BE%D8%B1%D8%B3%D8%AA-%D8%AD%D8%B3%D8%A7%D8%A8%D8%AF%D8%A7%D8%B1%DB%8C-%D8%AF%D8%B1-%D8%A7%D8%AA%D9%88-%D8%AE%D8%B7%DB%8C%D8%A8\",\n",
    "  \"https://jobinja.ir/companies/shabahang-misaghe-jonob/jobs/A4XX/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D9%85%D8%AF%DB%8C%D8%B1-%D9%85%D8%A7%D8%B1%DA%A9%D8%AA%DB%8C%D9%86%DA%AF-%D8%AE%D8%A7%D9%86%D9%85-%D8%AF%D8%B1-%D8%B4%D8%A8%D8%A7%D9%87%D9%86%DA%AF-%D9%85%DB%8C%D8%AB%D8%A7%D9%82-%D8%AC%D9%86%D9%88%D8%A8\",\n",
    "  \"https://jobinja.ir/companies/rayan-pardaz-kavosh/jobs/A4Ic/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-%D8%A7%D8%B1%D8%B4%D8%AF-%D8%AA%D8%AD%D9%84%DB%8C%D9%84-%D9%88-%D8%B7%D8%B1%D8%A7%D8%AD%DB%8C-%D9%86%D8%B1%D9%85-%D8%A7%D9%81%D8%B2%D8%A7%D8%B1-%D8%AF%D8%B1-%D8%B1%D8%A7%DB%8C%D8%A7%D9%86-%D9%BE%D8%B1%D8%AF%D8%A7%D8%B2-%DA%A9%D8%A7%D9%88%D8%B4\",\n",
    "  \"https://jobinja.ir/companies/turquoise-digital/jobs/AHjr/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-senior-python-developer-%D8%AF%D8%B1-%D9%81%DB%8C%D8%B1%D9%88%D8%B2%D9%87-%D8%AF%DB%8C%D8%AC%DB%8C%D8%AA%D8%A7%D9%84\",\n",
    "  \"https://jobinja.ir/companies/rassam-gostar-ravis/jobs/AviS/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-%D9%81%D8%B1%D9%88%D8%B4-%D9%88-%D8%A8%D8%A7%D8%B2%D8%A7%D8%B1%DB%8C%D8%A7%D8%A8%DB%8C-%D8%AF%D8%B1-%D8%B1%D8%B3%D8%A7%D9%85-%DA%AF%D8%B3%D8%AA%D8%B1-%D8%B1%D8%A7%D9%88%DB%8C%D8%B3\",\n",
    "  \"https://jobinja.ir/companies/vesta-software/jobs/C8np/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-%D9%BE%D8%B4%D8%AA%DB%8C%D8%A8%D8%A7%D9%86%DB%8C-%D9%88-%D8%A7%D8%B3%D8%AA%D9%82%D8%B1%D8%A7%D8%B1-%D9%86%D8%B1%D9%85-%D8%A7%D9%81%D8%B2%D8%A7%D8%B1-%D8%A7%D9%85%D8%B1%DB%8C%D9%87-%D8%B3%D8%B1%D8%A8%D8%A7%D8%B2%DB%8C-%D8%A2%D9%82%D8%A7-%D8%AF%D8%B1-%D9%81%D9%86%D8%A7%D9%88%D8%B1%D8%A7%D9%86-%D8%A7%D8%B7%D9%84%D8%A7%D8%B9%D8%A7%D8%AA-%D9%88%D8%B3%D8%AA%D8%A7\",\n",
    "  \"https://jobinja.ir/companies/eye-back-teb-pars/jobs/AHVW/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D8%AA%DA%A9%D9%86%D8%B3%DB%8C%D9%86-%D9%84%D8%A7%D8%A8%D8%B1%D8%A7%D8%AA%D9%88%D8%A7%D8%B1-%D8%AA%D8%B1%D8%A7%D8%B4-%D8%B9%D8%AF%D8%B3%DB%8C-%D8%B9%DB%8C%D9%86%DA%A9-%D8%AF%D8%B1-%D8%A2%DB%8C%D8%A8%DA%A9-%D8%B7%D8%A8-%D9%BE%D8%A7%D8%B1%D8%B3\",\n",
    "  \"https://jobinja.ir/companies/hasin/jobs/AanF/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-%D9%85%D8%B1%DA%A9%D8%B2-%D8%AA%D9%85%D8%A7%D8%B3-%D8%B1%D8%B3%DB%8C%D8%AF%DA%AF%DB%8C-%D8%A8%D9%87-%D8%B4%DA%A9%D8%A7%DB%8C%D8%A7%D8%AA-%D8%AF%D8%B1-%DA%AF%D8%B1%D9%88%D9%87-%D8%AD%D8%B5%DB%8C%D9%86\",\n",
    "  \"https://jobinja.ir/companies/worksite/jobs/AvTJ/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D9%83%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-%D9%81%D8%B1%D9%88%D8%B4-%D8%AF%D8%B1-%D8%A7%D8%A8%D8%B2%D8%A7%D8%B1%D8%A2%D9%84%D8%A7%D8%AA-%D9%81%DB%8C%D8%AF%D9%88%D8%B1%D8%A7\",\n",
    "  \"https://jobinja.ir/companies/karan-4/jobs/A7S9/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-%D8%AD%D8%B3%D8%A7%D8%A8%D8%AF%D8%A7%D8%B1%DB%8C-%D8%AF%D8%B1-%DA%A9%D8%B1%D8%A7%D9%86\",\n",
    "  \"https://jobinja.ir/companies/danapardaz/jobs/AmKa/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D8%A8%D8%B1%D9%86%D8%A7%D9%85%D9%87-%D9%86%D9%88%DB%8C%D8%B3-%D8%A7%D8%B1%D8%B4%D8%AF-net-core-%D8%AF%D8%B1-%D8%AF%D8%A7%D9%86%D8%A7-%D9%BE%D8%B1%D8%AF%D8%A7%D8%B2\",\n",
    "  \"https://jobinja.ir/companies/inknowtex-iran/jobs/CFUs/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-%D9%BE%D8%B4%D8%AA%DB%8C%D8%A8%D8%A7%D9%86%DB%8C-%D9%88-%D8%A7%D8%B3%D8%AA%D9%82%D8%B1%D8%A7%D8%B1-%D9%86%D8%B1%D9%85-%D8%A7%D9%81%D8%B2%D8%A7%D8%B1-%D8%AF%D8%B1-%D8%A7%DB%8C%D9%86%D9%88%D8%AA%DA%A9%D8%B3-%D8%A7%DB%8C%D8%B1%D8%A7%D9%86\",\n",
    "  \"https://jobinja.ir/companies/arad-74/jobs/Avy1/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D8%B7%D8%B1%D8%A7%D8%AD-%D9%85%D8%B9%D9%85%D8%A7%D8%B1%DB%8C-%D8%AF%D8%B1-%D8%A2%D8%B1%D8%A7%D8%AF-%D8%A7%D8%A8%D9%86%DB%8C%D9%87-%D8%A7%D8%B7%D9%85%DB%8C%D9%86%D8%A7%D9%86\",\n",
    "  \"https://jobinja.ir/companies/samin-khodro-pouya/jobs/A4Fg/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3-%D9%81%D8%B1%D9%88%D8%B4-%D8%AF%D8%B1-%D8%AB%D9%85%DB%8C%D9%86-%D8%AE%D9%88%D8%AF%D8%B1%D9%88-%D9%BE%D9%88%DB%8C%D8%A7\",\n",
    "  \"https://jobinja.ir/companies/bamboo-6/jobs/AvWG/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-back-end-developer-node-js-%D8%AF%D9%88%D8%B1%DA%A9%D8%A7%D8%B1%DB%8C-%D8%AF%D8%B1-%D8%A8%D8%A7%D9%85%D8%A8%D9%88\"\n",
    "]\n",
    "        \n",
    "        \n",
    "        \n",
    "    # Initialize scraper with custom save directory\n",
    "    scraper = JobinjaScraper(\n",
    "        save_dir=save_dir,\n",
    "        delay=1.0,  # 1 second delay between requests\n",
    "        max_workers=5  # 5 concurrent threads\n",
    "    )\n",
    "    \n",
    "    # Scrape all jobs\n",
    "    results = scraper.scrape_jobs(urls, batch_size=100)\n",
    "    \n",
    "    # Save final combined results\n",
    "    scraper.save_to_json(results, Path(save_dir) / \"all_jobs.json\")\n",
    "    scraper.save_to_csv(results, Path(save_dir) / \"all_jobs.csv\")\n",
    "    \n",
    "    print(f\"\\nScraping completed:\")\n",
    "    print(f\"Total jobs scraped: {len(results)}\")\n",
    "    print(f\"Results saved in: {save_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8092d9-2a5d-45de-94d7-84bcb2cc9689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8fa792-2cdb-4197-8e85-3ce805720272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87376cb2-7b71-4b1f-8c64-8ea28dcb1ec0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Define the target URL\n",
    "url = \"https://jobinja.ir/\"\n",
    "\n",
    "# Send a GET request to the website\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "}\n",
    "try:\n",
    "    response = requests.get(url, headers=headers, verify=False, timeout=10)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract all the links to tabs or subpages\n",
    "    links = soup.find_all('a', href=True)\n",
    "    \n",
    "    # Collect unique links to avoid repetition\n",
    "    base_url = \"https://jobinja.ir\"\n",
    "    subpage_links = set()\n",
    "\n",
    "    for link in links:\n",
    "        href = link['href']\n",
    "        if href.startswith('/'):  # It's a relative link\n",
    "            full_url = base_url + href\n",
    "        elif href.startswith('http'):\n",
    "            full_url = href\n",
    "        else:\n",
    "            continue  # Skip if the href doesn't look like a valid link\n",
    "\n",
    "        subpage_links.add(full_url)\n",
    "\n",
    "    # Extract text from each subpage\n",
    "    for subpage_url in subpage_links:\n",
    "        try:\n",
    "            subpage_response = requests.get(subpage_url, headers=headers, verify=False, timeout=10)\n",
    "            subpage_response.raise_for_status()\n",
    "\n",
    "            # Parse the HTML content of the subpage\n",
    "            subpage_soup = BeautifulSoup(subpage_response.text, 'html.parser')\n",
    "            \n",
    "            # Extract all text from the subpage\n",
    "            subpage_text = subpage_soup.get_text(strip=True)\n",
    "            print(subpage_text[1:])\n",
    "            # Print a snippet of the extracted text from each subpage\n",
    "            print(f\"URL: {subpage_url}\\nContent Snippet: {subpage_text[:1000]}\\n\")\n",
    "\n",
    "            # To avoid overwhelming the server, add a short delay\n",
    "            time.sleep(1)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"An error occurred while accessing {subpage_url}: {e}\")\n",
    "\n",
    "except requests.exceptions.SSLError as e:\n",
    "    print(f\"SSL error occurred: {e}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11e7294-d5f6-499d-a6cb-4a30b4267862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://jobinja.ir/jobs?filters%5Bkeywords%5D%5B0%5D=%DA%AF%D8%B1%D8%A7%D9%81%DB%8C%D8%B3%D8%AA\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bjob_categories%5D%5B%5D=%D9%85%D9%87%D9%86%D8%AF%D8%B3%DB%8C+%D9%86%D8%B3%D8%A7%D8%AC%DB%8C%D8%8C+%D8%B7%D8%B1%D8%A7%D8%AD%DB%8C+%D9%BE%D8%A7%D8%B1%DA%86%D9%87+%D9%88+%D9%84%D8%A8%D8%A7%D8%B3\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Blocations%5D%5B%5D=%D9%82%D8%B2%D9%88%DB%8C%D9%86\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bkeywords%5D%5B0%5D=%D8%A7%D8%AF%D8%A7%D8%B1%DB%8C\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bjob_categories%5D%5B%5D=%D8%AA%D8%B1%D8%A8%DB%8C%D8%AA+%D8%A8%D8%AF%D9%86%DB%8C\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?sort_by=published_at_desc\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import urllib3\n",
    "import json\n",
    "import scipy.io as sio\n",
    "import os\n",
    "\n",
    "# Suppress InsecureRequestWarning\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Define the target URL\n",
    "url = \"https://jobinja.ir/\"\n",
    "\n",
    "# Send a GET request to the website\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "}\n",
    "\n",
    "# Directory to save text files\n",
    "save_dir = \"JobInja\"\n",
    "os.makedirs(save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# Initialize list to store all job data\n",
    "all_jobs_data = []\n",
    "\n",
    "try:\n",
    "    # Send a GET request to the main page\n",
    "    response = requests.get(url, headers=headers, verify=False, timeout=10)\n",
    "    response.raise_for_status()  # Raise an error if the request failed\n",
    "    \n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract all the links to tabs or subpages\n",
    "    links = soup.find_all('a', href=True)\n",
    "    \n",
    "    # Collect unique links to avoid repetition\n",
    "    base_url = \"https://jobinja.ir\"\n",
    "    subpage_links = set()  # Use a set to store unique links\n",
    "\n",
    "    # Iterate through all the links found on the main page\n",
    "    for link in links:\n",
    "        href = link['href']\n",
    "        if href.startswith('/'):  # It's a relative link, prepend base URL\n",
    "            full_url = base_url + href\n",
    "        elif href.startswith('http'):  # It's an absolute link\n",
    "            full_url = href\n",
    "        else:\n",
    "            continue  # Skip if the href doesn't look like a valid link\n",
    "\n",
    "        subpage_links.add(full_url)  # Add the link to the set of subpage links\n",
    "\n",
    "    def clean_persian_text(text):\n",
    "        # Remove extra spaces, non-Persian characters, and control characters\n",
    "        cleaned_text = re.sub(r'[^\\u0600-\\u06FF\\s]', ' ', text)  # Keep only Persian characters and spaces\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Replace multiple spaces with a single space\n",
    "        return cleaned_text.strip()  # Strip leading and trailing spaces\n",
    "\n",
    "    def extract_job_features(subpage_soup):\n",
    "        # This function will extract and clean text related to job name, description, and other features\n",
    "        job_features = {}\n",
    "\n",
    "        # Extract job title (assuming job title is in an <h1> tag)\n",
    "        job_title_tag = subpage_soup.find('h1')\n",
    "        if job_title_tag:\n",
    "            job_features['job_title'] = clean_persian_text(job_title_tag.get_text())\n",
    "\n",
    "        # Extract job description (assuming job description is in a specific <div> class)\n",
    "        job_description_tag = subpage_soup.find('div', class_='job-description')  # Replace with the actual class name\n",
    "        if job_description_tag:\n",
    "            job_features['job_description'] = clean_persian_text(job_description_tag.get_text())\n",
    "\n",
    "        # Extract company name (assuming company name is in a specific <div> class)\n",
    "        company_name_tag = subpage_soup.find('div', class_='company-name')  # Replace with the actual class name\n",
    "        if company_name_tag:\n",
    "            job_features['company_name'] = clean_persian_text(company_name_tag.get_text())\n",
    "\n",
    "        return job_features\n",
    "\n",
    "    # Iterate through each subpage link to extract job information\n",
    "    for subpage_url in subpage_links:\n",
    "        try:\n",
    "            # Send a GET request to each subpage\n",
    "            subpage_response = requests.get(subpage_url, headers=headers, verify=False, timeout=10)\n",
    "            subpage_response.raise_for_status()  # Raise an error if the request failed\n",
    "\n",
    "            # Parse the HTML content of the subpage\n",
    "            subpage_soup = BeautifulSoup(subpage_response.text, 'html.parser')\n",
    "            \n",
    "            # Extract job features from the subpage\n",
    "            job_data = extract_job_features(subpage_soup)\n",
    "\n",
    "            # Save the extracted job features if available\n",
    "            if job_data:\n",
    "                all_jobs_data.append(job_data)\n",
    "\n",
    "                # Save Persian text to a .txt file\n",
    "                job_title = job_data.get('job_title', 'JobInja')  # Use 'JobInja' if job title is not available\n",
    "                file_path = os.path.join(save_dir, f\"{job_title}.txt\")\n",
    "                with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                    for key, value in job_data.items():\n",
    "                        file.write(f\"{key.capitalize()}: {value}\\n\")\n",
    "\n",
    "                # Print the extracted job features to the console\n",
    "                print(f\"URL: {subpage_url}\")\n",
    "                for key, value in job_data.items():\n",
    "                    print(f\"{key.capitalize()}: {value}\")\n",
    "                print(\"\\n\")\n",
    "\n",
    "            # To avoid overwhelming the server, add a short delay between requests\n",
    "            time.sleep(1)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # Print an error message if a request fails\n",
    "            print(f\"An error occurred while accessing {subpage_url}: {e}\")\n",
    "\n",
    "    # Save the dataset to .mat and .json files\n",
    "    mat_file_path = os.path.join(save_dir, \"jobinja_data.mat\")\n",
    "    json_file_path = os.path.join(save_dir, \"jobinja_data.json\")\n",
    "\n",
    "    # Convert job data to a format suitable for saving\n",
    "    job_data_dict = {f\"job_{i}\": job for i, job in enumerate(all_jobs_data)}\n",
    "    \n",
    "    # Save as .mat file using SciPy's savemat function\n",
    "    sio.savemat(mat_file_path, job_data_dict)\n",
    "\n",
    "    # Save as .json file using the json module\n",
    "    with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(all_jobs_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    # Print an error message if the initial request fails\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c5b584-44a4-4b18-8ebe-8a0cb5376e97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://jobinja.ir/jobs?filters%5Blocations%5D%5B%5D=%D8%A8%D9%88%D8%B4%D9%87%D8%B1\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bkeywords%5D%5B0%5D=%D8%B7%D8%B1%D8%A7%D8%AD+%DA%AF%D8%B1%D8%A7%D9%81%DB%8C%DA%A9\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bkeywords%5D%5B0%5D=%DA%A9%D8%A7%D8%B1%D8%B4%D9%86%D8%A7%D8%B3+%D9%81%D8%B1%D9%88%D8%B4\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/%D8%B3%D9%88%D8%A7%D9%84%D8%A7%D8%AA-%D9%85%D8%AA%D8%AF%D8%A7%D9%88%D9%84-%D8%AF%D8%B1%D8%AC-%D8%A2%DA%AF%D9%87%DB%8C-%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85\n",
      "Job_title: سوالات متداول کارفرمایان\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bjob_categories%5D%5B%5D=%D9%85%D8%AF%DB%8C%D8%B1%DB%8C%D8%AA+%D8%A8%DB%8C%D9%85%D9%87\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Blocations%5D%5B%5D=%D8%B3%DB%8C%D8%B3%D8%AA%D8%A7%D9%86+%D9%88+%D8%A8%D9%84%D9%88%DA%86%D8%B3%D8%AA%D8%A7%D9%86\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "An error occurred while accessing https://jobinja.ir/telegram: HTTPSConnectionPool(host='t.me', port=443): Max retries exceeded with url: /jobinja_jobs (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001876888E0D0>, 'Connection to t.me timed out. (connect timeout=10)'))\n",
      "URL: https://jobinja.ir/jobs?filters%5Bjob_categories%5D%5B%5D=%D8%AD%D9%85%D9%84+%D9%88+%D9%86%D9%82%D9%84\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/companies/hamisystem/jobs\n",
      "Job_title: فرصت های شغلی فعال در حامی سیستم شریف\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bkeywords%5D%5B0%5D=%D8%A8%D8%B1%D9%86%D8%A7%D9%85%D9%87+%D9%86%D9%88%DB%8C%D8%B3%DB%8C\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bkeywords%5D%5B0%5D=%DA%AF%D8%B1%D8%A7%D9%81%DB%8C%D8%B3%D8%AA\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/companies/kianam-ertebat-1/jobs\n",
      "Job_title: فرصت های شغلی فعال در کیانام ارتباط\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/companies/autokhatib/jobs\n",
      "Job_title: فرصت های شغلی فعال در اتو خطیب\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/knowledge/job-seeker-faq/pages/job-seeker-tos\n",
      "Job_title: سوالات متداول کارجویان\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/how-to\n",
      "Job_title: قدم بعدی شما در استخدام چیست؟\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Blocations%5D%5B%5D=%DA%86%D9%87%D8%A7%D8%B1%D9%85%D8%AD%D8%A7%D9%84+%D8%A8%D8%AE%D8%AA%DB%8C%D8%A7%D8%B1%DB%8C\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bkeywords%5D%5B0%5D=%D9%87%D9%88%D8%B4+%D9%85%D8%B5%D9%86%D9%88%D8%B9%DB%8C\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/companies/hastibazaar/jobs\n",
      "Job_title: فرصت های شغلی فعال در تپسی شاپ\n",
      "\n",
      "\n",
      "An error occurred while accessing https://twitter.com/Jobinja_ir: HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /Jobinja_ir (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000018766F30410>, 'Connection to twitter.com timed out. (connect timeout=10)'))\n",
      "URL: https://jobinja.ir/jobs?filters%5Bkeywords%5D%5B0%5D=React\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bjob_categories%5D%5B%5D=%DA%A9%D8%A7%D8%B1%DA%AF%D8%B1+%D8%B3%D8%A7%D8%AF%D9%87%D8%8C+%D9%86%DB%8C%D8%B1%D9%88%DB%8C+%D8%AE%D8%AF%D9%85%D8%A7%D8%AA%DB%8C\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/companies/asan-tahato-riranian/jobs/A4Oa/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D9%85%D8%AF%DB%8C%D8%B1-%D9%81%D8%B1%D9%88%D8%B4-%D8%AF%D8%B1-%D8%A7%D8%B3%D8%A7%D9%86-%D8%AA%D9%87%D8%A7%D8%AA%D8%B1-%D8%A7%DB%8C%D8%B1%D8%A7%D9%86%DB%8C%D8%A7%D9%86?_ref=18\n",
      "Job_title: استخدام مدیر فروش\n",
      "\n",
      "\n",
      "An error occurred while accessing https://blog.jobinja.ir: HTTPSConnectionPool(host='blog.jobinja.ir', port=443): Read timed out. (read timeout=10)\n",
      "URL: https://jobinja.ir/companies/dr-serita/jobs\n",
      "Job_title: فرصت های شغلی فعال در کلینیک سریتا\n",
      "\n",
      "\n",
      "An error occurred while accessing https://blog.jobinja.ir/%d8%a2%d9%85%d9%88%d8%b2%d8%b4-%d8%aa%da%a9%d9%86%db%8c%da%a9%d9%87%d8%a7%db%8c-%d9%85%d8%b5%d8%a7%d8%ad%d8%a8%d9%87-%d8%b4%d8%ba%d9%84%db%8c/: HTTPSConnectionPool(host='blog.jobinja.ir', port=443): Read timed out. (read timeout=10)\n",
      "URL: https://jobinja.ir/jobs?filters%5Bjob_categories%5D%5B%5D=%D9%85%D8%A7%D9%84%DB%8C+%D9%88+%D8%AD%D8%B3%D8%A7%D8%A8%D8%AF%D8%A7%D8%B1%DB%8C\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bkeywords%5D%5B0%5D=%D8%A7%D8%AF%D9%85%DB%8C%D9%86+%D8%A7%DB%8C%D9%86%D8%B3%D8%AA%D8%A7%DA%AF%D8%B1%D8%A7%D9%85\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Blocations%5D%5B%5D=%D8%A7%D8%B1%D8%AF%D8%A8%DB%8C%D9%84\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Blocations%5D%5B%5D=%D8%B2%D9%86%D8%AC%D8%A7%D9%86\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bkeywords%5D%5B0%5D=%D9%85%D8%B4%D8%A7%D9%88%D8%B1+%D8%AA%D8%AD%D8%B5%DB%8C%D9%84%DB%8C\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Blocations%5D%5B%5D=%DA%A9%D8%B1%D9%85%D8%A7%D9%86%D8%B4%D8%A7%D9%87\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bkeywords%5D%5B0%5D=%D8%A8%D8%B1%D9%86%D8%A7%D9%85%D9%87+%D9%86%D9%88%DB%8C%D8%B3\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bkeywords%5D%5B0%5D=%D8%A7%D8%AF%D9%85%DB%8C%D9%86\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bkeywords%5D%5B0%5D=%D8%B7%D8%B1%D8%A7%D8%AD+%D8%B3%D8%A7%DB%8C%D8%AA\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bkeywords%5D%5B0%5D=%D8%AF%DB%8C%D8%AC%DB%8C+%DA%A9%D8%A7%D9%84%D8%A7\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bjob_categories%5D%5B%5D=%D9%85%D9%87%D9%86%D8%AF%D8%B3%DB%8C+%D8%B9%D9%85%D8%B1%D8%A7%D9%86+%D9%88+%D9%85%D8%B9%D9%85%D8%A7%D8%B1%DB%8C\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/companies/bertina/jobs\n",
      "Job_title: فرصت های شغلی فعال در برتینا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bkeywords%5D%5B0%5D=%D8%A7%D8%AF%DB%8C%D8%AA%D9%88%D8%B1\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bjob_categories%5D%5B%5D=%D9%BE%D8%B4%D8%AA%DB%8C%D8%A8%D8%A7%D9%86%DB%8C+%D9%88+%D8%A7%D9%85%D9%88%D8%B1+%D9%85%D8%B4%D8%AA%D8%B1%DB%8C%D8%A7%D9%86\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/companies/signal/jobs\n",
      "Job_title: فرصت های شغلی فعال در سیگنال\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Blocations%5D%5B%5D=%D8%B3%D9%85%D9%86%D8%A7%D9%86\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bkeywords%5D%5B0%5D=%DA%A9%D8%A7%D8%B1%D9%85%D9%86%D8%AF+%D8%A7%D8%AF%D8%A7%D8%B1%DB%8C\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Blocations%5D%5B%5D=%D8%A7%D9%84%D8%A8%D8%B1%D8%B2\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://www.linkedin.com/company/10081041\n",
      "Job_title: \n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/companies/rayan-pardaz-kavosh/jobs\n",
      "Job_title: فرصت های شغلی فعال در رایان پرداز کاوش\n",
      "\n",
      "\n",
      "An error occurred while accessing https://blog.jobinja.ir/%d8%a8%d9%87%d8%aa%d8%b1%db%8c%d9%86-%d8%b1%d9%88%d8%b4%d9%87%d8%a7%db%8c-%d8%ac%d8%b0%d8%a8-%d9%88-%d8%a7%d8%b3%d8%aa%d8%ae%d8%af%d8%a7%d9%85/: HTTPSConnectionPool(host='blog.jobinja.ir', port=443): Read timed out. (read timeout=10)\n",
      "URL: https://jobinja.ir/jobs?filters%5Bkeywords%5D%5B0%5D=%D9%88%D8%B1%D8%AF%D9%BE%D8%B1%D8%B3\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bjob_categories%5D%5B%5D=%D9%88%D8%A8%D8%8C%E2%80%8C+%D8%A8%D8%B1%D9%86%D8%A7%D9%85%D9%87%E2%80%8C%D9%86%D9%88%DB%8C%D8%B3%DB%8C+%D9%88+%D9%86%D8%B1%D9%85%E2%80%8C%D8%A7%D9%81%D8%B2%D8%A7%D8%B1\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bjob_categories%5D%5B%5D=%D9%85%D9%87%D9%86%D8%AF%D8%B3%DB%8C+%D9%85%D8%B9%D8%AF%D9%86+%D9%88+%D9%85%D8%AA%D8%A7%D9%84%D9%88%D8%B1%DA%98%DB%8C\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bjob_categories%5D%5B%5D=%D9%85%D9%87%D9%86%D8%AF%D8%B3%DB%8C+%D9%BE%D9%84%DB%8C%D9%85%D8%B1\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?sort_by=published_at_desc\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/join/user\n",
      "Job_title: ایجاد حساب کاربری\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bkeywords%5D%5B0%5D=%D8%B7%D8%B1%D8%A7%D8%AD%DB%8C+%D8%B3%D8%A7%DB%8C%D8%AA\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/companies\n",
      "Job_title: شرکت ها\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bkeywords%5D%5B0%5D=%D9%85%D9%86%D8%B4%DB%8C\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bjob_categories%5D%5B%5D=%D8%B5%D9%86%D8%A7%DB%8C%D8%B9+%D8%BA%D8%B0%D8%A7%DB%8C%DB%8C\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bkeywords%5D%5B0%5D=python\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bkeywords%5D%5B0%5D=php\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Blocations%5D%5B%5D=%D8%AE%D8%B1%D8%A7%D8%B3%D8%A7%D9%86+%D8%B1%D8%B6%D9%88%DB%8C\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/companies/namayandegi-modiran-khodro-sotoodeh401/jobs/CwGh/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D8%A2%D8%A8%D8%AF%D8%A7%D8%B1%DA%86%DB%8C-%D9%86%D8%B8%D8%A7%D9%81%D8%AA-%D8%AF%D8%B1-%DA%AF%D8%B1%D9%88%D9%87-%D8%AE%D9%88%D8%AF%D8%B1%D9%88%DB%8C%DB%8C-%D8%B3%D8%AA%D9%88%D8%AF%D9%87?_ref=18\n",
      "Job_title: استخدام آبدارچی نظافت\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bjob_categories%5D%5B%5D=%D8%AA%D8%AD%D9%82%DB%8C%D9%82+%D9%88+%D8%AA%D9%88%D8%B3%D8%B9%D9%87\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/browsejobs\n",
      "Job_title: لیست مشاغل\n",
      "\n",
      "\n",
      "URL: https://jobinja.ir/jobs?filters%5Bjob_categories%5D%5B%5D=%D9%81%D8%B1%D9%88%D8%B4+%D9%88+%D8%A8%D8%A7%D8%B2%D8%A7%D8%B1%DB%8C%D8%A7%D8%A8%DB%8C\n",
      "Job_title: بهترین فرصت های شغلی در جابینجا\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import urllib3\n",
    "import json\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "class Jobinja:\n",
    "    def __init__(self, base_url=\"https://jobinja.ir/\", save_dir=\"JobInja\"):\n",
    "        self.base_url = base_url\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "        }\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(self.save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "        self.all_jobs_data = []\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)  # Suppress InsecureRequestWarning\n",
    "\n",
    "    def get_links(self):\n",
    "        try:\n",
    "            # Send a GET request to the main page\n",
    "            response = requests.get(self.base_url, headers=self.headers, verify=False, timeout=10)\n",
    "            response.raise_for_status()  # Raise an error if the request failed\n",
    "            \n",
    "            # Parse the HTML content using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Extract all the links to tabs or subpages\n",
    "            links = soup.find_all('a', href=True)\n",
    "            \n",
    "            # Collect unique links to avoid repetition\n",
    "            subpage_links = set()  # Use a set to store unique links\n",
    "\n",
    "            # Iterate through all the links found on the main page\n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                if href.startswith('/'):  # It's a relative link, prepend base URL\n",
    "                    full_url = self.base_url + href\n",
    "                elif href.startswith('http'):  # It's an absolute link\n",
    "                    full_url = href\n",
    "                else:\n",
    "                    continue  # Skip if the href doesn't look like a valid link\n",
    "\n",
    "                subpage_links.add(full_url)  # Add the link to the set of subpage links\n",
    "\n",
    "            return subpage_links\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"An error occurred while getting links: {e}\")\n",
    "            return set()\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_persian_text(text):\n",
    "        # Remove extra spaces, non-Persian characters, and control characters\n",
    "        cleaned_text = re.sub(r'[^\\u0600-\\u06FF\\s]', ' ', text)  # Keep only Persian characters and spaces\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Replace multiple spaces with a single space\n",
    "        return cleaned_text.strip()  # Strip leading and trailing spaces\n",
    "\n",
    "    def extract_job_features(self, subpage_soup):\n",
    "        # This function will extract and clean text related to job name, description, and other features\n",
    "        job_features = {}\n",
    "\n",
    "        # Extract job title (assuming job title is in an <h1> tag)\n",
    "        job_title_tag = subpage_soup.find('h1')\n",
    "        if job_title_tag:\n",
    "            job_features['job_title'] = self.clean_persian_text(job_title_tag.get_text())\n",
    "\n",
    "        # Extract job description (assuming job description is in a specific <div> class)\n",
    "        job_description_tag = subpage_soup.find('div', class_='job-description')  # Replace with the actual class name\n",
    "        if job_description_tag:\n",
    "            job_features['job_description'] = self.clean_persian_text(job_description_tag.get_text())\n",
    "\n",
    "        # Extract company name (assuming company name is in a specific <div> class)\n",
    "        company_name_tag = subpage_soup.find('div', class_='company-name')  # Replace with the actual class name\n",
    "        if company_name_tag:\n",
    "            job_features['company_name'] = self.clean_persian_text(company_name_tag.get_text())\n",
    "\n",
    "        return job_features\n",
    "\n",
    "    def scrape_jobs(self):\n",
    "        subpage_links = self.get_links()\n",
    "\n",
    "        # Iterate through each subpage link to extract job information\n",
    "        for subpage_url in subpage_links:\n",
    "            try:\n",
    "                # Send a GET request to each subpage\n",
    "                subpage_response = requests.get(subpage_url, headers=self.headers, verify=False, timeout=10)\n",
    "                subpage_response.raise_for_status()  # Raise an error if the request failed\n",
    "\n",
    "                # Parse the HTML content of the subpage\n",
    "                subpage_soup = BeautifulSoup(subpage_response.text, 'html.parser')\n",
    "                \n",
    "                # Extract job features from the subpage\n",
    "                job_data = self.extract_job_features(subpage_soup)\n",
    "\n",
    "                # Save the extracted job features if available\n",
    "                if job_data:\n",
    "                    self.all_jobs_data.append(job_data)\n",
    "\n",
    "                    # Save Persian text to a .txt file\n",
    "                    job_title = job_data.get('job_title', 'JobInja')  # Use 'JobInja' if job title is not available\n",
    "                    file_path = os.path.join(self.save_dir, f\"{job_title}.txt\")\n",
    "                    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                        for key, value in job_data.items():\n",
    "                            file.write(f\"{key.capitalize()}: {value}\\n\")\n",
    "\n",
    "                    # Print the extracted job features to the console\n",
    "                    print(f\"URL: {subpage_url}\")\n",
    "                    for key, value in job_data.items():\n",
    "                        print(f\"{key.capitalize()}: {value}\")\n",
    "                    print(\"\\n\")\n",
    "\n",
    "                # To avoid overwhelming the server, add a short delay between requests\n",
    "                time.sleep(1)\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                # Print an error message if a request fails\n",
    "                print(f\"An error occurred while accessing {subpage_url}: {e}\")\n",
    "\n",
    "    def save_dataset(self):\n",
    "        # Save the dataset to .mat and .json files\n",
    "        mat_file_path = os.path.join(self.save_dir, \"jobinja_data.mat\")\n",
    "        json_file_path = os.path.join(self.save_dir, \"jobinja_data.json\")\n",
    "\n",
    "        # Convert job data to a format suitable for saving\n",
    "        job_data_dict = {f\"job_{i}\": job for i, job in enumerate(self.all_jobs_data)}\n",
    "        \n",
    "        # Save as .mat file using SciPy's savemat function\n",
    "        sio.savemat(mat_file_path, job_data_dict)\n",
    "\n",
    "        # Save as .json file using the json module\n",
    "        with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(self.all_jobs_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    def display_dataset(self):\n",
    "        # Convert the list of job data to a pandas DataFrame for easy viewing and analysis\n",
    "        df = pd.DataFrame(self.all_jobs_data)\n",
    "        print(df)\n",
    "        return df\n",
    "\n",
    "    def descriptive_statistics(self):\n",
    "        # Generate descriptive statistics for the dataset\n",
    "        df = pd.DataFrame(self.all_jobs_data)\n",
    "        if not df.empty:\n",
    "            print(df.describe(include='all'))\n",
    "        else:\n",
    "            print(\"No data available to generate statistics.\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create an instance of the Jobinja class\n",
    "    jobinja_scraper = Jobinja()\n",
    "    \n",
    "    # Scrape job listings\n",
    "    jobinja_scraper.scrape_jobs()\n",
    "    \n",
    "    # Save the dataset to .mat and .json files\n",
    "    jobinja_scraper.save_dataset()\n",
    "    \n",
    "    # Display the dataset\n",
    "    df = jobinja_scraper.display_dataset()\n",
    "    \n",
    "    # Generate descriptive statistics\n",
    "    jobinja_scraper.descriptive_statistics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0dca6f-3bc9-473e-89b4-fc1f2bf01b56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import urllib3\n",
    "import json\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "class Jobinja:\n",
    "    def __init__(self, base_url=\"https://jobinja.ir/\", save_dir=\"JobInja\"):\n",
    "        self.base_url = base_url\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "        }\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(self.save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "        self.all_jobs_data = []\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)  # Suppress InsecureRequestWarning\n",
    "\n",
    "    def get_links(self):\n",
    "        try:\n",
    "            # Send a GET request to the main page\n",
    "            response = requests.get(self.base_url, headers=self.headers, verify=False, timeout=10)\n",
    "            response.raise_for_status()  # Raise an error if the request failed\n",
    "            \n",
    "            # Parse the HTML content using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Extract all the links to tabs or subpages\n",
    "            links = soup.find_all('a', href=True)\n",
    "            \n",
    "            # Collect unique links to avoid repetition\n",
    "            subpage_links = set()  # Use a set to store unique links\n",
    "\n",
    "            # Iterate through all the links found on the main page\n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                if href.startswith('/'):  # It's a relative link, prepend base URL\n",
    "                    full_url = self.base_url + href\n",
    "                elif href.startswith('http'):  # It's an absolute link\n",
    "                    full_url = href\n",
    "                else:\n",
    "                    continue  # Skip if the href doesn't look like a valid link\n",
    "\n",
    "                subpage_links.add(full_url)  # Add the link to the set of subpage links\n",
    "\n",
    "            return subpage_links\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"An error occurred while getting links: {e}\")\n",
    "            return set()\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_persian_text(text):\n",
    "        # Remove extra spaces, non-Persian characters, and control characters\n",
    "        cleaned_text = re.sub(r'[^؀-ۿ\\s]', ' ', text)  # Keep only Persian characters and spaces\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Replace multiple spaces with a single space\n",
    "        return cleaned_text.strip()  # Strip leading and trailing spaces\n",
    "\n",
    "    def extract_job_features(self, subpage_soup):\n",
    "        # This function will extract and clean text related to job name, description, and other features\n",
    "        job_features = {}\n",
    "\n",
    "        # Extract job title (assuming job title is in an <h1> tag)\n",
    "        job_title_tag = subpage_soup.find('h1')\n",
    "        if job_title_tag:\n",
    "            job_features['job_title'] = self.clean_persian_text(job_title_tag.get_text())\n",
    "\n",
    "        # Extract job description (assuming job description is in a specific <div> class)\n",
    "        job_description_tag = subpage_soup.find('div', class_='job-description')  # Replace with the actual class name\n",
    "        if job_description_tag:\n",
    "            job_features['job_description'] = self.clean_persian_text(job_description_tag.get_text())\n",
    "\n",
    "        # Extract company name (assuming company name is in a specific <div> class)\n",
    "        company_name_tag = subpage_soup.find('div', class_='company-name')  # Replace with the actual class name\n",
    "        if company_name_tag:\n",
    "            job_features['company_name'] = self.clean_persian_text(company_name_tag.get_text())\n",
    "\n",
    "        # Extract content snippet (all text from the page)\n",
    "        job_features['content_snippet'] = self.clean_persian_text(subpage_soup.get_text())\n",
    "\n",
    "        # Extract suggested jobs (assuming they are in a specific <div> class)\n",
    "        suggested_jobs = self.extract_suggested_jobs(subpage_soup)\n",
    "        if suggested_jobs:\n",
    "            job_features['suggested_jobs'] = suggested_jobs\n",
    "\n",
    "        return job_features\n",
    "\n",
    "    def extract_suggested_jobs(self, soup):\n",
    "        # This function extracts job listings from the \"آگهی های پیشنهادی\" section\n",
    "        suggested_jobs_section = soup.find('div', class_='suggested-jobs')  # Replace with the actual class name\n",
    "        suggested_jobs = []\n",
    "\n",
    "        if suggested_jobs_section:\n",
    "            job_items = suggested_jobs_section.find_all('div', class_='job-item')  # Replace with the actual class name\n",
    "            for job_item in job_items:\n",
    "                job_title = job_item.find('h2')  # Assuming the job title is in an <h2> tag\n",
    "                company_name = job_item.find('div', class_='company-name')  # Replace with the actual class name\n",
    "                if job_title and company_name:\n",
    "                    suggested_jobs.append(\n",
    "                        f\"{self.clean_persian_text(job_title.get_text())} - {self.clean_persian_text(company_name.get_text())}\"\n",
    "                    )\n",
    "        return ' | '.join(suggested_jobs) if suggested_jobs else None\n",
    "\n",
    "    def scrape_jobs(self):\n",
    "        subpage_links = self.get_links()\n",
    "\n",
    "        # Iterate through each subpage link to extract job information\n",
    "        for subpage_url in subpage_links:\n",
    "            try:\n",
    "                # Send a GET request to each subpage\n",
    "                subpage_response = requests.get(subpage_url, headers=self.headers, verify=False, timeout=10)\n",
    "                subpage_response.raise_for_status()  # Raise an error if the request failed\n",
    "\n",
    "                # Parse the HTML content of the subpage\n",
    "                subpage_soup = BeautifulSoup(subpage_response.text, 'html.parser')\n",
    "                \n",
    "                # Extract job features from the subpage\n",
    "                job_data = self.extract_job_features(subpage_soup)\n",
    "\n",
    "                # Save the extracted job features if available\n",
    "                if job_data:\n",
    "                    self.all_jobs_data.append(job_data)\n",
    "\n",
    "                    # Save all features to a .txt file\n",
    "                    job_title = job_data.get('job_title', 'JobInja')  # Use 'JobInja' if job title is not available\n",
    "                    file_path = os.path.join(self.save_dir, f\"{job_title}.txt\")\n",
    "                    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                        for key, value in job_data.items():\n",
    "                            file.write(f\"{key.capitalize()}: {value} | \")\n",
    "\n",
    "                    # Print the extracted job features to the console\n",
    "                    print(f\"URL: {subpage_url}\")\n",
    "                    for key, value in job_data.items():\n",
    "                        print(f\"{key.capitalize()}: {value}\")\n",
    "                    print(\"\\n\")\n",
    "\n",
    "                # To avoid overwhelming the server, add a short delay between requests\n",
    "                time.sleep(1)\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                # Print an error message if a request fails\n",
    "                print(f\"An error occurred while accessing {subpage_url}: {e}\")\n",
    "\n",
    "    def save_dataset(self):\n",
    "        # Save the dataset to .mat and .json files\n",
    "        mat_file_path = os.path.join(self.save_dir, \"jobinja_data.mat\")\n",
    "        json_file_path = os.path.join(self.save_dir, \"jobinja_data.json\")\n",
    "\n",
    "        # Convert job data to a format suitable for saving\n",
    "        job_data_dict = {f\"job_{i}\": job for i, job in enumerate(self.all_jobs_data)}\n",
    "        \n",
    "        # Save as .mat file using SciPy's savemat function\n",
    "        sio.savemat(mat_file_path, job_data_dict)\n",
    "\n",
    "        # Save as .json file using the json module\n",
    "        with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(self.all_jobs_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    def display_dataset(self):\n",
    "        # Convert the list of job data to a pandas DataFrame for easy viewing and analysis\n",
    "        df = pd.DataFrame(self.all_jobs_data)\n",
    "        print(df)\n",
    "        return df\n",
    "\n",
    "    def descriptive_statistics(self):\n",
    "        # Generate descriptive statistics for the dataset\n",
    "        df = pd.DataFrame(self.all_jobs_data)\n",
    "        if not df.empty:\n",
    "            print(df.describe(include='all'))\n",
    "        else:\n",
    "            print(\"No data available to generate statistics.\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create an instance of the Jobinja class\n",
    "    jobinja_scraper = Jobinja()\n",
    "    \n",
    "    # Scrape job listings\n",
    "    jobinja_scraper.scrape_jobs()\n",
    "    \n",
    "    # Save the dataset to .mat and .json files\n",
    "    jobinja_scraper.save_dataset()\n",
    "    \n",
    "    # Display the dataset\n",
    "    df = jobinja_scraper.display_dataset()\n",
    "    \n",
    "    # Generate descriptive statistics\n",
    "    jobinja_scraper.descriptive_statistics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4dbcb59f-56fc-42bd-80c8-d5c64b83160e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "No data available to generate statistics.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import urllib3\n",
    "import json\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "class Jobinja:\n",
    "    def __init__(self, base_url=\"https://jobinja.ir/\", save_dir=\"JobInja/v2\"):\n",
    "        self.base_url = base_url\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "        }\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(self.save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "        self.all_jobs_data = []\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)  # Suppress InsecureRequestWarning\n",
    "\n",
    "    def get_links(self):\n",
    "        try:\n",
    "            # Send a GET request to the main page\n",
    "            response = requests.get(self.base_url, headers=self.headers, verify=False, timeout=10)\n",
    "            response.raise_for_status()  # Raise an error if the request failed\n",
    "            \n",
    "            # Parse the HTML content using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Extract all the links to tabs or subpages\n",
    "            links = soup.find_all('a', href=True)\n",
    "            \n",
    "            # Collect unique links to avoid repetition\n",
    "            subpage_links = set()  # Use a set to store unique links\n",
    "\n",
    "            # Iterate through all the links found on the main page\n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                if href.startswith('/'):  # It's a relative link, prepend base URL\n",
    "                    full_url = self.base_url + href\n",
    "                elif href.startswith('http'):  # It's an absolute link\n",
    "                    full_url = href\n",
    "                else:\n",
    "                    continue  # Skip if the href doesn't look like a valid link\n",
    "\n",
    "                subpage_links.add(full_url)  # Add the link to the set of subpage links\n",
    "\n",
    "            return subpage_links\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"An error occurred while getting links: {e}\")\n",
    "            return set()\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_persian_text(text):\n",
    "        # Remove extra spaces, non-Persian characters, and control characters\n",
    "        cleaned_text = re.sub(r'[^؀-ۿ\\s]', ' ', text)  # Keep only Persian characters and spaces\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Replace multiple spaces with a single space\n",
    "        return cleaned_text.strip()  # Strip leading and trailing spaces\n",
    "\n",
    "    def extract_job_features(self, subpage_soup):\n",
    "        # This function will extract and clean text related to job name, description, and other features\n",
    "        job_features = {}\n",
    "\n",
    "        # Extract job title (assuming job title is in an <h1> tag)\n",
    "        job_title_tag = subpage_soup.find('h1')\n",
    "        if job_title_tag:\n",
    "            job_features['job_title'] = self.clean_persian_text(job_title_tag.get_text())\n",
    "\n",
    "        # Extract job description (assuming job description is in a specific <div> class)\n",
    "        job_description_tag = subpage_soup.find('div', class_='job-description')  # Replace with the actual class name\n",
    "        if job_description_tag:\n",
    "            job_features['job_description'] = self.clean_persian_text(job_description_tag.get_text())\n",
    "\n",
    "        # Extract company name (assuming company name is in a specific <div> class)\n",
    "        company_name_tag = subpage_soup.find('div', class_='company-name')  # Replace with the actual class name\n",
    "        if company_name_tag:\n",
    "            job_features['company_name'] = self.clean_persian_text(company_name_tag.get_text())\n",
    "\n",
    "        # Extract additional features\n",
    "        additional_features = {\n",
    "            'job_category': ('div', 'job-category'),  # Replace with the actual tag and class name\n",
    "            'job_location': ('div', 'job-location'),\n",
    "            'employment_type': ('div', 'employment-type'),\n",
    "            'min_experience': ('div', 'min-experience'),\n",
    "            'salary': ('div', 'salary'),\n",
    "            'gender': ('div', 'gender'),\n",
    "            'military_status': ('div', 'military-status'),\n",
    "            'education_level': ('div', 'education-level'),\n",
    "            'company_intro': ('div', 'company-intro'),\n",
    "            'skills_required': ('div', 'skills-required'),\n",
    "        }\n",
    "\n",
    "        for feature, (tag, class_name) in additional_features.items():\n",
    "            feature_tag = subpage_soup.find(tag, class_=class_name)\n",
    "            if feature_tag:\n",
    "                job_features[feature] = self.clean_persian_text(feature_tag.get_text())\n",
    "\n",
    "        # Extract content snippet (all text from the page)\n",
    "        job_features['content_snippet'] = self.clean_persian_text(subpage_soup.get_text())\n",
    "\n",
    "        # Extract suggested jobs (assuming they are in a specific <div> class)\n",
    "        suggested_jobs = self.extract_suggested_jobs(subpage_soup)\n",
    "        if suggested_jobs:\n",
    "            job_features['suggested_jobs'] = suggested_jobs\n",
    "\n",
    "        return job_features\n",
    "\n",
    "    def extract_suggested_jobs(self, soup):\n",
    "        # This function extracts job listings from the \"آگهی های پیشنهادی\" section\n",
    "        suggested_jobs_section = soup.find('div', class_='suggested-jobs')  # Replace with the actual class name\n",
    "        suggested_jobs = []\n",
    "\n",
    "        if suggested_jobs_section:\n",
    "            job_items = suggested_jobs_section.find_all('div', class_='job-item')  # Replace with the actual class name\n",
    "            for job_item in job_items:\n",
    "                job_title = job_item.find('h2')  # Assuming the job title is in an <h2> tag\n",
    "                company_name = job_item.find('div', class_='company-name')  # Replace with the actual class name\n",
    "                if job_title and company_name:\n",
    "                    suggested_jobs.append(\n",
    "                        f\"{self.clean_persian_text(job_title.get_text())} - {self.clean_persian_text(company_name.get_text())}\"\n",
    "                    )\n",
    "        return ' | '.join(suggested_jobs) if suggested_jobs else None\n",
    "\n",
    "    def scrape_jobs(self):\n",
    "        subpage_links = self.get_links()\n",
    "\n",
    "        # Iterate through each subpage link to extract job information\n",
    "        for subpage_url in subpage_links:\n",
    "            try:\n",
    "                # Send a GET request to each subpage\n",
    "                subpage_response = requests.get(subpage_url, headers=self.headers, verify=False, timeout=10)\n",
    "                subpage_response.raise_for_status()  # Raise an error if the request failed\n",
    "\n",
    "                # Parse the HTML content of the subpage\n",
    "                subpage_soup = BeautifulSoup(subpage_response.text, 'html.parser')\n",
    "                \n",
    "                # Extract job features from the subpage\n",
    "                job_data = self.extract_job_features(subpage_soup)\n",
    "\n",
    "                # Save the extracted job features if available\n",
    "                if job_data:\n",
    "                    self.all_jobs_data.append(job_data)\n",
    "\n",
    "                    # Save all features to a .txt file\n",
    "                    job_title = job_data.get('job_title', 'JobInja')  # Use 'JobInja' if job title is not available\n",
    "                    file_path = os.path.join(self.save_dir, f\"{job_title}.txt\")\n",
    "                    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                        for key, value in job_data.items():\n",
    "                            file.write(f\"{key.capitalize()}: {value} | \")\n",
    "\n",
    "                    # Print the extracted job features to the console\n",
    "                    print(f\"URL: {subpage_url}\")\n",
    "                    for key, value in job_data.items():\n",
    "                        print(f\"{key.capitalize()}: {value}\")\n",
    "                    print(\"\\n\")\n",
    "\n",
    "                # To avoid overwhelming the server, add a short delay between requests\n",
    "                time.sleep(1)\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                # Print an error message if a request fails\n",
    "                print(f\"An error occurred while accessing {subpage_url}: {e}\")\n",
    "\n",
    "    def save_dataset(self):\n",
    "        # Save the dataset to .mat and .json files\n",
    "        mat_file_path = os.path.join(self.save_dir, \"jobinja_data.mat\")\n",
    "        json_file_path = os.path.join(self.save_dir, \"jobinja_data.json\")\n",
    "\n",
    "        # Convert job data to a format suitable for saving\n",
    "        job_data_dict = {f\"job_{i}\": job for i, job in enumerate(self.all_jobs_data)}\n",
    "        \n",
    "        # Save as .mat file using SciPy's savemat function\n",
    "        sio.savemat(mat_file_path, job_data_dict)\n",
    "\n",
    "        # Save as .json file using the json module\n",
    "        with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(self.all_jobs_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    def display_dataset(self):\n",
    "        # Convert the list of job data to a pandas DataFrame for easy viewing and analysis\n",
    "        df = pd.DataFrame(self.all_jobs_data)\n",
    "        print(df)\n",
    "        return df\n",
    "\n",
    "    def descriptive_statistics(self):\n",
    "        # Generate descriptive statistics for the dataset\n",
    "        df = pd.DataFrame(self.all_jobs_data)\n",
    "        if not df.empty:\n",
    "            print(df.describe(include='all'))\n",
    "        else:\n",
    "            print(\"No data available to generate statistics.\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create an instance of the Jobinja class\n",
    "    jobinja_scraper = Jobinja()\n",
    "    \n",
    "    # Scrape job listings\n",
    "    jobinja_scraper.scrape_jobs()\n",
    "    \n",
    "    # Save the dataset to .mat and .json files\n",
    "    jobinja_scraper.save_dataset()\n",
    "    \n",
    "    # Display the dataset\n",
    "    df = jobinja_scraper.display_dataset()\n",
    "    \n",
    "    # Generate descriptive statistics\n",
    "    jobinja_scraper.descriptive_statistics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61e77a78-5092-47db-bef9-1cd33d564c7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "No data available to generate statistics.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import urllib3\n",
    "import json\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import pandas as pd\n",
    "from queue import Queue\n",
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "# Configure logging with a rotating file handler\n",
    "log_handler = RotatingFileHandler('jobinja_scraper.log', maxBytes=5*1024*1024, backupCount=2)\n",
    "log_handler.setLevel(logging.ERROR)\n",
    "log_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "log_handler.setFormatter(log_formatter)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "logger.addHandler(log_handler)\n",
    "\n",
    "class Jobinja:\n",
    "    def __init__(self, base_url=\"https://jobinja.ir/\", save_dir=\"JobInja/v7\"):\n",
    "        self.base_url = base_url\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "        }\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(self.save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "        self.all_jobs_data = []\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)  # Suppress InsecureRequestWarning\n",
    "\n",
    "    def get_links(self):\n",
    "        try:\n",
    "            # Send a GET request to the main page\n",
    "            response = requests.get(self.base_url, headers=self.headers, verify=False, timeout=10)\n",
    "            response.raise_for_status()  # Raise an error if the request failed\n",
    "            \n",
    "            # Parse the HTML content using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Extract all the links to tabs or subpages\n",
    "            links = soup.find_all('a', href=True)\n",
    "            \n",
    "            # Collect unique links using a queue for breadth-first crawling\n",
    "            subpage_links = Queue()\n",
    "            unique_links = set()  # To avoid duplicates\n",
    "\n",
    "            # Iterate through all the links found on the main page\n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                if href.startswith('/'):  # It's a relative link, prepend base URL\n",
    "                    full_url = self.base_url + href\n",
    "                elif href.startswith('http'):  # It's an absolute link\n",
    "                    full_url = href\n",
    "                else:\n",
    "                    continue  # Skip if the href doesn't look like a valid link\n",
    "\n",
    "                if full_url not in unique_links:\n",
    "                    subpage_links.put(full_url)\n",
    "                    unique_links.add(full_url)\n",
    "\n",
    "            return subpage_links\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"An error occurred while getting links: {e}\")\n",
    "            return Queue()\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_persian_text(text):\n",
    "        # Remove extra spaces, non-Persian characters, and control characters\n",
    "        cleaned_text = re.sub(r'[^؀-ۿ\\s]', ' ', text)  # Keep only Persian characters and spaces\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Replace multiple spaces with a single space\n",
    "        return cleaned_text.strip()  # Strip leading and trailing spaces\n",
    "\n",
    "    def extract_job_features(self, subpage_soup):\n",
    "        \"\"\"Extract job features from the page\"\"\"\n",
    "        job_features = {}\n",
    "\n",
    "        # Extract job title (using the nested structure)\n",
    "        title_container = subpage_soup.find('div', class_='c-jobView__title o-box__mainTitle u-clearFix')\n",
    "        if title_container:\n",
    "            title_text_div = title_container.find('div', class_='c-jobView__titleText')\n",
    "            if title_text_div:\n",
    "                title_h1 = title_text_div.find('h1')\n",
    "                if title_h1:\n",
    "                    job_features['job_title'] = self.clean_persian_text(title_h1.get_text())\n",
    "\n",
    "        # Extract additional features with correct Jobinja classes\n",
    "        additional_features = {\n",
    "            # Category info (in the meta section)\n",
    "            'job_category': ('span', 'c-jobView__metaItem--category'),\n",
    "\n",
    "            # Location info\n",
    "            'job_location': ('span', 'c-jobView__metaItem--location'),\n",
    "\n",
    "            # Employment type/contract\n",
    "            'employment_type': ('span', 'c-jobView__metaItem--contract'),\n",
    "\n",
    "            # Experience requirement\n",
    "            'min_experience': ('div', 'c-jobView__metaItem--experience'),\n",
    "\n",
    "            # Salary info\n",
    "            'salary': ('div', 'c-jobView__metaItem--salary'),\n",
    "\n",
    "            # Gender preference\n",
    "            'gender': ('div', 'c-jobView__metaItem--gender'),\n",
    "\n",
    "            # Military service status\n",
    "            'military_status': ('div', 'c-jobView__metaItem--military'),\n",
    "\n",
    "            # Education requirement\n",
    "            'education_level': ('div', 'c-jobView__metaItem--education'),\n",
    "\n",
    "            # Company description\n",
    "            'company_intro': ('div', 'c-companyDetail__description'),\n",
    "\n",
    "            # Required skills\n",
    "            'skills_required': ('div', 'c-jobView__skills'),\n",
    "\n",
    "            # Job description\n",
    "            'job_description': ('div', 's-jobDesc c-jobView__txtDesc'),\n",
    "\n",
    "            # Benefits\n",
    "            'benefits': ('div', 'c-jobBenefit__list'),\n",
    "\n",
    "            # Application deadline\n",
    "            'deadline': ('div', 'c-jobView__metaItem--deadline'),\n",
    "\n",
    "            # Company name\n",
    "            'company_name': ('h2', 'c-companyHeader__name'),\n",
    "\n",
    "            # Company size\n",
    "            'company_size': ('div', 'c-companyHeader__metaItem--size'),\n",
    "\n",
    "            # Company industry\n",
    "            'industry': ('div', 'c-companyHeader__metaItem--industry'),\n",
    "\n",
    "            # Work hours\n",
    "            'work_hours': ('div', 'c-jobView__metaItem--hours'),\n",
    "\n",
    "            # Required languages\n",
    "            'languages': ('div', 'c-jobView__metaItem--languages')\n",
    "        }\n",
    "\n",
    "        # Extract each feature\n",
    "        for feature, (tag, class_name) in additional_features.items():\n",
    "            element = subpage_soup.find(tag, class_=class_name)\n",
    "            if element:\n",
    "                # Special handling for skills\n",
    "                if feature == 'skills_required':\n",
    "                    skills_list = element.find_all('li', class_='c-jobSkills__item')\n",
    "                    if skills_list:\n",
    "                        job_features[feature] = ', '.join(\n",
    "                            [self.clean_persian_text(skill.get_text()) for skill in skills_list]\n",
    "                        )\n",
    "                # Special handling for benefits\n",
    "                elif feature == 'benefits':\n",
    "                    benefits_list = element.find_all('div', class_='c-jobBenefit__item')\n",
    "                    if benefits_list:\n",
    "                        job_features[feature] = ', '.join(\n",
    "                            [self.clean_persian_text(benefit.get_text()) for benefit in benefits_list]\n",
    "                        )\n",
    "                else:\n",
    "                    job_features[feature] = self.clean_persian_text(element.get_text())\n",
    "\n",
    "        return job_features\n",
    "\n",
    "    def scrape_jobs(self):\n",
    "        subpage_links = self.get_links()\n",
    "\n",
    "        # Iterate through each subpage link to extract job information\n",
    "        while not subpage_links.empty():\n",
    "            subpage_url = subpage_links.get()\n",
    "            try:\n",
    "                # Send a GET request to each subpage\n",
    "                subpage_response = requests.get(subpage_url, headers=self.headers, verify=False, timeout=10)\n",
    "                subpage_response.raise_for_status()  # Raise an error if the request failed\n",
    "\n",
    "                # Parse the HTML content of the subpage\n",
    "                subpage_soup = BeautifulSoup(subpage_response.text, 'html.parser')\n",
    "                \n",
    "                # Extract job features from the subpage\n",
    "                job_data = self.extract_job_features(subpage_soup)\n",
    "\n",
    "                # Save the extracted job features if available\n",
    "                if job_data:\n",
    "                    self.all_jobs_data.append(job_data)\n",
    "\n",
    "                    # Save all features to a .txt file\n",
    "                    job_title = job_data.get('job_title', 'company_name', 'job_category' , 'job_location',\n",
    "                                             'employment_type' , 'min_experience',\n",
    "                                             'education_level' , 'military_status' , 'skills_required','benefits', 'job_description')  # Use 'JobInja' if job title is not available\n",
    "                    file_path = os.path.join(self.save_dir, f\"{job_title}.txt\")\n",
    "                    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                        for key, value in job_data.items():\n",
    "                            file.write(f\"{key.capitalize()}: {value}\\n\")\n",
    "\n",
    "                    # Print the extracted job features to the console\n",
    "                    print(f\"URL: {subpage_url}\")\n",
    "                    for key, value in job_data.items():\n",
    "                        print(f\"{key.capitalize()}: {value}\")\n",
    "                    print(\"\\n\")\n",
    "\n",
    "                # To avoid overwhelming the server, add a short delay between requests\n",
    "                time.sleep(1)\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                # Log an error message if a request fails\n",
    "                logger.error(f\"An error occurred while accessing {subpage_url}: {e}\")\n",
    "\n",
    "    def save_dataset(self):\n",
    "        # Save the dataset to .mat and .json files\n",
    "        mat_file_path = os.path.join(self.save_dir, \"jobinja_data.mat\")\n",
    "        json_file_path = os.path.join(self.save_dir, \"jobinja_data.json\")\n",
    "\n",
    "        # Convert job data to a format suitable for saving\n",
    "        job_data_dict = {f\"job_{i}\": job for i, job in enumerate(self.all_jobs_data)}\n",
    "        \n",
    "        # Save as .mat file using SciPy's savemat function\n",
    "        sio.savemat(mat_file_path, job_data_dict)\n",
    "\n",
    "        # Save as .json file using the json module\n",
    "        with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(self.all_jobs_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    def display_dataset(self):\n",
    "        # Convert the list of job data to a pandas DataFrame for easy viewing and analysis\n",
    "        df = pd.DataFrame(self.all_jobs_data)\n",
    "        print(df)\n",
    "        return df\n",
    "\n",
    "    def descriptive_statistics(self):\n",
    "        # Generate descriptive statistics for the dataset\n",
    "        df = pd.DataFrame(self.all_jobs_data)\n",
    "        if not df.empty:\n",
    "            print(df.describe(include='all'))\n",
    "        else:\n",
    "            print(\"No data available to generate statistics.\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create an instance of the Jobinja class\n",
    "    jobinja_scraper = Jobinja()\n",
    "    \n",
    "    # Scrape job listings\n",
    "    jobinja_scraper.scrape_jobs()\n",
    "    \n",
    "    # Save the dataset to .mat and .json files\n",
    "    jobinja_scraper.save_dataset()\n",
    "    \n",
    "    # Display the dataset\n",
    "    df = jobinja_scraper.display_dataset()\n",
    "    \n",
    "    # Generate descriptive statistics\n",
    "    jobinja_scraper.descriptive_statistics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "221856c3-1307-4178-94c5-ab57fa61b8f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import urllib3\n",
    "import json\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from hazm import *\n",
    "\n",
    "class Jobinja:\n",
    "    def __init__(self, base_url=\"https://jobinja.ir/\", save_dir=\"JobInja/v4\"):\n",
    "        self.base_url = base_url\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "        }\n",
    "        self.save_dir = save_dir\n",
    "        self.corpus_dir = os.path.join(save_dir, 'corpus')\n",
    "        self.metadata_dir = os.path.join(save_dir, 'metadata')\n",
    "        \n",
    "        # Create necessary directories\n",
    "        for directory in [self.save_dir, self.corpus_dir, self.metadata_dir]:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "        self.all_jobs_data = []\n",
    "        self.metadata = defaultdict(list)\n",
    "        self.normalizer = Normalizer()\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "    def get_links(self):\n",
    "        try:\n",
    "            response = requests.get(self.base_url + \"jobs\", headers=self.headers, verify=False, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            job_cards = soup.find_all('div', class_='c-jobListView__titleRow')\n",
    "            \n",
    "            subpage_links = set()\n",
    "            for card in job_cards:\n",
    "                link = card.find('a', href=True)\n",
    "                if link:\n",
    "                    href = link['href']\n",
    "                    if href.startswith('/'):\n",
    "                        full_url = self.base_url.rstrip('/') + href\n",
    "                    elif href.startswith('http'):\n",
    "                        full_url = href\n",
    "                    else:\n",
    "                        continue\n",
    "                    subpage_links.add(full_url)\n",
    "            \n",
    "            return subpage_links\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"An error occurred while getting links: {e}\")\n",
    "            return set()\n",
    "\n",
    "    def scrape_jobs(self, max_pages=None):\n",
    "        \"\"\"\n",
    "        Scrape jobs with optional page limit\n",
    "        Args:\n",
    "            max_pages: Maximum number of pages to scrape (None for all pages)\n",
    "        \"\"\"\n",
    "        page = 1\n",
    "        total_jobs = 0\n",
    "        \n",
    "        while max_pages is None or page <= max_pages:\n",
    "            try:\n",
    "                # Construct URL for current page\n",
    "                page_url = f\"{self.base_url}jobs?page={page}\"\n",
    "                print(f\"Scraping page {page}...\")\n",
    "                \n",
    "                # Get job links from current page\n",
    "                subpage_links = self.get_links()\n",
    "                if not subpage_links:\n",
    "                    print(f\"No more jobs found on page {page}\")\n",
    "                    break\n",
    "                \n",
    "                # Process each job posting\n",
    "                for subpage_url in subpage_links:\n",
    "                    try:\n",
    "                        subpage_response = requests.get(subpage_url, headers=self.headers, verify=False, timeout=10)\n",
    "                        subpage_response.raise_for_status()\n",
    "                        \n",
    "                        subpage_soup = BeautifulSoup(subpage_response.text, 'html.parser')\n",
    "                        job_data = self.extract_job_features(subpage_soup)\n",
    "                        \n",
    "                        if job_data:\n",
    "                            # Add metadata\n",
    "                            job_data['scrape_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "                            job_data['url'] = subpage_url\n",
    "                            job_data['page_number'] = page\n",
    "                            \n",
    "                            self.all_jobs_data.append(job_data)\n",
    "                            self.update_metadata(job_data)\n",
    "                            total_jobs += 1\n",
    "                            \n",
    "                            # Save individual job data\n",
    "                            self.save_individual_job(job_data)\n",
    "                            \n",
    "                        time.sleep(1)  # Polite delay between requests\n",
    "                        \n",
    "                    except requests.exceptions.RequestException as e:\n",
    "                        print(f\"Error processing job at {subpage_url}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                page += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing page {page}: {e}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"Completed scraping {total_jobs} jobs across {page-1} pages\")\n",
    "        self.save_dataset()  # Save the complete dataset\n",
    "        self.save_metadata()  # Save metadata\n",
    "\n",
    "    def save_individual_job(self, job_data):\n",
    "        \"\"\"Save individual job data to a file\"\"\"\n",
    "        try:\n",
    "            job_title = job_data.get('job_title', 'unknown_job')\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            filename = f\"{job_title}_{timestamp}.json\"\n",
    "            filepath = os.path.join(self.save_dir, 'jobs', filename)\n",
    "            \n",
    "            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "            \n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                json.dump(job_data, f, ensure_ascii=False, indent=4)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving individual job data: {e}\")\n",
    "\n",
    "    def clean_persian_text(self, text):\n",
    "            \"\"\"\n",
    "            Enhanced text cleaning for Persian text\n",
    "            \"\"\"\n",
    "            if not text:\n",
    "                return \"\"\n",
    "\n",
    "            # Basic cleaning\n",
    "            text = re.sub(r'[\\u200c\\u200b\\u200d\\u200e\\u200f]', ' ', text)  # Remove zero-width characters\n",
    "            text = re.sub(r'[^\\u0600-\\u06FF\\s]', ' ', text)  # Keep only Persian characters and spaces\n",
    "\n",
    "            # Normalize using hazm\n",
    "            text = self.normalizer.normalize(text)\n",
    "\n",
    "            # Additional cleaning steps\n",
    "            text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with single space\n",
    "            text = text.strip()  # Remove leading and trailing spaces\n",
    "\n",
    "            return text\n",
    "\n",
    "    def extract_job_features(self, subpage_soup):\n",
    "        \"\"\"Extract job features from the page\"\"\"\n",
    "        job_features = {}\n",
    "\n",
    "        # Extract job title\n",
    "        job_title_tag = subpage_soup.find('h1', class_='c-jobView__titleText')\n",
    "        if job_title_tag:\n",
    "            job_features['job_title'] = self.clean_persian_text(job_title_tag.get_text())\n",
    "\n",
    "        # Extract job description\n",
    "        job_description_tag = subpage_soup.find('div', class_='o-box__text s-jobDesc')\n",
    "        if job_description_tag:\n",
    "            job_features['job_description'] = self.clean_persian_text(job_description_tag.get_text())\n",
    "\n",
    "        # Extract company name\n",
    "        company_name_tag = subpage_soup.find('span', class_='c-jobView__metaCompanyName')\n",
    "        if company_name_tag:\n",
    "            job_features['company_name'] = self.clean_persian_text(company_name_tag.get_text())\n",
    "\n",
    "        # Extract additional features\n",
    "        additional_features = {\n",
    "            'job_category': ('li', 'c-jobView__metaItem'),\n",
    "            'job_location': ('div', 'c-jobView__metaLocationIcon'),\n",
    "            'employment_type': ('div', 'c-jobView__metaContractType'),\n",
    "            'min_experience': ('span', 'c-jobView__minExperience'),\n",
    "            'salary': ('span', 'salary'),\n",
    "            'gender': ('div', 'c-jobView__gender'),\n",
    "            'military_status': ('div', 'c-jobView__militaryStatus'),\n",
    "            'education_level': ('div', 'c-jobView__educationLevel'),\n",
    "            'company_intro': ('div', 'c-companyDetail__description'),\n",
    "            'skills_required': ('div', 'c-jobView__skills')\n",
    "        }\n",
    "\n",
    "        for feature, (tag, class_name) in additional_features.items():\n",
    "            if feature == 'job_category':\n",
    "                category_items = subpage_soup.find_all(tag, class_=class_name)\n",
    "                for item in category_items:\n",
    "                    if 'دسته‌بندی شغلی' in item.get_text():\n",
    "                        job_features[feature] = self.clean_persian_text(item.get_text().split(':')[-1])\n",
    "                        break\n",
    "            else:\n",
    "                feature_tag = subpage_soup.find(tag, class_=class_name)\n",
    "                if feature_tag:\n",
    "                    job_features[feature] = self.clean_persian_text(feature_tag.get_text())\n",
    "\n",
    "        # Extract content snippet\n",
    "        job_features['content_snippet'] = self.clean_persian_text(subpage_soup.get_text())\n",
    "\n",
    "        # Extract suggested jobs\n",
    "        suggested_jobs = self.extract_suggested_jobs(subpage_soup)\n",
    "        if suggested_jobs:\n",
    "            job_features['suggested_jobs'] = suggested_jobs\n",
    "\n",
    "        return job_features\n",
    "\n",
    "    def extract_suggested_jobs(self, soup):\n",
    "        \"\"\"Extract suggested jobs from the page\"\"\"\n",
    "        suggested_jobs_section = soup.find('div', class_='c-similarJobs')\n",
    "        suggested_jobs = []\n",
    "\n",
    "        if suggested_jobs_section:\n",
    "            job_items = suggested_jobs_section.find_all('div', class_='c-similarJobs__item')\n",
    "            for job_item in job_items:\n",
    "                job_title = job_item.find('a', class_='c-similarJobs__itemTitle')\n",
    "                company_name = job_item.find('span', class_='c-similarJobs__itemCompany')\n",
    "                if job_title and company_name:\n",
    "                    suggested_jobs.append(\n",
    "                        f\"{self.clean_persian_text(job_title.get_text())} - {self.clean_persian_text(company_name.get_text())}\"\n",
    "                    )\n",
    "        \n",
    "        return ' | '.join(suggested_jobs) if suggested_jobs else None\n",
    "    def update_metadata(self, job_data):\n",
    "        \"\"\"Update metadata statistics from job data\"\"\"\n",
    "        # Update job categories count\n",
    "        if 'job_category' in job_data:\n",
    "            self.metadata['categories'].append(job_data['job_category'])\n",
    "            \n",
    "        # Update company statistics\n",
    "        if 'company_name' in job_data:\n",
    "            self.metadata['companies'].append(job_data['company_name'])\n",
    "            \n",
    "        # Update location statistics\n",
    "        if 'job_location' in job_data:\n",
    "            self.metadata['locations'].append(job_data['job_location'])\n",
    "            \n",
    "        # Update employment types\n",
    "        if 'employment_type' in job_data:\n",
    "            self.metadata['employment_types'].append(job_data['employment_type'])\n",
    "            \n",
    "        # Track required skills\n",
    "        if 'skills_required' in job_data:\n",
    "            skills = job_data['skills_required'].split('،') if job_data['skills_required'] else []\n",
    "            self.metadata['skills'].extend([skill.strip() for skill in skills])\n",
    "\n",
    "    def save_metadata(self):\n",
    "        \"\"\"Save metadata analysis to file\"\"\"\n",
    "        metadata_stats = {\n",
    "            'total_jobs': len(self.all_jobs_data),\n",
    "            'unique_companies': len(set(self.metadata['companies'])),\n",
    "            'top_companies': pd.Series(self.metadata['companies']).value_counts().head(10).to_dict(),\n",
    "            'top_locations': pd.Series(self.metadata['locations']).value_counts().head(10).to_dict(),\n",
    "            'top_categories': pd.Series(self.metadata['categories']).value_counts().head(10).to_dict(),\n",
    "            'top_employment_types': pd.Series(self.metadata['employment_types']).value_counts().head(5).to_dict(),\n",
    "            'top_skills': pd.Series(self.metadata['skills']).value_counts().head(20).to_dict(),\n",
    "            'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        \n",
    "        # Save metadata to JSON\n",
    "        metadata_file = os.path.join(self.metadata_dir, 'job_metadata.json')\n",
    "        with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata_stats, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        # Create and save metadata visualizations\n",
    "        self.create_metadata_visualizations()\n",
    "\n",
    "    def create_metadata_visualizations(self):\n",
    "        \"\"\"Create visualizations for metadata analysis\"\"\"\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            import seaborn as sns\n",
    "            \n",
    "            # Set style for Persian text\n",
    "            plt.rcParams['font.family'] = 'Arial'  # or another font that supports Persian\n",
    "            \n",
    "            # Create visualizations directory\n",
    "            viz_dir = os.path.join(self.metadata_dir, 'visualizations')\n",
    "            os.makedirs(viz_dir, exist_ok=True)\n",
    "            \n",
    "            # Plot top companies\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            company_counts = pd.Series(self.metadata['companies']).value_counts().head(10)\n",
    "            company_counts.plot(kind='bar')\n",
    "            plt.title('Top 10 Companies by Job Postings')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(viz_dir, 'top_companies.png'))\n",
    "            plt.close()\n",
    "            \n",
    "            # Plot job categories\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            category_counts = pd.Series(self.metadata['categories']).value_counts().head(10)\n",
    "            category_counts.plot(kind='bar')\n",
    "            plt.title('Top 10 Job Categories')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(viz_dir, 'top_categories.png'))\n",
    "            plt.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating visualizations: {e}\")\n",
    "\n",
    "    def create_job_corpus(self):\n",
    "        \"\"\"Create a corpus from job descriptions for text mining\"\"\"\n",
    "        # Initialize tokenizer and lemmatizer\n",
    "        tokenizer = WordTokenizer()\n",
    "        lemmatizer = Lemmatizer()\n",
    "        \n",
    "        corpus = []\n",
    "        corpus_metadata = []\n",
    "        \n",
    "        for job in self.all_jobs_data:\n",
    "            if 'job_description' in job:\n",
    "                # Clean and normalize the text\n",
    "                clean_desc = self.clean_persian_text(job['job_description'])\n",
    "                \n",
    "                # Tokenize\n",
    "                tokens = tokenizer.tokenize(clean_desc)\n",
    "                \n",
    "                # Lemmatize\n",
    "                lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "                \n",
    "                # Remove stop words and short tokens\n",
    "                stop_words = set(stopwords_list())\n",
    "                filtered_tokens = [token for token in lemmas \n",
    "                                 if token not in stop_words and len(token) > 1]\n",
    "                \n",
    "                # Add to corpus\n",
    "                corpus.append(' '.join(filtered_tokens))\n",
    "                \n",
    "                # Add metadata\n",
    "                corpus_metadata.append({\n",
    "                    'job_id': len(corpus) - 1,\n",
    "                    'job_title': job.get('job_title', ''),\n",
    "                    'company': job.get('company_name', ''),\n",
    "                    'category': job.get('job_category', ''),\n",
    "                    'token_count': len(filtered_tokens)\n",
    "                })\n",
    "        \n",
    "        # Save corpus\n",
    "        corpus_file = os.path.join(self.corpus_dir, 'job_corpus_v4.txt')\n",
    "        with open(corpus_file, 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(corpus))\n",
    "        \n",
    "        # Save corpus metadata\n",
    "        corpus_meta_file = os.path.join(self.corpus_dir, 'corpus_metadata.json')\n",
    "        with open(corpus_meta_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(corpus_metadata, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        return corpus, corpus_metadata\n",
    "\n",
    "    def save_dataset(self):\n",
    "        \"\"\"Save the complete dataset in multiple formats\"\"\"\n",
    "        # Save as JSON\n",
    "        json_file = os.path.join(self.save_dir, \"jobinja_data_v4.json\")\n",
    "        with open(json_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.all_jobs_data, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        # Save as CSV\n",
    "        df = pd.DataFrame(self.all_jobs_data)\n",
    "        csv_file = os.path.join(self.save_dir, \"jobinja_data_v4.csv\")\n",
    "        df.to_csv(csv_file, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        # Save as Excel\n",
    "        excel_file = os.path.join(self.save_dir, \"jobinja_data.xlsx\")\n",
    "        df.to_excel(excel_file, index=False)\n",
    "        \n",
    "        # Save as MAT file\n",
    "        mat_file = os.path.join(self.save_dir, \"jobinja_data.mat\")\n",
    "        job_data_dict = {f\"job_{i}\": job for i, job in enumerate(self.all_jobs_data)}\n",
    "        sio.savemat(mat_file, job_data_dict)\n",
    "        \n",
    "        print(f\"Dataset saved in multiple formats at: {self.save_dir}\")\n",
    "\n",
    "    def analyze_corpus(self):\n",
    "        \"\"\"Perform basic text analysis on the job corpus\"\"\"\n",
    "        try:\n",
    "            from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "            from sklearn.cluster import KMeans\n",
    "            \n",
    "            # Load corpus\n",
    "            corpus_file = os.path.join(self.corpus_dir, 'job_corpus.txt')\n",
    "            with open(corpus_file, 'r', encoding='utf-8') as f:\n",
    "                corpus = f.read().split('\\n')\n",
    "            \n",
    "            # Create TF-IDF matrix\n",
    "            vectorizer = TfidfVectorizer(max_features=1000)\n",
    "            tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "            \n",
    "            # Perform clustering\n",
    "            n_clusters = min(10, len(corpus))\n",
    "            kmeans = KMeans(n_clusters=n_clusters)\n",
    "            cluster_labels = kmeans.fit_predict(tfidf_matrix)\n",
    "            \n",
    "            # Get top terms per cluster\n",
    "            cluster_terms = {}\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            for i in range(n_clusters):\n",
    "                center = kmeans.cluster_centers_[i]\n",
    "                top_indices = center.argsort()[-10:][::-1]\n",
    "                top_terms = [feature_names[j] for j in top_indices]\n",
    "                cluster_terms[f\"Cluster_{i}\"] = top_terms\n",
    "            \n",
    "            # Save analysis results\n",
    "            analysis_file = os.path.join(self.corpus_dir, 'corpus_analysis_v4.json')\n",
    "            with open(analysis_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump({\n",
    "                    'cluster_terms': cluster_terms,\n",
    "                    'n_documents': len(corpus),\n",
    "                    'vocabulary_size': len(feature_names)\n",
    "                }, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "            return cluster_terms\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error performing corpus analysis: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19407fa6-5918-44ec-b28b-2d8ee0bab742",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting job posting data...\n",
      "Scraping page 1...\n",
      "No more jobs found on page 1\n",
      "Completed scraping 0 jobs across 0 pages\n",
      "Dataset saved in multiple formats at: JobInja/v4\n",
      "Error creating visualizations: index 0 is out of bounds for axis 0 with size 0\n",
      "An error occurred during analysis: 'company_name'\n",
      "\n",
      "An error occurred: 'company_name'\n",
      "\n",
      "Script completed\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Jobinja Scraper Advanced Usage Example\n",
    "This script demonstrates advanced usage of the Jobinja scraper including data analysis and visualization.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "class JobinjaAnalyzer:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_dir = save_dir\n",
    "        self.scraper = Jobinja()\n",
    "        self.data = None\n",
    "        \n",
    "    def run_full_analysis(self, max_pages=None):\n",
    "        \"\"\"Run a complete analysis of job postings\"\"\"\n",
    "        # 1. Collect Data\n",
    "        self.collect_data(max_pages)\n",
    "        \n",
    "        # 2. Analyze Data\n",
    "        self.analyze_job_market()\n",
    "        \n",
    "        # 3. Create Visualizations\n",
    "        self.create_visualizations()\n",
    "        \n",
    "        # 4. Generate Reports\n",
    "        self.generate_reports()\n",
    "        \n",
    "    def collect_data(self, max_pages=None):\n",
    "        \"\"\"Collect job posting data\"\"\"\n",
    "        print(\"Collecting job posting data...\")\n",
    "        self.scraper.scrape_jobs(max_pages=max_pages)\n",
    "        self.data = pd.DataFrame(self.scraper.all_jobs_data)\n",
    "        \n",
    "    def analyze_job_market(self):\n",
    "        \"\"\"Perform detailed job market analysis\"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data available. Run collect_data first.\")\n",
    "            \n",
    "        # Create analysis directory\n",
    "        analysis_dir = os.path.join(self.save_dir, 'analysis')\n",
    "        os.makedirs(analysis_dir, exist_ok=True)\n",
    "        \n",
    "        # Basic statistics\n",
    "        stats = {\n",
    "            'total_jobs': len(self.data),\n",
    "            'unique_companies': self.data['company_name'].nunique(),\n",
    "            'unique_categories': self.data['job_category'].nunique(),\n",
    "            'unique_locations': self.data['job_location'].nunique()\n",
    "        }\n",
    "        \n",
    "        # Salary analysis (if available)\n",
    "        if 'salary' in self.data.columns:\n",
    "            salary_stats = self.analyze_salaries()\n",
    "            stats.update(salary_stats)\n",
    "        \n",
    "        # Save statistics\n",
    "        with open(os.path.join(analysis_dir, 'market_analysis.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(stats, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "    def analyze_salaries(self):\n",
    "        \"\"\"Analyze salary information\"\"\"\n",
    "        # This is a placeholder for salary analysis\n",
    "        # You would need to implement proper salary parsing and analysis\n",
    "        return {}\n",
    "        \n",
    "    def create_visualizations(self):\n",
    "        \"\"\"Create various visualizations of the job market data\"\"\"\n",
    "        viz_dir = os.path.join(self.save_dir, 'visualizations')\n",
    "        os.makedirs(viz_dir, exist_ok=True)\n",
    "        \n",
    "        # Set style for Persian text\n",
    "        plt.rcParams['font.family'] = 'Arial'\n",
    "        \n",
    "        # 1. Job Categories Distribution\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        self.data['job_category'].value_counts().head(10).plot(kind='bar')\n",
    "        plt.title('Top 10 Job Categories')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(viz_dir, 'job_categories.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # 2. Companies Distribution\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        self.data['company_name'].value_counts().head(10).plot(kind='bar')\n",
    "        plt.title('Top 10 Companies by Job Postings')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(viz_dir, 'top_companies.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # 3. Location Distribution\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        self.data['job_location'].value_counts().head(10).plot(kind='bar')\n",
    "        plt.title('Top 10 Job Locations')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(viz_dir, 'job_locations.png'))\n",
    "        plt.close()\n",
    "        \n",
    "    def generate_reports(self):\n",
    "        \"\"\"Generate detailed reports from the analyzed data\"\"\"\n",
    "        reports_dir = os.path.join(self.save_dir, 'reports')\n",
    "        os.makedirs(reports_dir, exist_ok=True)\n",
    "        \n",
    "        # Generate summary report\n",
    "        summary = []\n",
    "        summary.append(\"# Jobinja Market Analysis Report\")\n",
    "        summary.append(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        \n",
    "        # Add basic statistics\n",
    "        summary.append(\"## Basic Statistics\")\n",
    "        summary.append(f\"Total Jobs Analyzed: {len(self.data)}\")\n",
    "        summary.append(f\"Unique Companies: {self.data['company_name'].nunique()}\")\n",
    "        summary.append(f\"Job Categories: {self.data['job_category'].nunique()}\\n\")\n",
    "        \n",
    "        # Add top categories\n",
    "        summary.append(\"## Top Job Categories\")\n",
    "        top_categories = self.data['job_category'].value_counts().head(10)\n",
    "        for cat, count in top_categories.items():\n",
    "            summary.append(f\"- {cat}: {count}\")\n",
    "        \n",
    "        # Save the report\n",
    "        with open(os.path.join(reports_dir, 'market_analysis_report.md'), 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(summary))\n",
    "\n",
    "def main():\n",
    "    # Create a unique directory for this analysis session\n",
    "    #timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    save_dir = \"JobInja/v4\"\n",
    "\n",
    "    try:\n",
    "        # Initialize analyzer\n",
    "        analyzer = JobinjaAnalyzer(save_dir)\n",
    "        \n",
    "        # Run complete analysis (limit to 10 pages for testing)\n",
    "        analyzer.run_full_analysis(max_pages=10)\n",
    "        \n",
    "        print(f\"\\nAnalysis completed. Results saved in: {save_dir}\")\n",
    "        print(\"\\nGenerated files:\")\n",
    "        print(f\"- Market analysis: {save_dir}/analysis/market_analysis.json\")\n",
    "        print(f\"- Visualizations: {save_dir}/visualizations/\")\n",
    "        print(f\"- Reports: {save_dir}/reports/market_analysis_report.md\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during analysis: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nAnalysis interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {e}\")\n",
    "    finally:\n",
    "        print(\"\\nScript completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8986b6c6-f211-468f-aabd-7c35d3a50464",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logo_url': 'https://example.com/iranicard-logo.png'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_company_data(html):\n",
    "    companies = []\n",
    "    \n",
    "    # Find all company divs\n",
    "    company_divs = re.findall(r'<div\\s+class=\"u-flex-important u-pL10-sd\">(.*?)</div>', html, re.DOTALL)\n",
    "    \n",
    "    for company_div in company_divs:\n",
    "        company = {}\n",
    "        \n",
    "        # Extract company name\n",
    "        company_name_match = re.search(r'<a href=\"https://jobinja\\.ir/companies/([^/]+)/jobs', company_div)\n",
    "        if company_name_match:\n",
    "            company['name'] = company_name_match.group(1).replace('-', ' ')\n",
    "        \n",
    "        # Extract company logo URL\n",
    "        logo_url_match = re.search(r'<img class=\"o-listView__itemIndicatorImage\" src=\"([^\"]+)\"', company_div)\n",
    "        if logo_url_match:\n",
    "            company['logo_url'] = logo_url_match.group(1)\n",
    "        \n",
    "        # Extract number of active jobs\n",
    "        active_jobs_match = re.search(r'<a href=\"https://jobinja\\.ir/companies/[^/]+/jobs\" class=\"color-dark-green\">\\s*(\\d+)\\s*شغل فعال\\s*</a>', company_div)\n",
    "        if active_jobs_match:\n",
    "            company['active_jobs'] = int(active_jobs_match.group(1))\n",
    "        \n",
    "        # Extract popularity score\n",
    "        popularity_match = re.search(r'محبوبیت میان کارجویان.*?(\\d+)', company_div)\n",
    "        if popularity_match:\n",
    "            company['popularity_score'] = int(popularity_match.group(1))\n",
    "        \n",
    "        # Extract job diversity score\n",
    "        diversity_match = re.search(r'تعدد و تنوع فرصت‌های شغلی.*?(\\d+)', company_div)\n",
    "        if diversity_match:\n",
    "            company['job_diversity_score'] = int(diversity_match.group(1))\n",
    "        \n",
    "        # Extract resume review score\n",
    "        resume_review_match = re.search(r'بررسی رزومه‌های دریافتی.*?(\\d+)', company_div)\n",
    "        if resume_review_match:\n",
    "            company['resume_review_score'] = int(resume_review_match.group(1))\n",
    "        \n",
    "        # Extract company description\n",
    "        description_match = re.search(r'<div class=\"color-gray-05 u-textSmall u-mT20 overflow-ellipsis three-line\">\\s*(.*?)\\s*</div>', company_div, re.DOTALL)\n",
    "        if description_match:\n",
    "            company['description'] = description_match.group(1).strip()\n",
    "        \n",
    "        companies.append(company)\n",
    "    \n",
    "    return companies\n",
    "\n",
    "# Example usage\n",
    "html_content = '''\n",
    "<div class=\"u-flex-important u-pL10-sd\">\n",
    "    <div class=\"card-border u-p20 u-p10-sd u-flex flex-column flex-space u-height100 companies-list-card-width\">\n",
    "        <div>\n",
    "            <div class=\"u-flex u-mB20\">\n",
    "                <div>\n",
    "                    <a href=\"https://jobinja.ir/companies/iranicard\" class=\"u-mB30\">\n",
    "                        <img class=\"o-listView__itemIndicatorImage\" src=\"https://example.com/iranicard-logo.png\" alt=\"ایرانیکارت\">\n",
    "                    </a>\n",
    "                </div>\n",
    "                <div class=\"u-mR20 u-text-0\">\n",
    "                    <a href=\"https://jobinja.ir/companies/iranicard\" class=\"u-bold color-gray-07 u-mB1 overflow-ellipsis-1 companies-items-width u-textBase\">\n",
    "                        ایرانیکارت\n",
    "                    </a>\n",
    "                    <div class=\"color-gray-05 u-textSmall overflow-ellipsis-1 companies-items-width\">\n",
    "                        کامپیوتر، فناوری اطلاعات و اینترنت\n",
    "                    </div>\n",
    "                </div>\n",
    "            </div>\n",
    "            <div class=\"color-gray-05 u-textSmall u-mT20 overflow-ellipsis three-line\">\n",
    "                ایرانیکارت گروهی جوان اما با تجربه است که سال&zwnj;ها در امور مالی بین المللی به صدها شرکت و هزاران شخص کمک رسانده و می&zwnj;رساند.\n",
    "            </div>\n",
    "        </div>\n",
    "        <div>\n",
    "            <hr class=\"u-mT20 u-mB20 u-width100\">\n",
    "            <div class=\"color-gray-07 u-textSmall\">\n",
    "                <div class=\"u-flex u-mB10\">\n",
    "                    <div class=\"u-bold-700 bg-light-green color-dark-green u-pR5 u-pL5 border-radius-2\">\n",
    "                        ۹\n",
    "                    </div>\n",
    "                    <div class=\"u-mR10\">محبوبیت میان کارجویان</div>\n",
    "                </div>\n",
    "                <div class=\"u-flex u-mB10\">\n",
    "                    <div class=\"u-bold-700 bg-light-green color-dark-green u-pR5 u-pL5 border-radius-2\">\n",
    "                        ۶\n",
    "                    </div>\n",
    "                    <div class=\"u-mR10\">تعدد و تنوع فرصت‌های شغلی</div>\n",
    "                </div>\n",
    "                <div class=\"u-flex\">\n",
    "                    <div class=\"u-bold-700 bg-light-green color-dark-green u-pR5 u-pL5 border-radius-2\">\n",
    "                        ۱۰\n",
    "                    </div>\n",
    "                    <div class=\"u-mR10\">بررسی رزومه‌های دریافتی</div>\n",
    "                </div>\n",
    "            </div>\n",
    "            <hr class=\"u-mT20 u-mB20 u-width100\">\n",
    "            <div class=\"u-flex flex-space u-textSmall\">\n",
    "                <div class=\"bg-light-green color-dark-green u-pR10 u-pL10 border-radius-2\">\n",
    "                    <a href=\"https://jobinja.ir/companies/iranicard/jobs\" class=\"color-dark-green\">\n",
    "                        ۱۹\n",
    "                        شغل فعال\n",
    "                    </a>\n",
    "                </div>\n",
    "                <div class=\"u-bold u-flex\">\n",
    "                    <a href=\"https://jobinja.ir/companies/iranicard\" class=\"u-flex align-items-center\">\n",
    "                        صفحه شرکت\n",
    "                        <div class=\"chevron-left-blue\"></div>\n",
    "                    </a>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "    '''\n",
    "\n",
    "companies_data = extract_company_data(html_content)\n",
    "for company in companies_data:\n",
    "    print(company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5900e8f5-6803-4e3f-a23f-a07ec181d1c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting company details from https://jobinja.ir/companies/asa-1/jobs/Ajcs/%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-senior-net-developer-%D8%AF%D8%B1-%D9%88%DB%8C%D8%B3%D8%AA%D8%A7-%D8%B3%D8%A7%D9%85%D8%A7%D9%86%D9%87-%D8%A2%D8%B3%D8%A7: 'NoneType' object has no attribute 'find'\n",
      "Company Details: None\n",
      "Job Details: JobDetails(title='', category='', location={'province': '', 'city': ''}, employment_type='', experience_required='', salary='', required_skills=[''], gender='', military_service='', minimum_degree='', description='', responsibilities=[], requirements=[], benefits=[], posted_date='2024-11-07')\n",
      "Found 0 jobs\n",
      "Found 0 Python jobs in Tehran\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "@dataclass\n",
    "class JobDetails:\n",
    "    title: str\n",
    "    category: str\n",
    "    location: Dict[str, str]\n",
    "    employment_type: str\n",
    "    experience_required: str\n",
    "    salary: str\n",
    "    required_skills: List[str]\n",
    "    gender: str\n",
    "    military_service: str\n",
    "    minimum_degree: str\n",
    "    description: str\n",
    "    responsibilities: List[str]\n",
    "    requirements: List[str]\n",
    "    benefits: List[str]\n",
    "    posted_date: str\n",
    "\n",
    "@dataclass\n",
    "class CompanyDetails:\n",
    "    name: str\n",
    "    english_name: str\n",
    "    website: str\n",
    "    industry_type: str\n",
    "    company_size: str\n",
    "    establishment_year: str\n",
    "    description: str\n",
    "    logo_url: str\n",
    "\n",
    "class JobinjaScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://jobinja.ir\"\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "\n",
    "    def _make_request(self, url: str) -> Optional[BeautifulSoup]:\n",
    "        \"\"\"Make HTTP request and return BeautifulSoup object\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.text, 'html.parser')\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error making request to {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_job_details(self, job_url: str) -> Optional[JobDetails]:\n",
    "        \"\"\"Extract details for a specific job posting\"\"\"\n",
    "        soup = self._make_request(job_url)\n",
    "        if not soup:\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            # Extract basic info\n",
    "            info_boxes = soup.find_all('li', class_='c-infoBox__item')\n",
    "            job_info = {}\n",
    "            \n",
    "            for box in info_boxes:\n",
    "                title = box.find('h4', class_='c-infoBox__itemTitle')\n",
    "                value = box.find('span', class_='black')\n",
    "                if title and value:\n",
    "                    job_info[title.text.strip()] = value.text.strip()\n",
    "\n",
    "            # Extract job description\n",
    "            description_div = soup.find('div', class_='o-box__text s-jobDesc')\n",
    "            description = description_div.text.strip() if description_div else \"\"\n",
    "\n",
    "            # Parse responsibilities, requirements and benefits from description\n",
    "            responsibilities = []\n",
    "            requirements = []\n",
    "            benefits = []\n",
    "\n",
    "            # Extract title from page\n",
    "            title_element = soup.find('h1')\n",
    "            title = title_element.text.strip() if title_element else \"\"\n",
    "\n",
    "            # Get posting date from meta tags or other sources\n",
    "            date_element = soup.find('meta', {'property': 'article:published_time'})\n",
    "            posted_date = date_element['content'] if date_element else datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "            # Create JobDetails object\n",
    "            return JobDetails(\n",
    "                title=title,\n",
    "                category=job_info.get('دسته‌بندی شغلی', ''),\n",
    "                location={\n",
    "                    'province': job_info.get('موقعیت مکانی', '').split('،')[0].strip(),\n",
    "                    'city': job_info.get('موقعیت مکانی', '').split('،')[-1].strip()\n",
    "                },\n",
    "                employment_type=job_info.get('نوع همکاری', ''),\n",
    "                experience_required=job_info.get('حداقل سابقه کار', ''),\n",
    "                salary=job_info.get('حقوق', ''),\n",
    "                required_skills=job_info.get('مهارت‌های مورد نیاز', '').split('،'),\n",
    "                gender=job_info.get('جنسیت', ''),\n",
    "                military_service=job_info.get('وضعیت نظام وظیفه', ''),\n",
    "                minimum_degree=job_info.get('حداقل مدرک تحصیلی', ''),\n",
    "                description=description,\n",
    "                responsibilities=responsibilities,\n",
    "                requirements=requirements,\n",
    "                benefits=benefits,\n",
    "                posted_date=posted_date\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting job details from {job_url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_company_details(self, company_url: str) -> Optional[CompanyDetails]:\n",
    "        \"\"\"Extract details for a specific company\"\"\"\n",
    "        soup = self._make_request(company_url)\n",
    "        if not soup:\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            # Find company header section\n",
    "            header = soup.find('div', class_='c-companyHeader')\n",
    "            \n",
    "            # Extract company information\n",
    "            name_element = header.find('h2', class_='c-companyHeader__name')\n",
    "            company_name = name_element.text.split('|')[0].strip() if name_element else \"\"\n",
    "            english_name = name_element.text.split('|')[1].strip() if name_element and '|' in name_element.text else \"\"\n",
    "\n",
    "            # Find meta information\n",
    "            meta_items = header.find_all('span', class_='c-companyHeader__metaItem')\n",
    "            \n",
    "            company_info = {}\n",
    "            for item in meta_items:\n",
    "                text = item.text.strip()\n",
    "                if 'تاسیس در' in text:\n",
    "                    company_info['establishment_year'] = text.replace('تاسیس در', '').strip()\n",
    "                elif 'نفر' in text:\n",
    "                    company_info['company_size'] = text\n",
    "                else:\n",
    "                    link = item.find('a')\n",
    "                    if link:\n",
    "                        if 'http' in link['href']:\n",
    "                            company_info['website'] = link.text.strip()\n",
    "                        else:\n",
    "                            company_info['industry_type'] = link.text.strip()\n",
    "\n",
    "            # Get company description\n",
    "            description_element = soup.find('div', class_='c-companyHeader__description')\n",
    "            description = description_element.text.strip() if description_element else \"\"\n",
    "\n",
    "            # Get logo URL\n",
    "            logo_element = soup.find('img', class_='c-companyHeader__logoImage')\n",
    "            logo_url = logo_element['src'] if logo_element else \"\"\n",
    "\n",
    "            return CompanyDetails(\n",
    "                name=company_name,\n",
    "                english_name=english_name,\n",
    "                website=company_info.get('website', ''),\n",
    "                industry_type=company_info.get('industry_type', ''),\n",
    "                company_size=company_info.get('company_size', ''),\n",
    "                establishment_year=company_info.get('establishment_year', ''),\n",
    "                description=description,\n",
    "                logo_url=logo_url\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting company details from {company_url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_all_jobs(self, max_pages: int = 1) -> List[Dict]:\n",
    "        \"\"\"Get all jobs from multiple pages\"\"\"\n",
    "        all_jobs = []\n",
    "        \n",
    "        for page in range(1, max_pages + 1):\n",
    "            url = f\"{self.base_url}/jobs?page={page}\"\n",
    "            soup = self._make_request(url)\n",
    "            if not soup:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                job_items = soup.find_all('li', class_='c-jobListView__item')\n",
    "                \n",
    "                for job_item in job_items:\n",
    "                    try:\n",
    "                        # Extract job link\n",
    "                        job_link = job_item.find('a', class_='c-jobListView__titleLink')\n",
    "                        if not job_link:\n",
    "                            continue\n",
    "\n",
    "                        job_url = job_link['href']\n",
    "                        \n",
    "                        # Extract basic job information from the list\n",
    "                        title = job_link.text.strip()\n",
    "                        company_element = job_item.find('li', class_='c-jobListView__metaItem')\n",
    "                        company_name = company_element.text.strip() if company_element else \"\"\n",
    "                        \n",
    "                        # Get detailed job information\n",
    "                        job_details = self.extract_job_details(job_url)\n",
    "                        \n",
    "                        if job_details:\n",
    "                            all_jobs.append({\n",
    "                                'title': title,\n",
    "                                'company_name': company_name,\n",
    "                                'job_url': job_url,\n",
    "                                'details': job_details\n",
    "                            })\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing job item: {e}\")\n",
    "                        continue\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing page {page}: {e}\")\n",
    "                continue\n",
    "\n",
    "        return all_jobs\n",
    "\n",
    "    def search_jobs(self, keyword: str, location: str = None) -> List[Dict]:\n",
    "        \"\"\"Search for jobs with specific keyword and location\"\"\"\n",
    "        url = f\"{self.base_url}/jobs/search?keywords={keyword}\"\n",
    "        if location:\n",
    "            url += f\"&location={location}\"\n",
    "            \n",
    "        soup = self._make_request(url)\n",
    "        if not soup:\n",
    "            return []\n",
    "\n",
    "        jobs = []\n",
    "        try:\n",
    "            job_items = soup.find_all('li', class_='c-jobListView__item')\n",
    "            \n",
    "            for job_item in job_items:\n",
    "                try:\n",
    "                    job_link = job_item.find('a', class_='c-jobListView__titleLink')\n",
    "                    if not job_link:\n",
    "                        continue\n",
    "\n",
    "                    job_url = job_link['href']\n",
    "                    title = job_link.text.strip()\n",
    "                    \n",
    "                    # Get company name\n",
    "                    company_element = job_item.find('li', class_='c-jobListView__metaItem')\n",
    "                    company_name = company_element.text.strip() if company_element else \"\"\n",
    "                    \n",
    "                    # Get job details\n",
    "                    job_details = self.extract_job_details(job_url)\n",
    "                    \n",
    "                    if job_details:\n",
    "                        jobs.append({\n",
    "                            'title': title,\n",
    "                            'company_name': company_name,\n",
    "                            'job_url': job_url,\n",
    "                            'details': job_details\n",
    "                        })\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing search result: {e}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching jobs: {e}\")\n",
    "            \n",
    "        return jobs\n",
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "    scraper = JobinjaScraper()\n",
    "    \n",
    "    # Get company details\n",
    "    company_url = \"https://jobinja.ir/companies/carpars\"\n",
    "    company_details = scraper.extract_company_details(company_url)\n",
    "    print(\"Company Details:\", company_details)\n",
    "    \n",
    "    # Get specific job details\n",
    "    job_url = \"https://jobinja.ir/companies/carpars/jobs/\"\n",
    "    job_details = scraper.extract_job_details(job_url)\n",
    "    print(\"Job Details:\", job_details)\n",
    "    \n",
    "    # Get all jobs from first page\n",
    "    all_jobs = scraper.get_all_jobs(max_pages=1)\n",
    "    print(f\"Found {len(all_jobs)} jobs\")\n",
    "    \n",
    "    # Search for specific jobs\n",
    "    python_jobs = scraper.search_jobs(keyword=\"python\", location=\"tehran\")\n",
    "    print(f\"Found {len(python_jobs)} Python jobs in Tehran\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5278f61b-e638-4a77-a599-1fd179a53c22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error making request: HTTPSConnectionPool(host='jobinja.ir', port=443): Max retries exceeded with url: /companies/carpars (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1006)')))\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_json_data():\n",
    "    # API URL\n",
    "    url = 'https://jobinja.ir/companies/carpars'\n",
    "    \n",
    "    try:\n",
    "        # Make request\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Get JSON data\n",
    "            data = response.json()\n",
    "            \n",
    "            # Print formatted JSON\n",
    "            print(json.dumps(data, indent=2))\n",
    "            \n",
    "            # Save to file\n",
    "            with open('user_data.json', 'w') as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "                \n",
    "            print(\"\\nData has been saved to 'user_data.json'\")\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code}\")\n",
    "            \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error making request: {e}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    get_json_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9987f70-dcd8-4544-a44f-6514ad66dc4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
